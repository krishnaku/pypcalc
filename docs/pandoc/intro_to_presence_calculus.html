<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>The Presence Calculus</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
    html {
      box-sizing: border-box;
      font-size: 100%;
    }

    *, *:before, *:after {
      box-sizing: inherit;
    }

    body {
      margin: 0;
      padding: 0;
      font-family: Georgia, serif;
      font-size: 1.1rem;
      line-height: 1.75;
      background: #fff;
      color: #111;

      /* Remove Pandoc's injected width restrictions */
      max-width: none !important;
      padding-left: 0 !important;
      padding-right: 0 !important;
      padding-top: 0 !important;
      padding-bottom: 0 !important;
    }

    .wrapper {
      display: flex;
      justify-content: center;
    }

    #page {
      max-width: 880px;
      width: 100%;
      padding: 2rem 1rem;
    }

    h1, h2, h3, h4 {
      font-family: Georgia, serif;
      font-weight: bold;
      margin-top: 2rem;
      margin-bottom: 1rem;
      line-height: 1.4;
    }

    p {
      margin: 1.25rem 0;
    }

    img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 2rem auto;
    }

    pre {
      background: #f8f8f8;
      padding: 1rem;
      overflow-x: auto;
      border-radius: 6px;
    }

    code {
      font-family: SFMono-Regular, Consolas, monospace;
      background: #f5f5f5;
      padding: 0.2em 0.4em;
      border-radius: 4px;
    }

    blockquote {
      margin: 2rem 0;
      padding-left: 1rem;
      border-left: 4px solid #ddd;
      color: #666;
      font-style: italic;
    }

    .subtitle, .author, .date {
      color: #666;
      font-size: 0.9rem;
      margin-top: 0.5rem;
      text-align: center;
    }

    nav#TOC {
      margin: 2rem 0;
      padding: 1rem;
      border: 1px solid #eee;
      background: #fafafa;
      overflow-x: auto;
    }

    figure {
      margin: 2rem auto;
      text-align: center;
    }

    figure img {
      width: 100%;
      height: auto;
      display: block;
      margin: 2rem auto;
    }

    figcaption {
      font-size: 0.9em;
      color: #555;
      margin-top: 0.75rem;
    }

    header#title-block-header {
      text-align: center;
    }

    @media screen and (min-width: 1024px) {
      body {
        font-size: 1.15rem;
      }

      #page {
        padding: 3rem 2rem;
      }
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
  <div class="wrapper">
    <div id="page">
                  <header id="title-block-header">
        <h1 class="title"><strong>The Presence Calculus</strong></h1>
        <p class="subtitle"><span style="font-size:1.2em;">A Gentle
Introduction</span></p>
        <p class="author"><p>Dr.Â Krishna Kumar<br />
<a href="https://exathink.com"><em>The Polaris Advisor
Program</em></a></p></p>
        
              </header>
                  <nav id="TOC" role="doc-toc">
        <h2 id="toc-title">Contents</h2>
        <ul>
        <li><a href="#what-is-the-presence-calculus"
        id="toc-what-is-the-presence-calculus"><span
        class="toc-section-number">1</span> What is The Presence
        Calculus?</a>
        <ul>
        <li><a href="#the-pitch" id="toc-the-pitch"><span
        class="toc-section-number">1.1</span> The pitch</a></li>
        <li><a href="#learning-about-the-presence-calculus"
        id="toc-learning-about-the-presence-calculus"><span
        class="toc-section-number">1.2</span> Learning about the
        presence calculus</a></li>
        </ul></li>
        <li><a href="#why-presence" id="toc-why-presence"><span
        class="toc-section-number">2</span> Why presence?</a>
        <ul>
        <li><a href="#an-example" id="toc-an-example"><span
        class="toc-section-number">2.1</span> An example</a></li>
        <li><a href="#a-software-example"
        id="toc-a-software-example"><span
        class="toc-section-number">2.2</span> A software
        example</a></li>
        </ul></li>
        <li><a href="#presence" id="toc-presence"><span
        class="toc-section-number">3</span> What is a presence?</a>
        <ul>
        <li><a href="#presence-mass-the-manifestation-of-presence"
        id="toc-presence-mass-the-manifestation-of-presence"><span
        class="toc-section-number">3.1</span> Presence mass: the
        manifestation of presence</a></li>
        <li><a href="#presence-density-functions-aka-signals"
        id="toc-presence-density-functions-aka-signals"><span
        class="toc-section-number">3.2</span> Presence density functions
        <em>aka</em> Signals</a></li>
        <li><a href="#more-examples" id="toc-more-examples"><span
        class="toc-section-number">3.3</span> More examples</a></li>
        <li><a href="#presence-a-summary"
        id="toc-presence-a-summary"><span
        class="toc-section-number">3.4</span> Presence: a
        summary</a></li>
        </ul></li>
        <li><a href="#systems-of-presence"
        id="toc-systems-of-presence"><span
        class="toc-section-number">4</span> Systems of presence</a>
        <ul>
        <li><a href="#presence-as-a-sample-of-a-signal"
        id="toc-presence-as-a-sample-of-a-signal"><span
        class="toc-section-number">4.1</span> Presence as a sample of a
        signal</a></li>
        <li><a href="#presence-assertions"
        id="toc-presence-assertions"><span
        class="toc-section-number">4.2</span> Presence
        assertions</a></li>
        </ul></li>
        <li><a href="#presence-invariant"
        id="toc-presence-invariant"><span
        class="toc-section-number">5</span> Co-presence and the presence
        invariant</a>
        <ul>
        <li><a href="#an-example-1" id="toc-an-example-1"><span
        class="toc-section-number">5.1</span> An example</a></li>
        <li><a href="#why-it-matters" id="toc-why-it-matters"><span
        class="toc-section-number">5.2</span> Why it matters</a></li>
        <li><a href="#binary-presences-and-littles-law"
        id="toc-binary-presences-and-littles-law"><span
        class="toc-section-number">5.3</span> Binary presences and
        Littleâs Law</a></li>
        <li><a href="#signal-dynamics" id="toc-signal-dynamics"><span
        class="toc-section-number">5.4</span> Signal dynamics</a></li>
        </ul></li>
        <li><a href="#presence-matrix" id="toc-presence-matrix"><span
        class="toc-section-number">6</span> The presence matrix</a>
        <ul>
        <li><a href="#the-presence-invariant-and-the-presence-matrix"
        id="toc-the-presence-invariant-and-the-presence-matrix"><span
        class="toc-section-number">6.1</span> The presence invariant and
        the presence matrix</a></li>
        </ul></li>
        <li><a href="#presence-accumulation-matrix"
        id="toc-presence-accumulation-matrix"><span
        class="toc-section-number">7</span> The presence accumulation
        matrix</a>
        <ul>
        <li><a href="#the-presence-accumulation-recurrence"
        id="toc-the-presence-accumulation-recurrence"><span
        class="toc-section-number">7.1</span> The presence accumulation
        recurrence</a></li>
        </ul></li>
        <li><a href="#signal-dynamics" id="toc-signal-dynamics"><span
        class="toc-section-number">8</span> Computing signal
        dynamics</a>
        <ul>
        <li><a href="#sample-paths" id="toc-sample-paths"><span
        class="toc-section-number">8.1</span> Sample paths</a></li>
        <li><a href="#convergence" id="toc-convergence"><span
        class="toc-section-number">8.2</span> Convergence and divergence
        of presence density</a></li>
        <li><a href="#detecting-convergence"
        id="toc-detecting-convergence"><span
        class="toc-section-number">8.3</span> Detecting
        convergence</a></li>
        <li><a href="#the-presence-invariant-and-rate-conservation-laws"
        id="toc-the-presence-invariant-and-rate-conservation-laws"><span
        class="toc-section-number">8.4</span> The presence invariant and
        rate conservation laws</a></li>
        </ul></li>
        <li><a href="#visualizing-signal-dynamics"
        id="toc-visualizing-signal-dynamics"><span
        class="toc-section-number">9</span> Visualizing signal
        dynamics</a>
        <ul>
        <li><a href="#phase-space" id="toc-phase-space"><span
        class="toc-section-number">9.1</span> The presence invariant in
        phase space</a></li>
        <li><a href="#flow-fields" id="toc-flow-fields"><span
        class="toc-section-number">9.2</span> Flow fields</a></li>
        <li><a href="#interpreting-flow-fields"
        id="toc-interpreting-flow-fields"><span
        class="toc-section-number">9.3</span> Interpreting flow
        fields</a></li>
        <li><a href="#attractors" id="toc-attractors"><span
        class="toc-section-number">9.4</span> Sample path trajectories
        and attractors</a></li>
        <li><a href="#feedback-loops-and-steering"
        id="toc-feedback-loops-and-steering"><span
        class="toc-section-number">9.5</span> Feedback loops and
        steering</a></li>
        </ul></li>
        <li><a href="#taking-stock" id="toc-taking-stock"><span
        class="toc-section-number">10</span> Taking stock</a>
        <ul>
        <li><a href="#why-should-i-care"
        id="toc-why-should-i-care"><span
        class="toc-section-number">10.1</span> Why should I
        care?</a></li>
        <li><a href="#where-can-i-apply-this-and-how"
        id="toc-where-can-i-apply-this-and-how"><span
        class="toc-section-number">10.2</span> Where can I apply this
        and how?</a></li>
        <li><a href="#towards-systems-of-systems"
        id="toc-towards-systems-of-systems"><span
        class="toc-section-number">10.3</span> Towards Systems of
        Systems</a></li>
        </ul></li>
        <li><a href="#a-personal-note" id="toc-a-personal-note"><span
        class="toc-section-number">11</span> A personal note</a>
        <ul>
        <li><a href="#the-road-ahead" id="toc-the-road-ahead"><span
        class="toc-section-number">11.1</span> The road ahead</a></li>
        </ul></li>
        <li><a href="#references"
        id="toc-references">References</a></li>
        </ul>
      </nav>
            <div
            style="text-align: center; font-size: 80%; margin-top: 3em;">
            <p>Â© 2025 Krishna Kumar. All rights reserved.</p>
            </div>
            <h2 data-number="1" id="what-is-the-presence-calculus"><span
            class="header-section-number">1</span> What is The Presence
            Calculus?</h2>
            <figure id="fig:key-concepts">
            <img src="../assets/pandoc/pcalc_machinery.png"
            alt="Figure 1: The Presence Calculus - Key Concepts" />
            <figcaption aria-hidden="true">Figure 1: The Presence
            Calculus - Key Concepts</figcaption>
            </figure>
            <p>The Presence Calculus is a quantitative model for
            reasoning about signal dynamics in a domain.</p>
            <p>Its purpose is to support principled modeling and
            rigorous decision-making using operational data and signals
            in business-critical contextsâensuring that such decisions
            rest on a mathematically precise, logically coherent, and
            epistemically grounded foundation.</p>
            <p>The Presence Calculus emerged from a search for better
            tools to reason about operations management in software
            product development and engineeringâdomains where prevailing
            approaches to measurement fall short on all three
            fronts.</p>
            <p>At a minimum, its foundational constructs bring
            mathematical precision and clarity to widely usedâbut poorly
            definedâconcepts such as <em>flow</em>, <em>stability</em>,
            <em>equilibrium</em>, and <em>coherence</em> for measurable
            signals in a domain.</p>
            <p>More importantly, it offers a uniform set of abstractions
            and computational tools that connect path-dependent domain
            signals to business-relevant measures such as delay, cost,
            revenue, and user experience.</p>
            <p>For software development, this enables the construction
            of bespoke, context-specific measurement models for
            operational improvementâa powerful alternative to
            one-size-fits-all metrics frameworks and visibility tools,
            which remain the dominant option today.</p>
            <p>The Presence Calculus is a novel modeling and measurement
            substrate with <em>measure theory</em>, a branch of <em>real
            analysis</em>, as its mathematical foundation. It treats
            time as a first-class concept, making it well suited for
            analyzing <em>continuous</em>, <em>time-dependent</em>
            behaviors of systems.</p>
            <p>It <em>complements</em> classical statistical and
            probabilistic analysis, and is particularly well suited to
            domains like software development where state, history, and
            path dependence complicate traditional inference
            techniques.</p>
            <p>On its own, the Presence Calculus provides a precisely
            defined set of modeling and computational primitives (shown
            in FigureÂ 1) for analyzing the history and evolution of
            time-varying signal systemsâstructures about which we can
            make mathematically provable claims.</p>
            <p>We can then use these primitives to construct richer,
            more expressive mathematical models to reason about time
            varying behavior in real-world domains.</p>
            <p>Our goal in this document to present the foundational
            ideas of the presence calculus as a coherent whole. We will
            use examples to motivate key concepts but the focus is here
            is not on domain modeling or specific applications but on
            presenting the core theory, tools and techniques of the
            calculus and how they fit together.</p>
            <p>As weâll see, however, the core concepts are broadly
            applicableâwell beyond the software domain where they
            originated.</p>
            <h3 data-number="1.1" id="the-pitch"><span
            class="header-section-number">1.1</span> The pitch</h3>
            <p>We introduce the simple but powerful concept of a
            <em>presence</em>.</p>
            <p>This lets us reason about the history and evolution of a
            set of <em>signals</em> that measure time-varying,
            path-dependent properties of <em>elements</em> that are
            present in defined <em>boundaries</em> in a domain using
            techniques from measure theory, topology and complex
            analysis.</p>
            <p>Classical statistics and probability theory often
            struggle here.</p>
            <p><em>History</em>âthe sequence and structure of changes in
            the domain over timeâ is usually fenced off under
            assumptions like ergodicity, stationarity, and
            independence.</p>
            <p>Our thesis is that to <em>complement</em> and
            <em>extend</em> statistical or probabilistic inference to
            reason effectively about global and long run behavior of
            many real world systems, especially those that arise
            commonly in software development, we need new analytical
            techniques that treat time and the history of signal
            interactions as first-class concepts we can model and
            calculate with.</p>
            <p>The Presence Calculus is a novel, <em>constructive</em>
            approach to this problemâan analytical framework for
            modeling <em>observed behavior</em> in systems ranging from
            simple, linear, and ordered to non-linear, stochastic,
            adaptive, and complex, all based on a small, uniform set of
            underlying concepts rooted in a mathematically precise
            primitive called <em>presence</em>.</p>
            <h3 data-number="1.2"
            id="learning-about-the-presence-calculus"><span
            class="header-section-number">1.2</span> Learning about the
            presence calculus</h3>
            <p>While the calculus was developed with mathematical rigor,
            an equally important goal was not to let mathematics get in
            the way of understanding the simple but very powerful ideas
            the calculus embodies.</p>
            <p>In this document, weâll motivate and introduce the
            intuitions and key ideas in the calculus with lots of
            evocative examples and mathematical simplifications to
            illustrate core concepts.</p>
            <p>In order to maintain precision, we dont shy away from
            mathematics where it is needed. We augment these with
            examples to build intuition throughout. However, given the
            nature of the material we have opted to stay on the side of
            rigor rather than dilute the concepts, even in this âgentleâ
            introduction.</p>
            <p>If you are inclined to skim over anything with
            mathematical notation in it, working through the examples
            alongside the math should be sufficient to grasp the key
            ideas and claims. However, for those who are comfortable
            with it, the mathematics should be easy to understand.</p>
            <p>The presence calculus is constructive - specifically
            everything in the calculus has a computational aspect, and
            this document is designed to lay the framework for
            describing how to perform those computations in a step by
            step manner.</p>
            <p>This means that it is best to read the sections in order,
            as each section builds on the concepts from the previous
            sections systematically. The concepts are not particularly
            difficult to grasp, but they will make more sense if you
            take time to understand how they fit together in the order
            they are presented here.</p>
            <p>So even though it is a gentle introduction, it is not the
            kind of introduction that you will get as much value from if
            you simply skim this document.</p>
            <blockquote>
            <p>If you are not into reading long technical documents and
            would like to listen to a podcast that explains the key
            ideas, you can find a remarkably astute summary that was
            produced by Googleâs NotebookLM <a
            href="./gentle_intro_audio_summary.html">here</a></p>
            </blockquote>
            <p>Listen to it (if you have 30 minutes) and then come back
            and read this document if it sounds interesting!</p>
            <p>If that deeper dive is not your cup of tea, weâll
            continue with ongoing informal exposition on our blog <a
            href="https://www.polaris-flow-dispatch.com">The Polaris
            Flow Dispatch</a>, where we will focus mostly on
            motivations, short pieces talking about specific areas, and
            applications of the concepts.</p>
            <p>If you are interested in working with the calculus, we
            recommend reading and understanding all the main ideas in
            this document before jumping deeper into the rest of the
            documentation at <a href="https://docs.pcalc.org">this
            site</a>, which get a fair bit more dense and technical.</p>
            <p>This document can be thought as the middle ground:
            detailed enough to understand the concepts and even
            implement and extend them yourself if you are so inclined,
            but just a starting point if you want to really dig deeper.
            The background and context from this document will be
            important if you want to apply the concepts from first
            principles and develop new applications.</p>
            <p>That next level of detail is in the API docs for <a
            href="http://docs.pcalc.org/api/pcalc.html">The Presence
            Calculus<br />
            Toolkit</a>.</p>
            <p>The toolkit is an open source reference implementation of
            the core concepts in the presence calculus. It is currently
            situated <a href="#fn1" class="footnote-ref" id="fnref1"
            role="doc-noteref"><sup>1</sup></a> as a middleware layer
            suitable for interfacing real world operational systems and
            complex system simulation, to the analytical machinery of
            the presence calculus.</p>
            <p>In the API documentation, we go into the concepts at a
            level of rigor that youâll need to compute and build
            applications based on the presence calculus.</p>
            <p>Finally, for those who want to dive deeper into the
            formal mathematical<br />
            underpinnings of the calculus, we have <a
            href="../theory_track.html">The Theory Track</a> These are
            terse, technical documents that go into more detail than the
            average practitioner will need to read or understand, but
            should be straightforward for a reader with a background in
            real analysis to clearly grasp the mathematical definitions
            and claims behind the calculus. In the footnotes and
            references, we link to specific documents in this track for
            deeper mathematical treatments of the concepts we discuss
            here.</p>
            <p>This document, and all related documentation can be found
            at the <a href="../index.html">Presence Calculus
            Documentation Site</a></p>
            <p>With all that as preamble, letâs jump inâ¦</p>
            <h2 data-number="2" id="why-presence"><span
            class="header-section-number">2</span> Why presence?</h2>
            <p>Presence is what we observe in the world.</p>
            <p>We dont experience reality as a sequence of discrete
            events in time, but as a continuous unfoldingâthings are
            present, or they appear, endure for a while, and then slip
            away.</p>
            <p>Permanence is simply a form of lasting presence. What we
            call <em>change</em> is the movement of presences into and
            out of awareness, often set against that permanence.</p>
            <p>The sense that something is presentâor no longer
            presentâis our most immediate way of detecting change. This
            applies equally to the tangibleâpeople, places, and
            thingsâand the intangibleâemotions, feelings, and
            experiences.</p>
            <p>Either way, reasoning about the presences and absences in
            our environment over time is key to understanding the
            dynamics of the world around us.</p>
            <p>The Presence Calculus begins here.</p>
            <p>Before we count, measure, compare, or optimize, we
            observe what <em>is</em>.</p>
            <p>And what we observe is presence.</p>
            <h3 data-number="2.1" id="an-example"><span
            class="header-section-number">2.1</span> An example</h3>
            <p>Imagine you see a dollar bill on the sidewalk on your way
            to get coffee.<br />
            Later, on your way back home, you see it againâstill lying
            in the same spot. It would be reasonable for you to assume
            that the dollar bill was present there the whole time.</p>
            <p>Of course, that may not be true. Someone might have
            picked it up in the<br />
            meantime, felt guilty, and quietly returned it. But in the
            absence of other<br />
            information, your assumption holds: it was there before, and
            itâs there now, so it must have been there in between.</p>
            <p>This simple act of inference is something we do all the
            time. We fill in gaps, assume continuity, and reason about
            what must have been present based on what we know from
            partial glimpses of the world.</p>
            <p>The presence calculus gives formal shape to this kind of
            inference about <em>things</em>, <em>places</em> and
            <em>time</em>âand shows how we can build upon it to
            <em>reason</em> about presence and <em>measure</em> its
            effects in an environment.</p>
            <h3 data-number="2.2" id="a-software-example"><span
            class="header-section-number">2.2</span> A software
            example</h3>
            <p>Since the ideas here emerged from the software world,
            letâs begin with a<br />
            mundane, but familiar example: task work in a software
            team.</p>
            <p>We usually reason about task work using <em>events</em>
            and <em>snapshots</em> of the state<br />
            of a process in time. A task âstartsâ when it enters
            development, and<br />
            âfinishesâ when itâs marked complete. We track âcycle timeâ
            by measuring the elapsed time between events, âthroughputâ
            by counting finish events, and â work-in-processâ by
            counting tasks that have started but not yet finished.</p>
            <p>When we look at a Kanban board, we see a point-in-time
            snapshot of where tasks are at that momentâbut not how they
            got there. And by the time we read a summary report of how
            many tasks were finished and how long they took to go
            through the process, much of the history of the system that
            produced those measurements has been lost. They become mere
            descriptive statistics about the system at a point in time.
            That makes it hard to reason about <em>why</em> those
            measurements are the way they are.</p>
            <p>In software development, workflows are path-dependent:
            each task often has a distinct historyâdifferent from other
            tasks present at the same time. Losing history makes it hard
            to reason about the interactions between tasks and how they
            impact the global behavior of the process.</p>
            <p>This problem is not unique to task work. Similar problems
            exist in almost all areas of business analysis that rely
            primarily on event models and descriptive statistics derived
            from events as the primary measurement tool for analyzing
            system behavior.</p>
            <p>We are reduced to trying to make inferences from local
            descriptive<br />
            statistics âthings like cycle times, throughput, and
            work-in-process levels- over highly irregular,
            path-dependent processes.</p>
            <p>We try to reason about a process which is shaped by its
            history, whose behavior emerges from non-uniform
            interactions of individual tasks that have impacts at
            different timescales, with measurement techniques that lack
            the ability to represent or reason about that history or the
            interactions.</p>
            <p>This is difficult to do, and we have no good tools right
            now that are fit for this purpose. So we either try to force
            fit our processes so that we can model and measure them more
            easily, or we simply make invalid inferences about them
            using the techniques that are not designed to operate
            accurately in these kinds of domains.</p>
            <p>This is where the presence calculus begins.</p>
            <p>By looking closely at how we reason about time varying
            quantities in the presence calculus, we can see how a subtle
            shift from an event-centered to a presence-centered
            perspective changes not just what we observe, but what we
            measure, and thus can reason about.</p>
            <p>In this particular case, the calculus focuses on the time
            <em>in between</em> snapshots of history: when a task was
            present, where it was present, for how long, and whether its
            presence shifted across people, tools, or systems.</p>
            <p>The connective tissue is no longer the task itself, or
            the process steps it<br />
            followed, or who was working on it, but a continuous,
            observable <em>thread of<br />
            presence</em>âthrough all of them, moving through time,
            interacting, crossing boundariesâa mathematical
            representation of history.</p>
            <p>With the presence calculus, these threads and their
            interactions across time and space can now be measured
            directly, dissected, composed, and analyzed as first-class
            constructsâbuilt on a remarkably simple yet general
            primitiveâthe presence.</p>
            <p>In the above example, the calculus exploits the
            difference between the two independent statementsââThe task
            started development on Mondayâ and âThe task completed
            development on Fridayââand a single, unified assertion: âThe
            task was present in development from Monday through
            Friday.â</p>
            <p>The latter is called a <em>presence</em>, and it is the
            foundational building block<br />
            of the calculus. At first glance, this might not seem like a
            meaningful difference.</p>
            <p>But treating the presence as the primary object of
            reasoningâas a <em>first-class</em> constructâopens up an
            entirely new space of possibilities.</p>
            <p>Specifically, it allows us to apply powerful mathematical
            tools that exploit the topology of time and the algebra of
            time intervals to reason about the interactions and
            configurations of presences in a rigorous and structured,
            and more importantly, computable way.</p>
            <h2 data-number="3" id="presence"><span
            class="header-section-number">3</span> What is a
            presence?</h2>
            <p>Letâs start by building intuition for the concept.
            Consider the statement: âThe task <span
            class="math inline">\(X\)</span> was in Development from
            Monday to Friday.â</p>
            <p>In the presence calculus, this would be expressed as a
            statement of the form: âThe element <span
            class="math inline">\(X\)</span> was in boundary <span
            class="math inline">\(Y\)</span> from <span
            class="math inline">\(t_0\)</span> to <span
            class="math inline">\(t_1\)</span> with mass 1.â</p>
            <p>Presences are statements about elements (from some
            domain) being present in a boundary (from a defined set of
            boundaries) over a <em>continuous</em> period of time,
            measured using some timescale.</p>
            <p>So why do we say âwith mass 1â?</p>
            <blockquote>
            <p>The presence calculus treats time as a physical
            dimension, much like space. Just as matter occupies space,
            presences occupy time. Just as mass quantifies <em>how</em>
            matter occupies space, the mass of a presence quantifies
            <em>how</em> a presence occupies time.</p>
            </blockquote>
            <p>The statement âThe task <span
            class="math inline">\(X\)</span> was in Development from
            Monday through Fridayâ is a <em>binary presence</em> with a
            uniform mass of 1 over the entire duration. The units of
            this mass are element-timeâin this case, task-days.</p>
            <p>Binary presences are sufficient to describe the
            <em>fact</em> of presence or absence<br />
            of things in places in a domain. These presences always have
            mass 1 in whatever units we use for elements and time.</p>
            <p>Typically, but not always, these represent the presence
            or absence of activity in a domain and can also be
            considered <em>activity signals</em>.</p>
            <h3 data-number="3.1"
            id="presence-mass-the-manifestation-of-presence"><span
            class="header-section-number">3.1</span> Presence mass: the
            manifestation of presence</h3>
            <p>Letâs consider a different set of statements:</p>
            <blockquote>
            <p>âTask <span class="math inline">\(X\)</span> had 2
            developers working on it from Monday to Wednesday,<br />
            3 developers on Thursday, and 1 developer on Friday.â</p>
            </blockquote>
            <p>These are no longer about just presence, but about the
            <em>effects</em> of presence.<br />
            They describe the <em>load</em> that task <span
            class="math inline">\(X\)</span> placed on the Development
            boundary<br />
            over time.</p>
            <p>The units of this presence are developer-days -
            potentially in a completely different dimension from the
            task, but grounded over the same time interval as the
            task.</p>
            <p>Here we are saying: âthe task being in this boundary over
            this time period, had this effect in a related
            dimension.â</p>
            <p>We will describe this using a presence with an arbitrary
            <em>real valued mass</em>. We will assume that there is some
            function, that computes this mass. In our example, letâs
            call this function <span
            class="math inline">\(\mathsf{load}.\)</span></p>
            <p>The presence can then be described as</p>
            <ul>
            <li><span class="math inline">\((\mathsf{load}, X,
            \text{Development}, \text{Monday}, \text{Wednesday},
            2)\)</span></li>
            <li><span class="math inline">\((\mathsf{load}, X,
            \text{Development}, \text{Thursday}, \text{Thursday},
            3)\)</span></li>
            <li><span class="math inline">\((\mathsf{load}, X,
            \text{Development}, \text{Friday}, \text{Friday},
            1)\)</span></li>
            </ul>
            <p>Here, <span class="math inline">\(\mathsf{load}(e, b,
            t)\)</span> is a time-varying function that takes an<br />
            element <span class="math inline">\(e\)</span>, a boundary
            <span class="math inline">\(b\)</span>, and a time <span
            class="math inline">\(t\)</span>, and returns a real-valued
            number<br />
            describing how much presence is concentrated at that point
            in time.</p>
            <p>The <em>presence mass</em> of such a presence is the
            total presence over the<br />
            interval <span class="math inline">\([t_0, t_1]\)</span>,
            defined as:</p>
            <p><span class="math display">\[ \text{mass} =
            \int_{t_0}^{t_1} \mathsf{load}(e, b, t)\, dt \]</span></p>
            <p>where <span class="math inline">\(\mathsf{load}\)</span>
            is called a <em>presence density function</em> <a
            href="#fn2" class="footnote-ref" id="fnref2"
            role="doc-noteref"><sup>2</sup></a>.</p>
            <blockquote>
            <p>Presence mass is composite that measures some real valued
            quantity and the amount of time it was present. Think of it
            as measuring an <em>area</em> in a two-dimensional space
            with time as one of the dimensions.</p>
            </blockquote>
            <p>Binary presences are much easier to understand
            intuitively, but the real power of the presence calculus
            comes from generalizing to <em>presence density
            functions</em> with arbitrary mass.</p>
            <h3 data-number="3.2"
            id="presence-density-functions-aka-signals"><span
            class="header-section-number">3.2</span> Presence density
            functions <em>aka</em> Signals</h3>
            <figure id="fig:presence-definition">
            <img src="../assets/pandoc/presence_definition.png"
            alt="Figure 2: Signals, Presence and Presence Mass" />
            <figcaption aria-hidden="true">Figure 2: Signals, Presence
            and Presence Mass</figcaption>
            </figure>
            <p>We can extrapolate from the example of the load function
            and think of defining a presence over an arbitrary time
            varying function with real numbers as values. We will call
            these <em>presence density functions.</em></p>
            <p>Such functions represent an underlying <em>signal</em>
            from the domain that we are interested in measuring. In what
            follows, we will use the terms signal and presence density
            functions interchangeably, opting for the latter only those
            cases where we want to focus specifically on the fact that
            what we are representing about the signal is the âamountâ of
            the signal (its presence) over time.</p>
            <p>As shown in FigureÂ 2, the mass of a presence density
            function, over any given time <em>interval</em> <span
            class="math inline">\([t_0, t1)\)</span> is the <em>integral
            of the function</em> over the interval, which is also the
            area under the signal over that interval<a href="#fn3"
            class="footnote-ref" id="fnref3"
            role="doc-noteref"><sup>3</sup></a>.</p>
            <p>The only requirement for a function to be a presence
            density function (signal) is that it is <em>measurable</em>,
            and that you can interpret <em>presence mass</em>âdefined as
            the integral of the function over a finite intervalâas a
            meaningful <em>measure</em> of the effect of presence in
            your domain.</p>
            <p>This is where measure theory enters the picture. Itâs not
            essential to understand the full technical details, but at
            its core, measure theory tells us which kinds of functions
            are measurableâin other words, which functions can support
            meaningful accumulation, comparison, and composition of
            values via <em>integration</em>.</p>
            <p>When a presence density functions (signal) is measurable,
            it gives us the confidence to do things like compute
            statistics, aggregate over elements or boundaries, and
            compose presence effectsâwhile preserving the semantics of
            the domain.</p>
            <p>From our perspective, a presence density function is a
            domain signal whose value that can be <em>accumulated</em>
            across time and across presences.</p>
            <p>This lets us reason mathematically about presences with
            confidenceâand since most of this reasoning will be
            performed by algorithms, we need technical constraints that
            ensure those calculations are both mathematically valid and
            semantically sound.</p>
            <p>For a mathematically precise definition of these concepts
            please see our theory track document <a
            href="./presence_calculus_foundations.html">Presence: A
            measure-theoretic-definition</a>.</p>
            <h3 data-number="3.3" id="more-examples"><span
            class="header-section-number">3.3</span> More examples</h3>
            <p>Letâs firm up our intuition about what signals in the
            presence calculus can describe with a few more examples of
            presence density functions.</p>
            <h4 data-number="3.3.1" id="work-in-software"><span
            class="header-section-number">3.3.1</span> âWorkâ in
            software</h4>
            <p>If youâve ever written a line of code in your life,
            youâve heard the question:<br />
            âWhen will it be done?â Work in software can be a slippery,
            fungible conceptâ and the presence calculus offers a useful
            way to describe it.</p>
            <p>We can express the work on a task using a presence
            density function whose value at time <span
            class="math inline">\(t\)</span> is the <em>remaining</em>
            work on the task at <span
            class="math inline">\(t\)</span>.</p>
            <p>This lets us model tasks whose duration is uncertain in
            general, but whose<br />
            remaining duration can be estimated, subject to revision, at
            any given timeâa common scenario in software contexts.</p>
            <p>A series of presences, where the (non-zero) mass of each
            presence corresponds to the total remaining work over its
            interval (interpreting the integral as a sum), gives us a
            way to represent <em>work as presence</em>.</p>
            <p>Such presences can represent estimates, forecasts, or
            confidence-weighted<br />
            projectionsâand as weâll see, they can be reasoned about and
            computed with just like any other kind of presence.</p>
            <h4 data-number="3.3.2" id="batch-size-in-software"><span
            class="header-section-number">3.3.2</span> âBatch sizeâ in
            software</h4>
            <p>Letâs consider another, more familiar example. There are
            well-understood benefits to delivering software more
            frequently, in smaller batches, and modern software
            engineering practices encourage teams to adopt this delivery
            model instead of relying on infrequent, large batch
            deliverables.</p>
            <p>For companies with legacy processes built around large
            batch delivery, and looking to measure progress on the
            journey toward smaller batches, the state-of-the-art metrics
            used to guide the transition are <em>deployment
            frequency</em> and <em>lead time for changes</em> â two of
            the four key DORA metrics considered gold standards in the
            industry <span class="citation" data-cites="forsgren2018"><a
            href="#ref-forsgren2018"
            role="doc-biblioref">[1]</a></span>.</p>
            <p>However, both deployment frequency and lead time for
            changes are <em>proxies</em> for batch size. And like all
            proxies, they are imperfect measurements of the true
            quantity of interest, often providing little insight into
            the <em>causal drivers</em> that shape the proxies
            themselves.</p>
            <p>With the presence calculus, we have a different way to
            model this problem.<br />
            We can define a system of presence density functions,
            where:</p>
            <ul>
            <li><em>Elements</em> are code branches,</li>
            <li><em>Boundaries</em> are stages along the path to
            production, and</li>
            <li>The <em>presence density function</em> of each branch is
            the number of unmerged lines of code on the branch as a
            function of time.</li>
            </ul>
            <p>Under this model, <em>batch size</em> becomes the total
            number of unmerged lines of code across the system at any
            given point in time â a quantity that is directly measurable
            using the machinery of presence calculus.</p>
            <p>With modern tooling, it is entirely feasible to
            instrument such a model in a real-world delivery system and
            keep it updated in real time.</p>
            <p>This kind of real-time, direct model has several
            important advantages over measuring the proxy metrics
            deployment frequency and lead time for changes after the
            fact.</p>
            <p>Here are some benefits of this approach <a href="#fn4"
            class="footnote-ref" id="fnref4"
            role="doc-noteref"><sup>4</sup></a>:</p>
            <ul>
            <li>It directly represents and measures the variable of
            interest, batch size - rather than proxies - in real
            time.</li>
            <li>Deployment frequency and lead time for changes become
            <em>derived values</em>, computable directly from the system
            using the calculus. Still useful for metrics and reporting
            but not the focus of improvement.</li>
            <li>It gives us access to the <em>input drivers</em> of
            batch size, enabling interventions at the cause rather than
            the statistical output.</li>
            <li>It allows us to <em>trace batch-size behavior</em> to
            both individual components (branches, repositories,
            developers, etc.) and to <em>emergent effects</em> from
            interactions over time.</li>
            <li>It gives us both <em>leading and lagging
            indicators</em>, and allows us to reason about the
            <em>dynamics that connect them</em>, all within a
            mathematically coherent, real-time model.</li>
            </ul>
            <p>These are all substantial improvements over the status
            quo measurement techniques in DevOps and it all falls out of
            the representation and computation machinery of the presence
            calculus we describe in this document. This also means the
            presence calculus gives an precise mathematical model that
            lets us directly <em>observe</em> the relationship between
            batch size and the two DORA metrics in any given delivery
            context in real time.</p>
            <h4 data-number="3.3.3"
            id="the-effects-of-interruptions"><span
            class="header-section-number">3.3.3</span> The effects of
            interruptions</h4>
            <p>Another useful example from the software world
            illustrates a different<br />
            application of a presence. Letâs assume the boundary in this
            case is a<br />
            developer, the element is an interruption (defined
            appropriately in the<br />
            domain), and the presence density function captures the
            <em>context switching<br />
            cost</em>âmeasured in lost timeâassociated with that
            interruption.</p>
            <p>The key insight here is that the <em>effects</em> of the
            presence extend beyond the<br />
            interval of the interruption itself. This is a classic case
            of a delayed or<br />
            <em>decaying effect</em>âa pattern that appears frequently
            in real-world systems.</p>
            <p>The presence density function can be modeled in different
            ways:</p>
            <ul>
            <li>As a constant cost: for example, each interruption
            causes a fixed<br />
            15-minute recovery period, regardless of its duration.</li>
            <li>As a decaying function: the cost is highest at the
            moment of interruption<br />
            and gradually decreases to zero over a defined recovery
            window (e.g.<br />
            15 minutes), representing a return to full focus.</li>
            </ul>
            <p>This approach gives us a precise way to model and reason
            about the<br />
            <em>aftereffects</em> of eventsâeffects that outlast the
            events themselves and<br />
            accumulate in subtle but measurable ways over longer
            timeframes.</p>
            <p>In this case, we measured an effect that decayed from a
            peak, but a similar<br />
            approach can be taken, for example, with a presence density
            function that<br />
            grows from zero and plateaus over the duration of the
            presenceâsuch as the net increase in revenue due to a
            released feature.</p>
            <p>Used this way, presence density functions give us a
            powerful tool for modeling the impact of binary
            presencesâcapturing their downstream or distributed effects
            over time, and reasoning about their relationship over a
            shared timeline.</p>
            <p>Another important use case in the same vein is modeling
            the cost of delay for a portfolio-level elementâand
            analyzing its cascading impact across a portfolio.</p>
            <p>These use cases show that it is possible to analyze not
            just binary presences, but entire chains of influence they
            exert across a timelineâa key prerequisite for causal
            reasoning.</p>
            <h4 data-number="3.3.4"
            id="self-reported-developer-productivity"><span
            class="header-section-number">3.3.4</span> Self-reported
            developer productivity</h4>
            <p>Imagine a developer filling out a simple daily check-in:
            âHow productive did you feel today?ââscored from 1 to 5, or
            sketched out as a rough curve over the day<a href="#fn5"
            class="footnote-ref" id="fnref5"
            role="doc-noteref"><sup>5</sup></a>.</p>
            <p>Over a week, this forms a presence density functionânot
            of the developer in a place, but of their <em>sense</em> of
            productivity over time.</p>
            <p>These types of presences, representing perceptions, are
            powerfulâhelping<br />
            teams track experience, spot early signs of burnout, or
            correlate perceived<br />
            productivity with meetings, environment changes, build
            failures, or interruptions.</p>
            <p>Now, letâs look at some examples outside software
            development.</p>
            <h4 data-number="3.3.5"
            id="browsing-behavior-on-an-e-commerce-site"><span
            class="header-section-number">3.3.5</span> Browsing behavior
            on an e-commerce site</h4>
            <p>Imagine a shopper visiting an online store. They spend 90
            seconds browsing kitchen gadgets, then linger for five full
            minutes comparing high-end headphones, before briefly
            glancing at a discounted blender.</p>
            <p>Each of these interactions can be modeled as a presence:
            the shopperâs<br />
            (element) attention occupying different parts of the site
            (boundaries) over<br />
            time. The varying durations reflect interest, and the
            shifting presence reveals patterns of engagement in a
            population of visitors in a boundary (an area of the
            site).</p>
            <p>By analyzing these presences across visitors to the site
            âwhere and for how long attention dwellsâwe can begin to
            understand population level preferences, intent, and even
            the likelihood of conversion (modeled as a different
            presence density function).</p>
            <h4 data-number="3.3.6"
            id="patient-movement-in-a-hospital"><span
            class="header-section-number">3.3.6</span> Patient movement
            in a hospital</h4>
            <p>Consider a patient navigating a hospital stay. They spend
            the morning in<br />
            radiology, move to a recovery ward for several hours, then
            are briefly<br />
            transferred to the ICU overnight.</p>
            <p>Each location records a presenceâwhen and where the
            patient was, and for how long. These presences can reveal
            bottlenecks, resource utilization, and potential risks.</p>
            <p>Over time, analyzing patient presences helps surface
            patterns in care<br />
            delivery, delays in treatment, and opportunities for
            improving patient flow.</p>
            <p>These are examples of classic operations management
            problems expressed in the language of the presence calculus.
            The calculus is well-suited to modeling scenarios like these
            as a base case.</p>
            <h3 data-number="3.4" id="presence-a-summary"><span
            class="header-section-number">3.4</span> Presence: a
            summary</h3>
            <p>Letâs summarize what weâve described so far.</p>
            <p>With signals and presences, we now have a framework for
            describing and measuring the behavior of time-varying domain
            signalsâeach representing how a specific element behaves
            within a given boundary.</p>
            <p>The key feature of a presence is that it abstracts these
            behaviors into a uniform representationâone we can reason
            about and compute with.</p>
            <p>Formally, a general presence is defined by:</p>
            <ul>
            <li>a presence density function (or signal) <span
            class="math inline">\(f(e, b, t)\)</span>,</li>
            <li>an element <span class="math inline">\(e\)</span>,</li>
            <li>a boundary <span class="math inline">\(b\)</span>,</li>
            <li>and a time interval <span class="math inline">\([t_0,
            t_1]\)</span>.</li>
            </ul>
            <p>Its <em>mass</em> is the integral of <span
            class="math inline">\(f\)</span> over the interval:</p>
            <p><span class="math display">\[ \text{mass}(e, b, [t_0,
            t_1]) = \int_{t_0}^{t_1} f(e, b, t)\, dt \]</span></p>
            <p>This mass captures both <em>that</em> the element was
            present, and <em>how</em> it was<br />
            presentâuniformly, variably, or intermittentlyâover the
            interval.</p>
            <p>Intuitively, you can think of <em>integration</em> as the
            mathematical process by which we construct a continuous
            temporal model from discrete events in a domain.</p>
            <p>Sensors in the real world often generate both discrete
            event streams and continuous signals. Modeling all signals
            uniformly as presences with temporal mass is the first step
            toward analyzing interactions and dynamics <em>across</em>
            heterogeneous signals within a domain.</p>
            <h2 data-number="4" id="systems-of-presence"><span
            class="header-section-number">4</span> Systems of
            presence</h2>
            <p>In this section, we move from individual presence density
            functions to systems of signalsâeach capturing the presence
            behavior of multiple elements across multiple boundaries
            within a domain.</p>
            <p>A core aspect of the Presence Calculus is its treatment
            of fine-grained signals: each domain element is associated
            with its own presence density function, reflecting its
            path-dependent trajectory through one or more boundaries in
            the system.</p>
            <p>We study how the presence masses of these signals
            interact over time to produce <em>cumulative</em>
            effectsâobservable outcomes that carry semantic meaning in
            the domain.</p>
            <p>Each signal represents the <em>continuous</em> presence
            behavior of a specific element within a boundary over time.
            The calculus emphasizes that each signal traces a distinct
            trajectory, and seeks to explain how their interactions over
            shared intervals shape system-level behavior.</p>
            <figure id="fig:pdf-examples">
            <img src="../assets/pandoc/pdf_examples.png"
            alt="Figure 3: A System of Presences" />
            <figcaption aria-hidden="true">Figure 3: A System of
            Presences</figcaption>
            </figure>
            <p>From the standpoint of the calculus itself, it does not
            matter what the signals representâwe treat them uniformly as
            real-valued presence masses and define a standard set of
            mathematical operations over them.</p>
            <p>From the standpoint of semantics, however, the
            <em>meaningfulness</em> of these operations depends on the
            domain. What it means to combine signals, which ones to
            combine, and how to interpret the resultâall of this is
            context-dependent and crucial for producing useful
            insights.</p>
            <p>This aspectâmodeling and model developmentâis essential
            for applying the machinery effectively. However, our focus
            here is on the calculus itself: what it can say in general
            about arbitrary systems of signals and presence density
            functions that interact in time.</p>
            <p>In what follows, we will treat signals and presence
            density functions as abstract mathematical objects and
            describe the properties and operations of such systems. We
            will build some domain-specific intuition through examples,
            but will not focus on applications.</p>
            <p>Those will be the subject of future postsâan area that is
            both rich and complex, and which will depend heavily on the
            foundational machinery developed in this document.</p>
            <h3 data-number="4.1"
            id="presence-as-a-sample-of-a-signal"><span
            class="header-section-number">4.1</span> Presence as a
            sample of a signal</h3>
            <p>A signal, in general, describes the continuous behavior
            of an element within a boundary over time. This continuous
            signal may have one or more disjoint periods where its value
            is non-zero. These non-zero periods are called the
            <em>support</em> of the signal.</p>
            <p>As we see in FigureÂ 4, a single signal (for a given
            element and boundary) might have multiple support intervals.
            These may correspond to episodic behavior in the underlying
            domain, for example, user sessions in an e-commerce context,
            or rework loops in software development when a task
            âreturnsâ to development many times over its lifecycle.</p>
            <figure id="fig:multiple-support">
            <img src="../assets/pandoc/multiple_support.png"
            alt="Figure 4: A presence mass as a sample of a signal over an interval" />
            <figcaption aria-hidden="true">Figure 4: A presence mass as
            a sample of a signal over an interval</figcaption>
            </figure>
            <p>A <em>presence</em> (the 5-tuple <span
            class="math inline">\(p = (e, b, t_0, t_1, m)\)</span> that
            we work with in the calculus) is generated by taking an
            <em>observation</em> of this underlying signal over a
            specific time interval <span class="math inline">\([t_0,
            t_1)\)</span>, and computing its mass. This interval can be
            chosen in many ways:</p>
            <ul>
            <li>It might perfectly align with a single support interval
            (a âhillâ in the signal).</li>
            <li>It might span multiple disjoint support intervals,
            including the âzeroâ regions in between.</li>
            <li>It might capture only a portion of a single
            support.</li>
            </ul>
            <p>All we require is that the interval chosen for the
            presence calculation intersects a region of non-zero area
            from the signal that can be reduced to a non-zero presence
            mass.</p>
            <h3 data-number="4.2" id="presence-assertions"><span
            class="header-section-number">4.2</span> Presence
            assertions</h3>
            <p>Thus presence is best thought of as a <em>sampled
            measurement</em> of the underlying signal, taken by an
            <em>observer</em> over a specific time interval, which
            yields a non-zero mass for that interval.</p>
            <p>A given observer may not âseeâ the underlying signalâonly
            the <em>mass</em> of the presence they sampled over an
            observation window.</p>
            <p>Different observers may observe different intervals of
            the same signal and derive different presence masses for the
            same signal.</p>
            <p>So here is the first key assumption of the presence
            calculus:</p>
            <blockquote>
            <p>A system of presences refers to a collection of discrete
            observations drawn from an underlying set of presence
            density functions. In general, we do not assume we have
            access to the âtrueâ underlying functions. Instead, we
            reason about these functions based sampled presence masses
            and treat these presences as the basis of representation and
            inference within the calculus.</p>
            </blockquote>
            <p>This brings us to the concept of <em>presence
            assertions</em>, which formalize the act of recording a
            presence based on an observerâs local âviewâ of the
            underlying density function.</p>
            <p>A <em>presence assertion</em> is simply a presence as
            defined in the previous section augmented with additional
            attributes:</p>
            <ul>
            <li><em>who</em> the observer was</li>
            <li>and an <em>assertion timestamp</em>âthe time at which
            the observation was made.</li>
            </ul>
            <h4 data-number="4.2.1" id="the-open-world-assumption"><span
            class="header-section-number">4.2.1</span> The open world
            assumption</h4>
            <p>Time in the presence calculus is explicitly defined to be
            over <em>extended</em> reals <span
            class="math inline">\(\overline{\mathbb{R}}\)</span>: the
            real line <span class="math inline">\(\mathbb{R}\)</span>
            extended with the symbols <span
            class="math inline">\(-\infty\)</span> and <span
            class="math inline">\(+\infty\)</span>.</p>
            <p>This is a mathematical representation of an open world
            assumption, which holds that the history of a system of
            presences extends indefinitely into the past and future.</p>
            <p>An observer will typically only see a finite portion of
            this history and has to make inferences on the basis of
            those observations, but in general, we need to make
            inferences with partial information about the past and
            contingent assumptions about the future.</p>
            <p>A presence with <span class="math inline">\(t_0 =
            -\infty\)</span> represents a presence whose beginning is
            unknown, and <span class="math inline">\(t_1 =
            +\infty\)</span> represents a presence whose end is
            unknown.</p>
            <p>Presences with both start and end unknown are valid
            constructs and represent eternal presences.</p>
            <p>Many of the most interesting questions in the presence
            calculus involve reasoning about the signal dynamics of a
            domain under the epistemic uncertainty introduced by such
            presences.</p>
            <p>Further we will note that the assertion time in a
            presence assertion doesnât need to align with the time
            interval of the presence. This allows assertions to refer to
            the past, reflect the present, or even anticipate the future
            behavior of a signal.</p>
            <h4 data-number="4.2.2" id="a-note-on-epistemology"><span
            class="header-section-number">4.2.2</span> A note on
            epistemology</h4>
            <p>Presence assertions give us the ability to assign
            <em>provenance</em> to a presenceâ not just <em>what</em> we
            know, but <em>how</em> we know it. This is essential
            in<br />
            representing contexts where the observer and the act of
            observation are first-class concerns.</p>
            <p>Further, since reasoning about time and history is a
            primary focus of the calculus, presences with unknown
            beginnings or endings provide a way to explicitly model what
            is knownâand unknownâabout that history. This will prove
            more valuable than it might initially seem.</p>
            <p>We wonât go too deeply into the epistemological aspects
            of the presence<br />
            calculus in this documentâthis remains an active and open
            area of research and development and complements much of
            what we discuss here.</p>
            <p>But itâs important to acknowledge that this layer exists,
            that modeling and interpreting the output of the presence
            calculus requires an explicit treatment of how observations
            are made and by whom, and the fact that this has a huge
            impact on the validity of the inferences one makes using the
            machinery of the calculus.</p>
            <p>With this caveat in place, once weâve represented a
            problem domain as a<br />
            <em>system of presences</em>, much of the machinery of the
            presence calculus (which weâll introduce in the next few
            sections) can be applied uniformly.</p>
            <h4 data-number="4.2.3" id="a-note-on-path-dependence"><span
            class="header-section-number">4.2.3</span> A note on path
            dependence</h4>
            <p>By representing a presence at the granularity of an
            element in a boundary, we explicitly recognize the path
            dependent nature of the domain signals.</p>
            <p>In the presence calculus, signals represent some behavior
            of domain elements in boundaries. The calculus itself is
            agnostic to what counts as an âelementâ or a âboundaryââ
            this is a domain modeling decision.</p>
            <p>Even if they represent the same underlying quantity, we
            recognize that system behavior emerges from the interactions
            between <em>individual</em> signals at the element-boundary
            granularityâeach potentially with distinct presence density
            functions. We are interested in studying <em>how</em>
            system-level behavior arises from the <em>interactions</em>
            between these signals over time.</p>
            <p>Modeling boundaries are crucial because, for a given
            domain element, the boundary typically determines both which
            signals are relevant and how we wish to analyze system
            behavior using them.</p>
            <p>For example, a software feature (an element) may be
            modeled using one set of signals during development (one
            boundary), another during production (a second boundary),
            and yet another when customers begin using it (a third
            boundary).</p>
            <p>All three signals are part of the featureâs history. Each
            feature follows a unique path through these boundaries,
            producing its own independent set of signals with a distinct
            history and evolution. These signals interact in complex
            waysâover time, within boundaries, and with each other. The
            boundary is what brings coherence to the analysisâit defines
            which signals and interactions we choose to focus on, and
            why.</p>
            <blockquote>
            <p>The boundary allows us to bring a coherent set of
            elements and related signals together for analysis. That
            analysis focuses on how these signals interact in
            <em>time</em>.</p>
            </blockquote>
            <p>Constructing an appropriate set of element-boundary
            signals is <em>the</em> key modeling decision. But once
            these are defined, much of the machinery of the presence
            calculus can be applied without regard to the semantics of
            the specific element-boundaries involved.</p>
            <p>Just as important as modeling time, is the explicit
            modeling of timescales. In software development the effects
            of presence often manifest across boundaries and across
            timescales.</p>
            <p>We will see how the machinery of the presence calculus
            enables us to reason precisely and
            <em>deterministically</em> about presence at different
            timescales.</p>
            <p>Semantics are, of course, crucial in
            <em>interpreting</em> the inferences one draws<br />
            using the machinery of the calculus.</p>
            <blockquote>
            <p>When we refer to a âsystemâ in the presence calculus we
            are explicitly defining it as an <em>evolving</em> set of
            presence assertionsâi.e., the âsystemâ is what we can assert
            about a domain using presence assertions at a given
            time.</p>
            </blockquote>
            <p>In summary, this is what we refer to as a âsystem of
            presencesâ - a time-indexed collection of presence
            assertions derived from an underlying set of path dependent
            element-boundary signals.</p>
            <h2 data-number="5" id="presence-invariant"><span
            class="header-section-number">5</span> Co-presence and the
            presence invariant</h2>
            <p>FigureÂ 5 illustrates an example of a system of presences,
            where we focus on the subset of presences observed over a
            <em>common interval</em>.</p>
            <figure id="fig:presence-invariant-continuous">
            <img
            src="../assets/pandoc/presence_invariant_continuous.png"
            alt="Figure 5: Co-Presence and The Presence Invariant" />
            <figcaption aria-hidden="true">Figure 5: Co-Presence and The
            Presence Invariant</figcaption>
            </figure>
            <p>These presences are called <em>co-present</em>âthey
            represent an observer making simultaneous measurements of
            presence mass across multiple signals over a common interval
            of time.</p>
            <p>Co-presence is a necessary (but not sufficient) condition
            for interaction between one or more signals. This section
            introduces a key construct: the presence invariant. It
            expresses a general and powerful relationship that holds for
            any co-present subset of presences within a finite
            observation window, and it is a fundamental relationship
            that governs how the masses of co-present signals
            interact.</p>
            <p>Letâs establish this relationship.</p>
            <p>Given, any finite observation interval, weâve already
            shown that each presence density function has a <em>presence
            mass</em>, defined as the integral of the density over the
            observation interval.</p>
            <p>These can be thought of as mass <em>contributions</em>
            from those presences to that interval. The sum of these
            individual mass contributions gives the total presence mass
            observed across the system in that interval.</p>
            <p>In our example from FigureÂ 5,</p>
            <p><span class="math display">\[ A = M_0 + M_1 + M_3
            \]</span></p>
            <p>is the cumulative mass contribution from the signals that
            have non-zero mass over the interval <span
            class="math inline">\([t_0, t_1)\)</span>. The length of
            this interval is <span class="math inline">\(T = t_1 -
            t_0\)</span>.</p>
            <p>Let <span class="math inline">\(N\)</span> be the number
            of <em>active signals</em>: distinct signals with a presence
            in the observation window<a href="#fn6" class="footnote-ref"
            id="fnref6" role="doc-noteref"><sup>6</sup></a>.</p>
            <p>Now letâs consider the quantity <span
            class="math display">\[
            \delta = \frac{A}{T}
            \]</span></p>
            <p>Since the mass comes from integrating a density function
            over time, the quantity <span
            class="math inline">\(\frac{A}{T}\)</span> represents the
            <em>presence density</em> over the observation interval
            <span class="math inline">\(T\)</span> <a href="#fn7"
            class="footnote-ref" id="fnref7"
            role="doc-noteref"><sup>7</sup></a>.</p>
            <p>We can now decompose this as:</p>
            <p><span class="math display">\[ \delta = \frac{A}{T} =
            \frac{A}{N} \times \frac{N}{T} \]</span></p>
            <p>This separates the presence density into two
            interpretable components:</p>
            <ul>
            <li><span class="math inline">\(\bar{m} =
            \frac{A}{N}\)</span>: the <em>mass contribution</em> per
            active signal,</li>
            <li><span class="math inline">\(\iota =
            \frac{N}{T}\)</span>: the signal <em>incidence
            rate</em>âi.e., the number of active signals per unit
            time.</li>
            </ul>
            <p>This leads to the <em>presence invariant</em>:</p>
            <p><span class="math display">\[ \text{Presence Density} =
            \text{Signal Incidence Rate} \times \text{Mass Contribution
            per Signal} \]</span> or in our notation</p>
            <p><span class="math display">\[ \delta = \iota \cdot
            \bar{m} \]</span></p>
            <p>This identity holds for <em>any</em> co-present subset of
            signals over <em>any</em> finite time interval.</p>
            <blockquote>
            <p>The key insight here is that the presence invariant
            establishes a fundamental relationship between the
            individual mass contributions of signals and their
            cumulative, observable effect over a shared time
            intervalâthat is, the presence density they induce together
            over time.</p>
            </blockquote>
            <p>That this identity holds for <em>any</em> co-present
            subset over <em>any</em> finite observation window makes it
            a powerful constraint. It connects local behaviors of
            individual signals to a global, observable quantity <a
            href="#fn8" class="footnote-ref" id="fnref8"
            role="doc-noteref"><sup>8</sup></a>.</p>
            <p>As weâll see, this constraintâlinking local contributions
            to global measurable structureâis central to how the
            calculus enables reasoning about the dynamics of a system of
            presences.</p>
            <h3 data-number="5.1" id="an-example-1"><span
            class="header-section-number">5.1</span> An example</h3>
            <p>To build intuition for these abstract terms, letâs look
            at a practical example.</p>
            <p>For example, suppose our signals represent revenues from
            customer purchase over some time period. If we look at a
            system of presences, across an interval of time, say a week,
            the total presence mass <span
            class="math inline">\(A\)</span> represents the total
            revenues across all customers who contributed to that
            revenue. <span class="math inline">\(T\)</span> is the time
            period measured in some unit of time (say days) and <span
            class="math inline">\(N\)</span> is the number of paying
            customers in that period.</p>
            <p>The presence density is the daily revenue rate, the
            signal mass contribution for each signal is revenue for each
            customer, the signal mass contribution is the revenue per
            customer for that week, and the incidence rate represents
            the daily rate of active customers over the week.</p>
            <p>So the presence invariant is stating that the revenue
            rate for the week is the product of the revenue per customer
            and the number of active customers over the week.</p>
            <h3 data-number="5.2" id="why-it-matters"><span
            class="header-section-number">5.2</span> Why it matters</h3>
            <p>While algebraically, the presence invariant is a
            tautology, it imposes a powerful constraint on system
            behaviorâone that is independent of the specific system,
            semantics, or timescale. Think of it as the generalization
            of our intuitive revenue example to any arbitrary system of
            presences.</p>
            <p>The presence invariant is a foundational conservation law
            of the presence calculus: the <em>conservation of mass
            (contribution)</em>.</p>
            <p>Just as the conservation of energy or mass constrains the
            evolution of physical systemsâregardless of the specific
            materials or forces involvedâthe conservation of presence
            mass constrains how observable mass is distributed over time
            in a system of presences.</p>
            <p>The version of the invariant described here is a special
            case of an even more general measure theoretic construction
            that we derive in the document <a
            href="./generalized_presence_invariant.html">The Presence
            Invariant</a>. We show there that the presence invariant is
            not just a mathematical trick, but a deep property of
            product measure spaces.</p>
            <p>Thus, the conservation of presence mass plays a role in
            the presence calculus similar to that of other conservation
            laws in physics: it constrains the behavior of three key
            observable, measurable parameters of any system of
            presences.</p>
            <p>While independent of the semantics of what is being
            observed, like energy, presence mass can shift, accumulate,
            or redistribute, but its total balance across presences
            within a finite time interval remains invariant.</p>
            <p>Exploiting this constraint allows us to study and
            characterize the long-run behavior of a system.</p>
            <h3 data-number="5.3"
            id="binary-presences-and-littles-law"><span
            class="header-section-number">5.3</span> Binary presences
            and Littleâs Law</h3>
            <p>Letâs make things a bit more concrete by interpreting
            this identity in the special case of <em>binary</em>
            presences.</p>
            <p>Recall that a <em>binary</em> signal is a function whose
            density is either <span class="math inline">\(0\)</span> or
            <span class="math inline">\(1\)</span>. That is, we are
            modeling the presence or absence of an underlying signal in
            the domain.</p>
            <p>In this case, the <em>mass contribution</em> of a signal
            becomes an <em>element-time duration</em>. For example, if
            the signal represents the time during which a task is
            present in development, the mass contribution of that task
            over an observation interval is the portion of its duration
            that intersects the interval. This is also called the
            <em>residence time</em> <a href="#fn9" class="footnote-ref"
            id="fnref9" role="doc-noteref"><sup>9</sup></a> for the task
            in the observation window.</p>
            <figure id="fig:presence-invariant-binary">
            <img src="../assets/pandoc/presence_invariant_binary.png"
            alt="Figure 6: The Presence Invariant for binary signals" />
            <figcaption aria-hidden="true">Figure 6: The Presence
            Invariant for binary signals</figcaption>
            </figure>
            <p>FigureÂ 6 shows possible configurations of binary signals
            intersecting a finite observation interval. Suppose the unit
            of time is days.</p>
            <p>The total presence mass accumulation <span
            class="math inline">\(A\)</span> is <span
            class="math inline">\(11\)</span> task-days. The number
            <span class="math inline">\(N\)</span> of tasks that
            intersect the observation interval is <span
            class="math inline">\(4\)</span>. The length of the
            observation window is <span class="math inline">\(T =
            4\)</span> days. It is straightforward to verify that the
            presence invariant holds.</p>
            <p>Now, letâs interpret its meaning.</p>
            <p>Since each task contributes <span
            class="math inline">\(1\)</span> unit of mass for each unit
            of time it is present, the presence density <span
            class="math inline">\(\delta=\frac{A}{T}\)</span> represents
            the * number of tasks* present per unit time in the
            intervalâdenoted <span class="math inline">\(L\)</span>.</p>
            <p>Conversely, since each unit of mass corresponds to a unit
            of time associated with a task, the mass per active signal,
            <span class="math inline">\(\bar{m} = \frac{A}{N}\)</span>,
            is the time a task spends in the observation window. This
            value is typically called the <em>residence time</em> <span
            class="math inline">\(w\)</span> of a task in the
            observation window.</p>
            <p>The incidence rate <span class="math inline">\(\iota =
            \frac{N}{T}\)</span> may be interpreted as the
            <em>activation rate</em> of tasks in the intervalâa proxy
            for the rate at which tasks start (onset) or finish (reset)
            within the window.</p>
            <p>For example, <span class="math inline">\(N\)</span> may
            be counted as the number of tasks that start inside the
            interval, plus the number that started before but are still
            active. Thus, <span
            class="math inline">\(\frac{N}{T}\)</span> is a
            <em>cumulative onset rate</em> <span
            class="math inline">\(\Lambda\)</span>.</p>
            <p>The presence invariant can now be rewritten as:</p>
            <p><span class="math display">\[ L = \Lambda \times w
            \]</span></p>
            <p>which you may recognize as <em>Littleâs Law</em> in its
            finite-window form.</p>
            <p>Thus, the presence invariant serves as a
            <em>generalization of Littleâs Law</em>âextending it to
            arbitrary systems of presence density functions (signals)
            measured over finite observation windows.</p>
            <p>It is important to note that we are referring to
            <em>Littleâs Law over a finite observation window</em>,
            rather than the more familiar steady-state, equilibrium form
            of Littleâs Law <a href="#fn10" class="footnote-ref"
            id="fnref10" role="doc-noteref"><sup>10</sup></a>.</p>
            <p>Unlike the equilibrium form of the law, this version
            holds <em>unconditionally</em>. The key is that the
            quantities involved are <em>observer-relative</em>: the time
            signals spend <em>within a finite observation window</em>,
            and the <em>incidence rate</em> of signals <em>over the
            window</em>, rather than the signal-relative durations or
            long-run onset/reset rates assumed in the equilibrium
            form.</p>
            <p>Indeed, the difference between these two forms of the
            identity will serve as the basis for how we <em>define</em>
            whether a system of presences is in equilibrium or not. The
            idea is that the system of presences is at equilibrium when
            observed over sufficiently long observation windows such
            that the observer-relative and task-relative values of
            presence density, incidence rate and presence mass
            converge.</p>
            <p>Since real-world systems often operate far from
            equilibriumâand since the presence invariant holds
            <em>regardless</em> of equilibriumâthe finite-window form
            becomes far more valuable for analyzing the long-run
            behavior of such systems as they <em>move into and
            between</em> equilibrium states.</p>
            <p>All this is the focus of section 6, where we will
            formally make the connections between the presence invariant
            and equilibrium states in systems of presence with the help
            of a very general form of Littleâs Law originally proven by
            Brumelle <span class="citation" data-cites="brumelle71"><a
            href="#ref-brumelle71" role="doc-biblioref">[4]</a></span>,
            and Heyman and Stidham <span class="citation"
            data-cites="heyman80"><a href="#ref-heyman80"
            role="doc-biblioref">[5]</a></span>.</p>
            <h3 data-number="5.4" id="signal-dynamics"><span
            class="header-section-number">5.4</span> Signal
            dynamics</h3>
            <p>Now that weâve defined how to observe a system of
            presences over a single finite interval, we turn to what
            happens when we observe the system continuously over
            timeâcomputing the parameters of the presence invariant
            across consecutive, equal-sized, half-open intervals.</p>
            <figure id="fig:system-presences-discrete">
            <img src="../assets/pandoc/system_presences_discrete.png"
            alt="Figure 7: Sampling a system of presences across uniform intervals" />
            <figcaption aria-hidden="true">Figure 7: Sampling a system
            of presences across uniform intervals</figcaption>
            </figure>
            <p>This step is fundamental to the presence calculus: it
            allows us to study how presence density evolves across
            consecutive observationsâthe <em>signal dynamics</em> of the
            system.</p>
            <p>The presence invariant holds at each interval and defines
            a constraint among the three key parameters: presence
            density, signal incidence rate, and mass contribution per
            signal.</p>
            <blockquote>
            <p>Given any two of these, the third is completely
            determined.<br />
            Among them, presence density is the output; incidence rate
            and mass contribution are the inputs.</p>
            </blockquote>
            <p>At each interval, presence density can be directly
            observedâbut the invariant requires that it always equal the
            product of incidence rate and mass contribution:</p>
            <blockquote>
            <p>Any change in presence density must result from a change
            in incidence rate, mass contribution, or both.</p>
            <p>In other words, the system has only <em>two degrees of
            freedom</em> among three interdependent variables.</p>
            </blockquote>
            <p>Because the invariant holds across <em>any</em> finite
            interval, tracking how these parameters shift reveals how a
            particular system of presences evolves.</p>
            <p>In Section 7, weâll explore a geometric view of this
            idea. If we treat these three parameters as coordinates of
            the systemâs state in each interval, we can trace its
            evolution as a trajectory through time.</p>
            <blockquote>
            <p>This makes the presence invariant a powerful tool for
            causal reasoningâone that helps explain <em>why</em>
            presence density changes the way it does.</p>
            </blockquote>
            <p>In the next section, weâll introduce the <em>presence
            matrix</em>âa compact representation of the sampled signals
            shown in FigureÂ 7. It is a key building block in the
            machinery for computing these trajectories.</p>
            <h2 data-number="6" id="presence-matrix"><span
            class="header-section-number">6</span> The presence
            matrix</h2>
            <p>A <em>presence matrix</em> is a data structure that
            records the presence mass values resulting from the sampling
            process described in FigureÂ 7.</p>
            <p>The length of each sampling interval is called the
            <em>sampling granularity</em>. This defines the smallest
            time resolution at which the <em>matrix</em> represents
            presence <a href="#fn11" class="footnote-ref" id="fnref11"
            role="doc-noteref"><sup>11</sup></a>.</p>
            <p>The union of these intervals is called the
            <em>observation window</em> of the matrix.</p>
            <blockquote>
            <p>Choosing the right sampling granularity is a key modeling
            decision. It directly affects the kinds of insights we can
            extract from the presence matrix using the machinery we
            develop later.</p>
            </blockquote>
            <p>Given <span class="math inline">\(M\)</span> presence
            density functions and an observation window consisting of
            <span class="math inline">\(N\)</span> intervals at some
            fixed sampling granularity, the presence matrix <span
            class="math inline">\(P\)</span> is an <span
            class="math inline">\(M \times N\)</span> matrix where:</p>
            <ul>
            <li><p><em>Rows</em> correspond to individual presence
            density functions (typically indexed by <span
            class="math inline">\((e, b)\)</span> pairs),</p></li>
            <li><p><em>Columns</em> correspond to half-open time
            intervals at the sampling granularity,</p></li>
            <li><p><em>Entries</em> contain the <em>presence
            mass</em>âthat is, the integral of the corresponding density
            function over the associated time interval:</p>
            <p><span class="math display">\[
            P(i,j) = M_{(e,b),j} = \int_{t_j}^{t_{j+1}} f_{(e,b)}(t) \,
            dt
            \]</span></p></li>
            </ul>
            <blockquote>
            <p>The presence matrix <span
            class="math inline">\(P\)</span> is a discrete, time-aligned
            representation of presence mass for a given system of
            presences.</p>
            </blockquote>
            <p>Since we are accumulating presence masses over an
            interval, the value of presence mass in a matrix entry is
            always a a real number. FigureÂ 8 shows the presence matrix
            for the system in FigureÂ 7.</p>
            <figure id="fig:presence-matrix-neat">
            <img src="../assets/pandoc/presence_matrix_neat.png"
            alt="Figure 8: Presence Matrix for a system of presences" />
            <figcaption aria-hidden="true">Figure 8: Presence Matrix for
            a system of presences</figcaption>
            </figure>
            <p>In FigureÂ 8,</p>
            <ul>
            <li>Each row in the matrix maps to a single element-boundary
            signal.</li>
            <li>Each column represents a sampling interval at the
            sampling granularity.</li>
            <li>Each matrix entry consists of the observed presence mass
            of a signal at that time interval.</li>
            </ul>
            <p>The alert reader will note the difference between the
            first two rows in the matrix. Even though the underlying
            signals both have two distinct support intervals, the first
            signal is represented by a single presence in the matrix,
            while the second is broken up into two disjoint
            presences.</p>
            <p>This is entirely an artifact of the granularity at which
            the signal is sampled. At a suitably fine sampling
            granularity, the first signal could also be represented by
            two presences. <em>However, we are not losing any
            information</em> - the total presence mass is simply
            distributed across different observation windows during the
            integration step that computes the presence mass.</p>
            <p>In other words, this has no real impact on the behavior
            of the invariant <a href="#fn12" class="footnote-ref"
            id="fnref12" role="doc-noteref"><sup>12</sup></a>
            <em>provided we cover the timeline with our
            observations.</em></p>
            <p>The presence matrix encodes deep structural properties of
            a system of presences. Many of key concepts we want to
            highlight are easier to define and understand in terms of
            this representation.</p>
            <h3 data-number="6.1"
            id="the-presence-invariant-and-the-presence-matrix"><span
            class="header-section-number">6.1</span> The presence
            invariant and the presence matrix</h3>
            <p>Letâs revisit FigureÂ 4, reproduced below, which
            introduced the idea of interpreting presence mass as a
            sample from an underlying signal.</p>
            <div style="text-align: center; margin:2em">
            <p><img src="../assets/pandoc/multiple_support.png" width="600px" /></p>
            </div>
            <p>When we observe a system of presences across a finite
            observation windowâas we do when deriving the presence
            invariantâwe are looking at presence mass across a
            âverticalâ slice of time, spanning many signals.</p>
            <figure id="fig:presence-matrix-table"
            class="subfigures figure">
            <img src="../assets/placeholder.png"
            id="fig:presence-matrix-table" style="display: none;"
            alt="a" />
            <table>
            <thead>
            <tr>
            <th>
            E
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>
            e1_b1
            </td>
            <td style="background-color: #c6f6c6">
            0.3
            </td>
            <td style="background-color: #c6f6c6">
            2.3
            </td>
            <td style="background-color: #c6f6c6">
            3.4
            </td>
            <td style="background-color: #c6f6c6">
            1.1
            </td>
            <td style="background-color: #c6f6c6">
            2.9
            </td>
            <td style="background-color: #c6f6c6">
            3.2
            </td>
            <td style="background-color: #c6f6c6">
            1.1
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            </tr>
            <tr>
            <td>
            e1_b2
            </td>
            <td style="background-color: #c6f6c6">
            0.3
            </td>
            <td style="background-color: #c6f6c6">
            2.3
            </td>
            <td style="background-color: #c6f6c6">
            3.4
            </td>
            <td style="background-color: #c6f6c6">
            1.1
            </td>
            <td>
            0.0
            </td>
            <td style="background-color: #c6f6c6">
            1.1
            </td>
            <td style="background-color: #c6f6c6">
            2.2
            </td>
            <td style="background-color: #c6f6c6">
            2.4
            </td>
            <td style="background-color: #c6f6c6">
            2.3
            </td>
            <td style="background-color: #c6f6c6">
            0.8
            </td>
            </tr>
            <tr>
            <td>
            e2_b2
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td style="background-color: #c6f6c6">
            0.9
            </td>
            <td style="background-color: #c6f6c6">
            1.8
            </td>
            <td style="background-color: #c6f6c6">
            3.2
            </td>
            <td style="background-color: #c6f6c6">
            0.9
            </td>
            </tr>
            <tr>
            <td>
            e2_b1
            </td>
            <td style="background-color: #c6f6c6">
            0.8
            </td>
            <td style="background-color: #c6f6c6">
            1.3
            </td>
            <td style="background-color: #c6f6c6">
            2.4
            </td>
            <td style="background-color: #c6f6c6">
            2.8
            </td>
            <td style="background-color: #c6f6c6">
            3.0
            </td>
            <td style="background-color: #c6f6c6">
            3.2
            </td>
            <td style="background-color: #c6f6c6">
            3.4
            </td>
            <td style="background-color: #c6f6c6">
            2.4
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            </tr>
            </tbody>
            </table>
            <figcaption><p>Figure 9: Mass contributions by signal.
            </p></figcaption>
            </figure>
            <p>In the presence matrix from FigureÂ 8, reproduced in
            FigureÂ 9, each entry represents the presence mass obtained
            by sampling a signal over a particular interval. Rows
            correspond to distinct signals, and columns represent
            sampling intervals.</p>
            <p>We can construct a presence matrix from any subset of the
            rows of another presence matrixâand it would still be a
            valid presence matrix for the underlying system. Similarly,
            any <em>consecutive sub-sequence</em> of columns within such
            a matrix also constitutes a presence matrix over a
            sub-interval of the systemâs history.</p>
            <p>Letâs see what this means in terms of the presence
            invariant.</p>
            <figure id="fig:matrix-invariant">
            <img src="../assets/pandoc/matrix_invariant.png"
            alt="Figure 10: Computing the invariant from the matrix" />
            <figcaption aria-hidden="true">Figure 10: Computing the
            invariant from the matrix</figcaption>
            </figure>
            <p>In FigureÂ 10 we interpret a consecutive sequence of
            columns in the matrix as an <em>observation window</em>.
            Given such a window and the submatrix it induces:</p>
            <ul>
            <li>The row sums of the submatrix give us the <em>mass
            contributions per signal</em> within the window.</li>
            <li>The sum of these row sums gives us the <em>cumulative
            presence mass</em> <span class="math inline">\(A\)</span>
            across all signals for the window.</li>
            <li>The number of <em>incident signals</em> <span
            class="math inline">\(N\)</span> is the number of rows in
            the submatrix with non-zero values.</li>
            <li><span class="math inline">\(T\)</span> is the number of
            columns in the submatrix, measured in units of the sampling
            granularity <a href="#fn13" class="footnote-ref"
            id="fnref13" role="doc-noteref"><sup>13</sup></a>.</li>
            </ul>
            <blockquote>
            <p>Because <span class="math inline">\(A\)</span>, <span
            class="math inline">\(N\)</span>, and <span
            class="math inline">\(T\)</span> are directly computable
            from the submatrix, we can derive the three parameters of
            the presence invariant:</p>
            <ul>
            <li>presence density: <span class="math inline">\(\delta =
            \frac{A}{T}\)</span></li>
            <li>signal incidence rate: <span class="math inline">\(\iota
            = \frac{N}{T}\)</span></li>
            <li>mass contribution per signal: <span
            class="math inline">\(\bar{m} = \frac{A}{N}\)</span></li>
            </ul>
            </blockquote>
            <p>Thus the parameters of the presence invariant are well
            defined for <em>any observation window</em> over a presence
            matrix.</p>
            <p>Next, we introduce a structure that allows us to compute
            and analyze these parameters <em>across multiple observation
            windows</em>âthe final piece we need to compute signal
            dynamics for the system.</p>
            <h2 data-number="7" id="presence-accumulation-matrix"><span
            class="header-section-number">7</span> The presence
            accumulation matrix</h2>
            <p>Signal dynamics requires us to reason about both the
            micro and macro behaviors of a system of presences. To do
            this efficiently we will derive a data structure called the
            <em>Presence Accumulation Matrix</em>.</p>
            <p>This matrix compresses information about the interaction
            between micro and macro behavior of a system of presences
            across time windows and time scales. Letâs see how it is
            constructed.</p>
            <p>We will start with the presence matrix of FigureÂ 10.
            Recall that this is an <span class="math inline">\(M \times
            N\)</span> matrix of <span class="math inline">\(M\)</span>
            signal sampled at <span class="math inline">\(N\)</span>
            consecutive intervals, and the value at row <span
            class="math inline">\(i\)</span> and column <span
            class="math inline">\(j\)</span> represents the sampled
            presence mass of signal <span
            class="math inline">\(i\)</span> over the observation window
            <span class="math inline">\(j\)</span>.</p>
            <p>Now, consider the matrix <span
            class="math inline">\(A\)</span> that accumulates presences
            masses by applying the windowing operation of FigureÂ 10
            across <em>all possible observation windows</em> over the
            original matrix.</p>
            <p>So, for each <em>pair</em> of columns <span
            class="math inline">\(i,j,\)</span> the entry in <span
            class="math inline">\(A[i,j]\)</span> equals the value of
            the total presence mass <span
            class="math inline">\(A\)</span> for the submatrix induced
            by the columns <span class="math inline">\([i,..,j]\)</span>
            in the original presence matrix.</p>
            <p>As shown in FigureÂ 11, the diagonal of the matrix
            contains the accumulated presence mass across signals at the
            sampling granularity. Each entry here is the sum of a single
            column in the presence matrix.</p>
            <figure id="fig:acc-diagonal" class="subfigures figure">
            <img src="../assets/pandoc/placeholder.png"
            id="fig:acc-diagonal" style="display: none;" alt="a" />
            <table>
            <thead>
            <tr>
            <th>
            i\j
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>
            1
            </td>
            <td style="background-color:#e6ffe6">
            1.4
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            2
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            3
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            9.2
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            4
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.0
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            5
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            6
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.5
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            7
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.6
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            8
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            6.6
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.5
            </td>
            </tr>
            <tr>
            <td>
            10
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            1.7
            </td>
            </tr>
            </tbody>
            </table>
            <figcaption><p>Figure 11: Presence mass for single
            observation window.. </p></figcaption>
            </figure>
            <p>The next diagonal contains the cumulative presence mass
            for intervals of length 2 - ie <span
            class="math inline">\(A(1,2)\)</span> contains the sum of
            all entries in columns 1 and 2. <span
            class="math inline">\(A(2,3)\)</span> contains the sum of
            all entries column 2 and 3 and so on..</p>
            <figure id="fig:acc-second-diagonal"
            class="subfigures figure">
            <img src="../assets/placeholder.png"
            id="fig:acc-second-diagonal" style="display: none;"
            alt="a" />
            <table>
            <thead>
            <tr>
            <th>
            i\j
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>
            1
            </td>
            <td style="background-color:#e6ffe6">
            1.4
            </td>
            <td style="background-color:#e6f0ff">
            7.3
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            2
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td style="background-color:#e6f0ff">
            15.1
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            3
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            9.2
            </td>
            <td style="background-color:#e6f0ff">
            14.2
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            4
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.0
            </td>
            <td style="background-color:#e6f0ff">
            10.9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            5
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td style="background-color:#e6f0ff">
            13.4
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            6
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.5
            </td>
            <td style="background-color:#e6f0ff">
            15.1
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            7
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.6
            </td>
            <td style="background-color:#e6f0ff">
            14.2
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            8
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            6.6
            </td>
            <td style="background-color:#e6f0ff">
            12.1
            </td>
            </tr>
            <tr>
            <td>
            9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.5
            </td>
            <td style="background-color:#e6f0ff">
            7.2
            </td>
            </tr>
            <tr>
            <td>
            10
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            1.7
            </td>
            </tr>
            </tbody>
            </table>
            <figcaption><p>Figure 12: Cumulative presence mass along
            intervals of length 2. </p></figcaption>
            </figure>
            <p>We can continue filling the matrix out in diagonal order
            this way until we get the presence accumulation matrix shown
            below.</p>
            <figure id="fig:acc-matrix-full" class="subfigures figure">
            <img src="../assets/placeholder.png"
            id="fig:acc-matrix-full" style="display: none;" alt="a" />
            <table>
            <thead>
            <tr>
            <th>
            i\j
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <!-- Row 1 -->
            <tr>
            <td>
            1
            </td>
            <td style="background-color:#e6ffe6">
            1.4
            </td>
            <td style="background-color:#e6f0ff">
            7.3
            </td>
            <td style="background-color:#e6ffe6">
            16.5
            </td>
            <td style="background-color:#e6f0ff">
            21.5
            </td>
            <td style="background-color:#e6ffe6">
            27.4
            </td>
            <td style="background-color:#e6f0ff">
            34.9
            </td>
            <td style="background-color:#e6ffe6">
            42.5
            </td>
            <td style="background-color:#e6f0ff">
            49.1
            </td>
            <td style="background-color:#e6ffe6">
            54.6
            </td>
            <td style="background-color:#e6f0ff">
            56.3
            </td>
            </tr>
            <!-- Row 2 -->
            <tr>
            <td>
            2
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td style="background-color:#e6f0ff">
            15.1
            </td>
            <td style="background-color:#e6ffe6">
            20.1
            </td>
            <td style="background-color:#e6f0ff">
            26.0
            </td>
            <td style="background-color:#e6ffe6">
            33.5
            </td>
            <td style="background-color:#e6f0ff">
            41.1
            </td>
            <td style="background-color:#e6ffe6">
            47.7
            </td>
            <td style="background-color:#e6f0ff">
            53.2
            </td>
            <td style="background-color:#e6ffe6">
            54.9
            </td>
            </tr>
            <!-- Row 3 -->
            <tr>
            <td>
            3
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            9.2
            </td>
            <td style="background-color:#e6f0ff">
            14.2
            </td>
            <td style="background-color:#e6ffe6">
            20.1
            </td>
            <td style="background-color:#e6f0ff">
            27.6
            </td>
            <td style="background-color:#e6ffe6">
            35.2
            </td>
            <td style="background-color:#e6f0ff">
            41.8
            </td>
            <td style="background-color:#e6ffe6">
            47.3
            </td>
            <td style="background-color:#e6f0ff">
            49.0
            </td>
            </tr>
            <!-- Row 4 -->
            <tr>
            <td>
            4
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.0
            </td>
            <td style="background-color:#e6f0ff">
            10.9
            </td>
            <td style="background-color:#e6ffe6">
            18.4
            </td>
            <td style="background-color:#e6f0ff">
            26.0
            </td>
            <td style="background-color:#e6ffe6">
            32.6
            </td>
            <td style="background-color:#e6f0ff">
            38.1
            </td>
            <td style="background-color:#e6ffe6">
            39.8
            </td>
            </tr>
            <!-- Row 5 -->
            <tr>
            <td>
            5
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td style="background-color:#e6f0ff">
            13.4
            </td>
            <td style="background-color:#e6ffe6">
            21.0
            </td>
            <td style="background-color:#e6f0ff">
            27.6
            </td>
            <td style="background-color:#e6ffe6">
            33.1
            </td>
            <td style="background-color:#e6f0ff">
            34.8
            </td>
            </tr>
            <!-- Row 6 -->
            <tr>
            <td>
            6
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.5
            </td>
            <td style="background-color:#e6f0ff">
            15.1
            </td>
            <td style="background-color:#e6ffe6">
            21.7
            </td>
            <td style="background-color:#e6f0ff">
            27.2
            </td>
            <td style="background-color:#e6ffe6">
            28.9
            </td>
            </tr>
            <!-- Row 7 -->
            <tr>
            <td>
            7
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.6
            </td>
            <td style="background-color:#e6f0ff">
            14.2
            </td>
            <td style="background-color:#e6ffe6">
            19.7
            </td>
            <td style="background-color:#e6f0ff">
            21.4
            </td>
            </tr>
            <!-- Row 8 -->
            <tr>
            <td>
            8
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            6.6
            </td>
            <td style="background-color:#e6f0ff">
            12.1
            </td>
            <td style="background-color:#e6ffe6">
            13.8
            </td>
            </tr>
            <!-- Row 9 -->
            <tr>
            <td>
            9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.5
            </td>
            <td style="background-color:#e6f0ff">
            7.2
            </td>
            </tr>
            <!-- Row 10 -->
            <tr>
            <td>
            10
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            1.7
            </td>
            </tr>
            </tbody>
            </table>
            <figcaption><p>Figure 13: Final accumulation matrix.
            </p></figcaption>
            </figure>
            <p>FigureÂ 14, shows this computation.</p>
            <figure id="fig:acc-matrix-construction">
            <img src="../assets/pandoc/acc_matrix_construction.png"
            alt="Figure 14: Accumulation Matrix Construction" />
            <figcaption aria-hidden="true">Figure 14: Accumulation
            Matrix Construction</figcaption>
            </figure>
            <blockquote>
            <p>At the observation granularity we have windows of length
            1, next we have accumulated presence from two consecutive
            observation windows, next we have accumulated presence from
            three consecutive windows, and so on.</p>
            </blockquote>
            <p>We can see that in practice, this matrix compresses a
            large amount of information in a very compact form.</p>
            <blockquote>
            <p>For example if the columns represent weekly samples of
            signals a 52x52 matrix allows us to analyze a whole years
            worth of presence accumulation across every time-scale
            ranging from a single week to a whole year in one compact
            structure.</p>
            </blockquote>
            <h4 data-number="7.0.1"
            id="presence-accumulation-matrix---definition"><span
            class="header-section-number">7.0.1</span> Presence
            Accumulation Matrix - Definition</h4>
            <p>To summarize lets define the general structure of the
            presence accumulation matrix, formally.</p>
            <p>Let <span class="math inline">\(P \in \mathbb{R}^{M
            \times N}\)</span> be the presence matrix of <span
            class="math inline">\(M\)</span> signals sampled at <span
            class="math inline">\(N\)</span> consecutive time intervals.
            The <em>Presence Accumulation Matrix</em> <span
            class="math inline">\(A \in \mathbb{R}^{N \times N}\)</span>
            is defined as:</p>
            <p><span class="math display">\[
            A(i, j) = \sum_{k=1}^{M} \sum_{\ell=i}^{j} P(k, \ell)
            \quad \text{for all } 1 \leq i \leq j \leq N
            \]</span></p>
            <p>That is, <span class="math inline">\(A(i,j)\)</span>
            gives the total presence mass of all signals across the
            interval <span class="math inline">\([i,j]\)</span>.</p>
            <ul>
            <li><span class="math inline">\(A\)</span> is upper
            triangular: <span class="math inline">\(A(i,j)\)</span> is
            defined only when <span class="math inline">\(i \leq
            j\)</span>.</li>
            <li>The diagonal entries <span
            class="math inline">\(A(i,i)\)</span> equal the column sums
            of <span class="math inline">\(P\)</span>.</li>
            <li>Each entry <span class="math inline">\(A(i,j)\)</span>
            reflects the cumulative presence mass over the interval
            <span class="math inline">\([i,j]\)</span> in the original
            presence matrix.</li>
            </ul>
            <p>As we will see below, this matrix compactly encodes
            multi-scale information about system behavior and supports
            the analysis of both micro and macro scale behavior of a
            system of presences.</p>
            <h3 data-number="7.1"
            id="the-presence-accumulation-recurrence"><span
            class="header-section-number">7.1</span> The presence
            accumulation recurrence</h3>
            <p>In this section, we demonstrate a key implication of
            modeling signals as sampled presences over time: that the
            relationship between presence mass accumulation at micro and
            macro timescales can be described
            <em>deterministically</em>.</p>
            <p>This powerful property arises directly from the fact that
            presence masses are measures over time intervalsâand that
            such measures satisfy a mathematical property called
            <em>finite additivity</em>.</p>
            To explain this, we reproduce the presence accumulation
            matrix from Figure 12 below.
            <div style="text-align: center; margin:2em">
            <table>
            <thead>
            <tr>
            <th>
            i\j
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <!-- Row 1 -->
            <tr>
            <td>
            1
            </td>
            <td style="background-color:#e6ffe6">
            1.4
            </td>
            <td style="background-color:#e6f0ff">
            7.3
            </td>
            <td style="background-color:#e6ffe6">
            16.5
            </td>
            <td style="background-color:#e6f0ff">
            21.5
            </td>
            <td style="background-color:#e6ffe6">
            27.4
            </td>
            <td style="background-color:#e6f0ff">
            34.9
            </td>
            <td style="background-color:#e6ffe6">
            42.5
            </td>
            <td style="background-color:#e6f0ff">
            49.1
            </td>
            <td style="background-color:#e6ffe6">
            54.6
            </td>
            <td style="background-color:#e6f0ff">
            56.3
            </td>
            </tr>
            <!-- Row 2 -->
            <tr>
            <td>
            2
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td style="background-color:#e6f0ff">
            15.1
            </td>
            <td style="background-color:#e6ffe6">
            20.1
            </td>
            <td style="background-color:#e6f0ff">
            26.0
            </td>
            <td style="background-color:#e6ffe6">
            33.5
            </td>
            <td style="background-color:#e6f0ff">
            41.1
            </td>
            <td style="background-color:#e6ffe6">
            47.7
            </td>
            <td style="background-color:#e6f0ff">
            53.2
            </td>
            <td style="background-color:#e6ffe6">
            54.9
            </td>
            </tr>
            <!-- Row 3 -->
            <tr>
            <td>
            3
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            9.2
            </td>
            <td style="background-color:#e6f0ff">
            14.2
            </td>
            <td style="background-color:#e6ffe6">
            20.1
            </td>
            <td style="background-color:#e6f0ff">
            27.6
            </td>
            <td style="background-color:#e6ffe6">
            35.2
            </td>
            <td style="background-color:#e6f0ff">
            41.8
            </td>
            <td style="background-color:#e6ffe6">
            47.3
            </td>
            <td style="background-color:#e6f0ff">
            49.0
            </td>
            </tr>
            <!-- Row 4 -->
            <tr>
            <td>
            4
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.0
            </td>
            <td style="background-color:#e6f0ff">
            10.9
            </td>
            <td style="background-color:#e6ffe6">
            18.4
            </td>
            <td style="background-color:#e6f0ff">
            26.0
            </td>
            <td style="background-color:#e6ffe6">
            32.6
            </td>
            <td style="background-color:#e6f0ff">
            38.1
            </td>
            <td style="background-color:#e6ffe6">
            39.8
            </td>
            </tr>
            <!-- Row 5 -->
            <tr>
            <td>
            5
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td style="background-color:#e6f0ff">
            13.4
            </td>
            <td style="background-color:#e6ffe6">
            21.0
            </td>
            <td style="background-color:#e6f0ff">
            27.6
            </td>
            <td style="background-color:#e6ffe6">
            33.1
            </td>
            <td style="background-color:#e6f0ff">
            34.8
            </td>
            </tr>
            <!-- Row 6 -->
            <tr>
            <td>
            6
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.5
            </td>
            <td style="background-color:#e6f0ff">
            15.1
            </td>
            <td style="background-color:#e6ffe6">
            21.7
            </td>
            <td style="background-color:#e6f0ff">
            27.2
            </td>
            <td style="background-color:#e6ffe6">
            28.9
            </td>
            </tr>
            <!-- Row 7 -->
            <tr>
            <td>
            7
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.6
            </td>
            <td style="background-color:#e6f0ff">
            14.2
            </td>
            <td style="background-color:#e6ffe6">
            19.7
            </td>
            <td style="background-color:#e6f0ff">
            21.4
            </td>
            </tr>
            <!-- Row 8 -->
            <tr>
            <td>
            8
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            6.6
            </td>
            <td style="background-color:#e6f0ff">
            12.1
            </td>
            <td style="background-color:#e6ffe6">
            13.8
            </td>
            </tr>
            <!-- Row 9 -->
            <tr>
            <td>
            9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.5
            </td>
            <td style="background-color:#e6f0ff">
            7.2
            </td>
            </tr>
            <!-- Row 10 -->
            <tr>
            <td>
            10
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            1.7
            </td>
            </tr>
            </tbody>
            </table>
            </div>
            <p>As noted earlier, the diagonal of the matrix represents a
            sample path through the systemâs history, and the top row
            records cumulative presence mass over the entire observed
            history. The diagonal shows accumulation at the finest
            (micro) timescale; the top row reflects accumulation at the
            coarsest (macro) timescale. Each diagonal represents
            accumulation across intervals of increasingly coarse
            granularity.</p>
            <p>In essence, the accumulation matrix is a compact encoding
            of how presence mass builds up across timescales.</p>
            <p>But thereâs more: the matrix entries obey a <em>local
            recurrence relationship</em>, which allows us to reconstruct
            the entire matrix from its diagonal:</p>
            <p><span class="math display">\[
            A[i,j] = A[i,j-1] + A[i+1,j] - A[i+1,j-1]
            \quad \text{for } 1 \le i &lt; j \le N
            \]</span></p>
            <p>It tells us that the cumulative mass from column <span
            class="math inline">\(i\)</span> to column <span
            class="math inline">\(j\)</span> can be<br />
            computed from three nearby entries:</p>
            <ul>
            <li><span class="math inline">\(A[i,j-1]\)</span>: the
            cumulative mass from <span class="math inline">\(i\)</span>
            to <span class="math inline">\(j{-}1\)</span></li>
            <li><span class="math inline">\(A[i+1, j]\)</span>: the mass
            from <span class="math inline">\(i{+}1\)</span> to <span
            class="math inline">\(j\)</span></li>
            <li>a correction term subtracting the overlap <span
            class="math inline">\(A[i+1, j-1]\)</span> from <span
            class="math inline">\(i{+}1\)</span> to <span
            class="math inline">\(j{-}1\)</span></li>
            </ul>
            <p>This equation reflects the finite additivity of
            cumulative presence mass over rectangular regions in the
            presence matrix <a href="#fn14" class="footnote-ref"
            id="fnref14" role="doc-noteref"><sup>14</sup></a>.</p>
            <p>So, the entire matrixâand thus, the systemâs accumulation
            dynamicsâis governed by a simple, local rule. Given the
            values on the diagonal, the rest of the matrix is completely
            determined.</p>
            <p>The physical meaning is this: if we know the first <span
            class="math inline">\(N{-}1\)</span> values along a sample
            path, then observing the <span
            class="math inline">\(N^\text{th}\)</span> value allows us
            to explain how the macro behavior of the system evolved
            across <em>all</em> timescales up to that point.</p>
            <p>Whatâs remarkable is that this determinism holds
            regardless of the nature of the underlying processes. The
            signals might come from deterministic, stochastic, linear,
            non-linear, or even chaotic processes. As long as the
            signals they generate are <em>measurable</em> they satisfy
            the finite additivity property and when we sample their
            <em>observed</em> presence masses, this local recurrence
            always applies <a href="#fn15" class="footnote-ref"
            id="fnref15" role="doc-noteref"><sup>15</sup></a>.</p>
            <p>Combined with the presence invariantâwhich also holds at
            every level of this accumulationâthis gives us a powerful
            framework for dissecting the dynamics of a system of
            presences.</p>
            <h2 data-number="8" id="signal-dynamics"><span
            class="header-section-number">8</span> Computing signal
            dynamics</h2>
            <p>Presence calculus conceptsâsuch as presence mass,
            incidence rate, and densityâ are not unlike abstract
            physical notions like force, mass, and acceleration <a
            href="#fn16" class="footnote-ref" id="fnref16"
            role="doc-noteref"><sup>16</sup></a>. In principle, these
            are measurable quantities constrained by nature to behave in
            prescribed ways at a micro scale.</p>
            <p>Once we understand the rules governing their micro-scale
            behavior, we gain the ability to measure, reason about, and
            explain a wide range of macro-scale phenomena. Much of
            physics is built on this principle.</p>
            <p>In a similar vein, the presence calculusâand especially
            the <em>presence invariant</em>â provides a foundational
            constraint that governs the local, time-based behavior of
            any system described by measurable, time-varying signals and
            the measures they induce: presence masses.</p>
            <p>Recognizing that such a constraint exists allows us to
            construct tools that describe, interpret, and explain
            macro-scale system behavior.</p>
            <p>Newtonian mechanics, for example, enables us to describe
            and predict the motion of certain physical systemsâsuch as
            planetary orbits or the trajectories of falling objectsâwith
            remarkable precision. Yet even within this well-established
            framework, limitations persist: the general three-body
            problem, for example, has no closed-form solution, and
            systems like the double pendulum exhibit chaotic behavior
            that defies exact prediction.</p>
            <p>Still, such systems can be represented and observed as
            <em>deterministic</em> trajectories through a parameter
            space. Even when precise long-term behavior is inaccessible,
            we can often uncover structure and explain <em>observed</em>
            behavior.</p>
            <p>In much the same way, by explicitly modeling signal
            histories and representing system trajectories in a
            parameter spaceâthe parameters of the presence invariantâthe
            presence calculus provides powerful descriptive tools for
            explaining how systems of presence evolve.</p>
            <p>Within this context, <em>convergence</em> and
            <em>divergence</em> of presence density are the two most
            important macro-scale behaviors we can study. These allow us
            to defineâformally and operationallyâwhat it means for the
            accumulation of presence mass in a system to be in
            equilibrium.</p>
            <blockquote>
            <p>Itâs important to note that the presence calculus is, at
            its core, a tool for explanation, not prediction. However,
            when supplemented with additional domain knowledge and
            assumptions, it can provide a distinct, non-statistical
            substrate on which to base predictive reasoning.</p>
            </blockquote>
            <h3 data-number="8.1" id="sample-paths"><span
            class="header-section-number">8.1</span> Sample paths</h3>
            <p>Consider the highlighted portions of the accumulation
            matrix <span class="math inline">\(A\)</span> in
            FigureÂ 15.</p>
            <figure id="fig:diagonal-top-row" class="subfigures figure">
            <img src="../assets/placeholder.png"
            id="fig:diagonal-top-row" style="display: none;" alt="a" />
            <table>
            <thead>
            <tr>
            <th>
            i\j
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>
            1
            </td>
            <td style="background-color:#e6ffe6">
            1.4
            </td>
            <td style="background-color:#ffffcc">
            7.3
            </td>
            <td style="background-color:#ffffcc">
            16.5
            </td>
            <td style="background-color:#ffffcc">
            21.5
            </td>
            <td style="background-color:#ffffcc">
            27.4
            </td>
            <td style="background-color:#ffffcc">
            34.9
            </td>
            <td style="background-color:#ffffcc">
            42.5
            </td>
            <td style="background-color:#ffffcc">
            49.1
            </td>
            <td style="background-color:#ffffcc">
            54.6
            </td>
            <td style="background-color:#ffffcc">
            56.3
            </td>
            </tr>
            <tr>
            <td>
            2
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            3
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            9.2
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            4
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.0
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            5
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            6
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.5
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            7
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.6
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            8
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            6.6
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.5
            </td>
            </tr>
            <tr>
            <td>
            10
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            1.7
            </td>
            </tr>
            </tbody>
            </table>
            <figcaption><p>Figure 15: Diagonal and top row of the
            accumulation matrix. </p></figcaption>
            </figure>
            <p>Each diagonal entry represents the total presence mass
            observed across all signals at the sampling granularity.
            Thus, the diagonal traces the discrete-time evolution of the
            systemâs directly observed signalsâone sample at a time.</p>
            <p>We will call the sequence of values on the diagonal a
            <em>sample path</em> for the system of presences <a
            href="#fn17" class="footnote-ref" id="fnref17"
            role="doc-noteref"><sup>17</sup></a>.</p>
            <figure id="fig:diagonal-values">
            <img src="../assets/pandoc/diagonal_values.png"
            alt="Figure 16: Computing the accumulated values on the sample path" />
            <figcaption aria-hidden="true">Figure 16: Computing the
            accumulated values on the sample path</figcaption>
            </figure>
            <p>By contrast, each entry on the top row represents the
            accumulated presence mass along a <em>prefix</em> of a
            sample path. In other words, the top row represents the
            accumulated presence over the observed <em>history</em> of
            the system.</p>
            <figure id="fig:top-row-values">
            <img src="../assets/pandoc/top_row_values.png"
            alt="Figure 17: The top row: accumuleted presences over system history" />
            <figcaption aria-hidden="true">Figure 17: The top row:
            accumuleted presences over system history</figcaption>
            </figure>
            <p>In other words, the diagonal and top row represent
            presence mass accumulations in the system at fundamentally
            different timescales: the diagonal captures the
            <em>micro-level behavior</em>â presence mass across signals
            in each interval at the sampling granularityâwhile the top
            row encodes the <em>macro-level behavior</em>âthe cumulative
            effect of those presences over the observed history of the
            system.</p>
            <p>Both views are compactly encoded in the structure of
            the<br />
            accumulation matrix, as are every timescale <em>in
            between.</em></p>
            <p>As shown in FigureÂ 18, the following geometric
            relationship holds between the values on the diagonal and
            the top row:</p>
            <figure id="fig:sample-path-area">
            <img src="../assets/pandoc/sample_path_area.png"
            alt="Figure 18: Top row as the area under the sample path" />
            <figcaption aria-hidden="true">Figure 18: Top row as the
            area under the sample path</figcaption>
            </figure>
            <blockquote>
            <p>Each entry on the top row is an integral <a href="#fn18"
            class="footnote-ref" id="fnref18"
            role="doc-noteref"><sup>18</sup></a> and equals the <em>area
            under a prefix of the sample path</em> represented by the
            diagonal.</p>
            </blockquote>
            <h3 data-number="8.2" id="convergence"><span
            class="header-section-number">8.2</span> Convergence and
            divergence of presence density</h3>
            <p>If we divide each of the entries in the accumulation
            matrix by the length of the time interval it covers, we get
            the presence density for each interval. For the diagonal
            interval has length 1 (time unit at sampling granularity)
            and for the top row the lengths range from 1 to <span
            class="math inline">\(N-1.\)</span></p>
            <figure id="fig:presence-density-diag-top-row"
            class="subfigures figure">
            <img src="../assets/placeholder.png"
            id="fig:presence-density-diag-top-row"
            style="display: none;" alt="a" />
            <table>
            <thead>
            <tr>
            <th>
            i\j
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>
            1
            </td>
            <td style="background-color:#e6ffe6">
            1.4
            </td>
            <td style="background-color:#ffffcc">
            3.6
            </td>
            <td style="background-color:#ffffcc">
            5.5
            </td>
            <td style="background-color:#ffffcc">
            5.4
            </td>
            <td style="background-color:#ffffcc">
            5.5
            </td>
            <td style="background-color:#ffffcc">
            5.8
            </td>
            <td style="background-color:#ffffcc">
            6.1
            </td>
            <td style="background-color:#ffffcc">
            6.1
            </td>
            <td style="background-color:#ffffcc">
            6.1
            </td>
            <td style="background-color:#ffffcc">
            5.6
            </td>
            </tr>
            <tr>
            <td>
            2
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            3
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            9.2
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            4
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.0
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            5
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            6
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.5
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            7
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            7.6
            </td>
            <td>
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            8
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            6.6
            </td>
            <td>
            </td>
            </tr>
            <tr>
            <td>
            9
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            5.5
            </td>
            </tr>
            <tr>
            <td>
            10
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color:#e6ffe6">
            1.7
            </td>
            </tr>
            </tbody>
            </table>
            <figcaption><p>Figure 19: Presence density: diagonal and top
            row.. </p></figcaption>
            </figure>
            <p>So now we have the left-hand side of the presence
            invariant encoded in matrix form for every pair of
            continuous intervals in the system.</p>
            <p>Letâs chart the values on the diagonal and the top row in
            the matrix.</p>
            <figure id="fig:presence-density-chart">
            <img src="../assets/pandoc/first_row_vs_main_diagonal.png"
            alt="Figure 20: Convergence of long-run presence density." />
            <figcaption aria-hidden="true">Figure 20: Convergence of
            long-run presence density.</figcaption>
            </figure>
            <p>We can see from FigureÂ 20 that while the values on the
            sample path are volatile, the values on the top row converge
            towards a finite value.</p>
            <p>We can define this notion of convergence of the values on
            the top row precisely using the mathematical concept of a
            limit.</p>
            <p>Let:</p>
            <p><span class="math display">\[
            \Delta = \lim_{T \to \infty} \frac{1}{T} \int_0^T \delta(t)
            \, dt
            \]</span></p>
            <p>Here, <span class="math inline">\(\delta(t)\)</span> is
            the presence density across all signals, measured at the
            base sampling granularity <span
            class="math inline">\(t\)</span>âthat is, the total presence
            mass per unit time over a single co-presence interval of
            width <span class="math inline">\(t\)</span>.</p>
            <p>The quantity <span class="math inline">\(\Delta\)</span>
            represents the presence density over the systemâs sample
            path, that is, its observed history.</p>
            <p>To make this correspondence explicit in terms of the
            accumulation matrix, we can express <span
            class="math inline">\(\Delta\)</span> as the limiting value
            of the presence density along the top row - the value
            towards which the value on the top row in</p>
            <p><span class="math display">\[
            \Delta = \lim_{j \to \infty} \frac{A(1,j)}{j}
            \]</span></p>
            <p>where <span class="math inline">\(A(1,j)\)</span> is the
            total presence mass accumulated from time 0 through interval
            <span class="math inline">\(j\)</span>.</p>
            <p>Not every system of presences has such a limit.</p>
            <blockquote>
            <p>We call a system <em>convergent</em> if <span
            class="math inline">\(\Delta\)</span> exists and is finite,
            and <em>divergent</em> otherwise.</p>
            </blockquote>
            <p>To summarize: in a fully convergent system, the presence
            density over time converges toward a finite value and stays
            there. Intuitively, this means that after observing enough
            of the systemâs history, additional observation does not
            significantly alter our understanding of its long-term
            behavior.</p>
            <p>Some systems converge rapidly to a single stable limit.
            Others may not settle at all, but instead move among a small
            number of such limits. These<br />
            represent dominant behavioral modes â quasi-equilibrium
            states that the system can enter and sustain for extended
            periods. Such behavior is called<br />
            <em>metastable</em> or <em>multi-modal</em>.</p>
            <p>It is also possible for the presence system to exhibit
            multiple recurring modes, but never remain in any one of
            them for any meaningful length of time. This is
            characteristic of a <em>chaotic</em> operating regime. At
            the macro level, the presence density may appear to trace
            out a pattern across a limited set of valuesâknown as
            <em>attractors</em>âbut the micro-scale trajectory is
            deterministic yet highly sensitive to initial conditions,
            making long-term behavior effectively unpredictable. In such
            cases, the attractor is not a fixed point but a complex
            structure that the system continuously explores. Such
            systems are convergent yet unpredictable and impossible to
            steer.</p>
            <p>Divergence, by contrast, implies the absence of such
            limiting behavior. The<br />
            presence density in divergent systems continues to grow
            without bound,<br />
            indicating that the dominant behavior of the system is an
            unbounded accumulation of presence.</p>
            <p>FigureÂ 21 shows examples of each of these behaviors.</p>
            <figure id="fig:convergence-divergence">
            <img src="../assets/pandoc/convergence_divergence.png"
            alt="Figure 21: Convergent, divergent, and metastable behavior in systems of presences." />
            <figcaption aria-hidden="true">Figure 21: Convergent,
            divergent, and metastable behavior in systems of
            presences.</figcaption>
            </figure>
            <p>If a limit <span class="math inline">\(\Delta\)</span>
            exists and is sustained over time, it signifies a stable
            long-run presence density for the system. This value
            represents a specific pattern of behavior toward which the
            systemâs observable presence density gravitates over
            extended periods, regardless of short-term fluctuations.</p>
            <p>This concept aligns with the broader notion of attractors
            in dynamical systems. While a systemâs full,
            high-dimensional state might exhibit complex dynamics, the
            long-run presence density can itself stabilize around a
            particular value or set of values. When the presence density
            consistently settles around such a limit, it indicates that
            the systemâs observable behavior has entered a stable
            regime.</p>
            <p>This provides a powerful way to characterize the systemâs
            overall operational modes in the long term.</p>
            <p>It is important to emphasize that convergence and
            divergence are properties of the <em>observed long-run
            behavior</em> of a system of presences â not intrinsic
            properties of the underlying system.</p>
            <p>We cannot infer the systemâs nature (whether it is
            deterministic, stochastic, linear, non-linear, chaotic etc)
            solely from whether it appears convergent or divergent at
            any given time. Any of these <em>types</em> of systems may
            be convergent or divergent at different points in time. We
            can only observe the dynamics of presence accumulation over
            time and assess whether they <em>exhibit</em> convergence
            over an observation interval.</p>
            <p>The key difference between convergence and the other two
            modes is that a<br />
            fully convergent system can effectively <em>forget</em> its
            history beyond the point of convergence. Its future behavior
            becomes representative of its past, allowing the system to
            be characterized by a stable long-run value.</p>
            <p>Such systems are relatively rare in the real world. This
            is where much of the utility of the presence calculus lies.
            It shines when analyzing the behavior of systems of presence
            when they operate in those liminal phases between
            convergence and divergence - the states where most
            real-world systems spend most of their time.</p>
            <p>Among other things, the presence calculus equips us with
            the computational tools needed to identify convergent,
            divergent and metastable states of a system of presences and
            monitor how they move in between these states over time.</p>
            <h4 data-number="8.2.1"
            id="the-semantics-of-convergence"><span
            class="header-section-number">8.2.1</span> The semantics of
            convergence</h4>
            <p>An important point to emphasize is that, depending on how
            presence density is interpreted in a given domain,
            <em>any</em> of the three behavioral modes â convergent,
            divergent, or metastable â may be desirable. Convergence is
            not inherently â good,â nor is divergence necessarily
            âbad.â</p>
            <p>For example, in repetitive manufacturing or many customer
            service domains, convergence is often desired. In these
            contexts, presence typically represents <em>demand</em> on a
            constrained resource, and managing the demand is essential
            for ensuring consistent service times, throughput,<br />
            and resource utilization.</p>
            <p>Traditional operations management and queueing theory
            therefore seek out â and emphasize â stability and
            convergence in key operational signals.</p>
            <p>By contrast, if presence represents a companyâs
            <em>customer base</em> or <em>market<br />
            share</em>, we <em>want</em> the long-run presence density
            to look like the chart on<br />
            the right: up and to the right â that is,
            <em>divergent</em>.</p>
            <p>Metastable modes are common in presence signals generated
            in complex<br />
            systems, for example market facing software delivery teams,
            where teams must shift between modes of operation in
            response to external demands or changing market conditions.
            Indeed, the ability to transition between such modes
            effectively is often a hallmark of a high-functioning,
            adaptive software organization â <em>provided</em> it is
            done intentionally and with awareness.</p>
            <p>One of the major practical applications of the presence
            calculus is to<br />
            bring new analytical tools to <em>observe</em>,
            <em>categorize</em>, and <em>steer</em> the<br />
            behavior of such systems â aligning low level presence
            signals with the desired modes of operation in a given
            domain, <em>before</em> critical tipping points are
            reached.</p>
            <h3 data-number="8.3" id="detecting-convergence"><span
            class="header-section-number">8.3</span> Detecting
            convergence</h3>
            <p>In the last section, we <em>defined</em> convergent
            behavior in terms of the<br />
            existence of the limit <span
            class="math inline">\(\Delta\)</span>, the long-run presence
            density<br />
            of the system.</p>
            <p>Now we ask: under what observable conditions does such a
            limit exist?</p>
            <p>If we can identify these conditions, we gain levers to
            begin <em>steering</em><br />
            systems toward desired modes of operation.</p>
            <p>It turns out the answer is hiding in plain sight â in the
            presence<br />
            invariant, which, as weâve seen, holds for <em>any</em>
            finite observation<br />
            window. The limit <span
            class="math inline">\(\Delta\)</span> represents the
            asymptotic value of <span
            class="math inline">\(\delta(t)\)</span>,<br />
            the left-hand side of the invariant, measured over a
            sequence of consecutive overlapping intervals, each one a
            prefix of the sample path.</p>
            <p>For each such prefix interval <span
            class="math inline">\(t\)</span> , the <em>presence
            invariant</em> gives us:</p>
            <p><span class="math display">\[
            \delta(t) = \iota(t) \times \bar{m}(t)
            \]</span></p>
            <p>This tells us that each value of <span
            class="math inline">\(\delta(t)\)</span> is determined by
            the<br />
            product of two measurable quantities: <span
            class="math inline">\(\iota(t)\)</span>, the incidence
            rate,<br />
            and <span class="math inline">\(\bar{m}(t)\)</span>, the
            mass contribution per signal.</p>
            <p>To understand when the long-run value of <span
            class="math inline">\(\delta(t)\)</span> converges, we can
            ask<br />
            a simpler question: do the corresponding long-run s of <span
            class="math inline">\(\iota(t)\)</span><br />
            and <span class="math inline">\(\bar{m}(t)\)</span>
            converge? If both do, we should expect that their product
            â<br />
            and hence <span class="math inline">\(\Delta\)</span> â
            converges as well,and it does, with some technical
            conditions in place<a href="#fn19" class="footnote-ref"
            id="fnref19" role="doc-noteref"><sup>19</sup></a>.</p>
            <p>So, letâs write down precise definitions for the limits
            of <span class="math inline">\(\iota(t)\)</span> and<br />
            <span class="math inline">\(\bar{m}(t)\)</span> and examine
            how these limits behave.</p>
            <h4 data-number="8.3.1"
            id="convergence-of-mass-contribution-per-signal"><span
            class="header-section-number">8.3.1</span> Convergence of
            mass contribution per signal</h4>
            <p>We will derive the limit for <span
            class="math inline">\(\bar{m}\)</span>. We will denote this
            by <span class="math inline">\(\bar{M}\)</span>.</p>
            <p><span class="math display">\[
            \bar{M} = \lim_{T \to \infty} \frac{1}{N(0,T)} \sum_{(e,b)}
            \int_0^T P_{(e,b)}(t) \, dt
            \]</span></p>
            <p>This expression means: for each signal <span
            class="math inline">\((e,b)\)</span>, accumulate its total
            presence mass over time, then sum across all signals, and
            divide by the total number of signals active during that
            window. Each integral in the sum is a row sum in the
            original presence matrix - the mass contribution of an
            individual signal over the interval.</p>
            <p>Thus we can also write this as</p>
            <p><span class="math display">\[
            \bar{M} = \lim_{j \to \infty} \frac{1}{N(1,j)} \sum_{(e,b)}
            \sum_{k=1}^j P_{(e,b)}(k)
            \]</span></p>
            <p><span class="math inline">\(\bar{M}\)</span> is the limit
            of mass contribution per signal over a sufficiently long
            observation interval. Here, <span
            class="math inline">\(P_{(e,b)}(t)\)</span> is the presence
            density function for signal <span
            class="math inline">\((e,b)\)</span>, and <span
            class="math inline">\(N(0,T)\)</span> is the total number of
            signals observed during the interval <span
            class="math inline">\([0,T]\)</span>.</p>
            <p>Letâs work this out for our running example and see what
            it means. Weâll reproduce Figure 9, our starting presence
            matrix, here for easy reference.</p>
            <div style="text-align: center; margin:2em">
            <table>
            <thead>
            <tr>
            <th>
            E
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>
            e1_b1
            </td>
            <td style="background-color: #c6f6c6">
            0.3
            </td>
            <td style="background-color: #c6f6c6">
            2.3
            </td>
            <td style="background-color: #c6f6c6">
            3.4
            </td>
            <td style="background-color: #c6f6c6">
            1.1
            </td>
            <td style="background-color: #c6f6c6">
            2.9
            </td>
            <td style="background-color: #c6f6c6">
            3.2
            </td>
            <td style="background-color: #c6f6c6">
            1.1
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            </tr>
            <tr>
            <td>
            e1_b2
            </td>
            <td style="background-color: #c6f6c6">
            0.3
            </td>
            <td style="background-color: #c6f6c6">
            2.3
            </td>
            <td style="background-color: #c6f6c6">
            3.4
            </td>
            <td style="background-color: #c6f6c6">
            1.1
            </td>
            <td>
            0.0
            </td>
            <td style="background-color: #c6f6c6">
            1.1
            </td>
            <td style="background-color: #c6f6c6">
            2.2
            </td>
            <td style="background-color: #c6f6c6">
            2.4
            </td>
            <td style="background-color: #c6f6c6">
            2.3
            </td>
            <td style="background-color: #c6f6c6">
            0.8
            </td>
            </tr>
            <tr>
            <td>
            e2_b2
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td style="background-color: #c6f6c6">
            0.9
            </td>
            <td style="background-color: #c6f6c6">
            1.8
            </td>
            <td style="background-color: #c6f6c6">
            3.2
            </td>
            <td style="background-color: #c6f6c6">
            0.9
            </td>
            </tr>
            <tr>
            <td>
            e2_b1
            </td>
            <td style="background-color: #c6f6c6">
            0.8
            </td>
            <td style="background-color: #c6f6c6">
            1.3
            </td>
            <td style="background-color: #c6f6c6">
            2.4
            </td>
            <td style="background-color: #c6f6c6">
            2.8
            </td>
            <td style="background-color: #c6f6c6">
            3.0
            </td>
            <td style="background-color: #c6f6c6">
            3.2
            </td>
            <td style="background-color: #c6f6c6">
            3.4
            </td>
            <td style="background-color: #c6f6c6">
            2.4
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            </tr>
            </tbody>
            </table>
            </div>
            <p>The cumulative mass per signal over each interval <span
            class="math inline">\([1, j], j \le 10\)</span> is shown
            below. Each value in this matrix is the sum of all the
            values in that row to the left (inclusive) of the value.</p>
            <figure id="fig:signal-mass-contribution-matrix"
            class="subfigures figure">
            <img src="../assets/placeholder.png"
            id="fig:signal-mass-contribution-matrix"
            style="display: none;" alt="a" />
            <table>
            <thead>
            <tr>
            <th>
            E
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>
            e1_b1
            </td>
            <td>
            0.3
            </td>
            <td>
            2.6
            </td>
            <td>
            6.0
            </td>
            <td>
            7.1
            </td>
            <td>
            10.0
            </td>
            <td>
            13.2
            </td>
            <td>
            14.3
            </td>
            <td>
            14.3
            </td>
            <td>
            14.3
            </td>
            <td>
            14.3
            </td>
            </tr>
            <tr>
            <td>
            e1_b2
            </td>
            <td>
            0.3
            </td>
            <td>
            2.6
            </td>
            <td>
            6.0
            </td>
            <td>
            7.1
            </td>
            <td>
            7.1
            </td>
            <td>
            8.2
            </td>
            <td>
            10.4
            </td>
            <td>
            12.8
            </td>
            <td>
            15.1
            </td>
            <td>
            15.9
            </td>
            </tr>
            <tr>
            <td>
            e2_b2
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.0
            </td>
            <td>
            0.9
            </td>
            <td>
            2.7
            </td>
            <td>
            5.9
            </td>
            <td>
            6.8
            </td>
            </tr>
            <tr>
            <td>
            e2_b1
            </td>
            <td>
            0.8
            </td>
            <td>
            2.1
            </td>
            <td>
            4.5
            </td>
            <td>
            7.3
            </td>
            <td>
            10.3
            </td>
            <td>
            13.5
            </td>
            <td>
            16.9
            </td>
            <td>
            19.3
            </td>
            <td>
            19.3
            </td>
            <td>
            19.3
            </td>
            </tr>
            </tbody>
            </table>
            <figcaption><p>Figure 22: Signal mass contribution matrix.
            </p></figcaption>
            </figure>
            <p>Now lets chart each row of this matrix to see how the
            mass contribution for each signal grows over time. In
            FigureÂ 23 we are showing each row in the matrix as a line in
            the chart.</p>
            <figure id="fig:mass-contribution-per-signal">
            <img src="../assets/pandoc/mass_contribution_per_signal.png"
            alt="Figure 23: Mass contributions of each signal over time" />
            <figcaption aria-hidden="true">Figure 23: Mass contributions
            of each signal over time</figcaption>
            </figure>
            <p>Finally FigureÂ 24 shows the how mass contribution per
            signal grows over time. Each point in this chart represents
            the mass contributions per signal for the window <span
            class="math inline">\([1,j]\)</span> which is the sum of the
            value in column j divided by the number of non-zero rows in
            the sub-matrix spanned by the columns in <span
            class="math inline">\([1,j]\)</span>.</p>
            <p>As we can see, this curve converges to a limit.</p>
            <figure id="fig:avg-mass-contribution-per-signal">
            <img
            src="../assets/pandoc/avg_mass_contribution_per_signal.png"
            alt="Figure 24: Convergence of mass contribution per signal" />
            <figcaption aria-hidden="true">Figure 24: Convergence of
            mass contribution per signal</figcaption>
            </figure>
            <p>So lets ask, what would make the mass contribution per
            signal <em>not</em> converge to a finite value?</p>
            <p>FigureÂ 23 suggests that the mass contribution of every
            individual signal is monotonically non-decreasing and it
            increases continuously over every non-zero support interval
            of the signal and flattens out over every interval where the
            underlying signal is zero.</p>
            <p>Suppose when measured over a sufficient long interval,
            each signal remains bounded, that is every onset is matched
            with a corresponding reset, then each individual signal
            contributes a finite mass to the cumulative value.</p>
            <p>Thus, the only way the cumulative mass can grow without
            limit is if <em>some</em> signal grows without limit.</p>
            <figure id="fig:onset-reset-patterns">
            <img src="../assets/pandoc/pdf_examples.png"
            alt="Figure 25: Onset-reset-patterns" />
            <figcaption aria-hidden="true">Figure 25:
            Onset-reset-patterns</figcaption>
            </figure>
            <p>For example, in FigureÂ 25 we show some onset-reset
            patterns for signals and the signal for Element-4, which has
            an onset but no apparent reset within the observation
            window, would grow without limit in FigureÂ 23 assuming there
            was no reset.</p>
            <p>This gives us the first condition for convergence of
            <span class="math inline">\(\Delta\)</span> :</p>
            <div
            style="border: 1px solid #ccc; border-radius: 6px; padding: 1em; background-color: #f9f9f9; margin: 2em 0;">
            <p><em>Boundedness of Signal Mass</em></p>
            <p><em>In a convergent system of presences, every signal
            onset is eventually followed by a corresponding reset, when
            observed over a sufficiently long time interval.</em></p>
            </div>
            <p>Weâll note, once again, that depending upon the semantics
            of the domain, we may or may not want to have this condition
            hold depending upon whether we are looking to steer the
            system towards convergence or towards divergence.</p>
            <p>For example, if a signal represents a new revenue source,
            a mass contribution represents incremental revenues and
            ideally we want many onsets without matching resets: every
            reset corresponds to a lost revenue stream.</p>
            <p>If, on the other hand, a signal onset represents a new
            unfinished task on your to-do list, then a reset marks its
            completion â and convergence becomes desirable, as it
            indicates tasks are being completed in a timely manner and
            that your todo list is not growing without limit.</p>
            <h4 data-number="8.3.2"
            id="convergence-of-incidence-rate"><span
            class="header-section-number">8.3.2</span> Convergence of
            Incidence Rate</h4>
            <p>We now define the long-run incidence rate, denoted <span
            class="math inline">\(I\)</span>, in exact analogy to how we
            defined the mass contribution per signal <span
            class="math inline">\(\bar{M}\)</span>. Recall that <span
            class="math inline">\(\iota(t)\)</span> is the incidence
            rate observed over the interval <span
            class="math inline">\([0, t]\)</span>, defined as the number
            of signals observed in that interval divided by its
            duration. Then we define the long-run incidence rate as the
            following limit:</p>
            <p><span class="math display">\[
            I = \lim_{T \to \infty} \iota(T) = \lim_{T \to \infty}
            \frac{N(0, T)}{T}
            \]</span></p>
            <p>where <span class="math inline">\(N(0, T)\)</span> is the
            number of signals observed over the interval <span
            class="math inline">\([0, T]\)</span>â that is, the number
            of distinct element-boundary signals that are active at some
            point during the interval. The incidence rate measures how
            many such signals are activated, per unit time.</p>
            <p>This limit <span class="math inline">\(I\)</span>, when
            it exists, represents the asymptotic rate at which signals
            appear in the system. It plays a symmetric role to <span
            class="math inline">\(\bar{M}\)</span> in the convergence of
            the presence density, and its existence is the second key
            condition we will examine next.</p>
            <p>To better understand the behavior of the incidence rate
            <span class="math inline">\(\iota(T)\)</span>, letâs now
            examine the cumulative signal count <span
            class="math inline">\(N(0,T)\)</span> over the interval
            <span class="math inline">\([0,T]\)</span> for increasing
            values of <span class="math inline">\(T\)</span>. This is
            directly analogous to how we analyzed the growth of
            cumulative mass contributions per signal when analyzing
            <span class="math inline">\(\bar{M}\)</span>.</p>
            <p>Recall that for a given observation window <span
            class="math inline">\([0,T]\)</span>, <span
            class="math inline">\(N(0,T)\)</span> counts the number of
            element-boundary signals that are active at some point
            within the interval. We can compute this by scanning across
            the presence matrix and, for each column (from <span
            class="math inline">\(t=1\)</span> to <span
            class="math inline">\(t=T\)</span>), counting how many
            <em>new</em> signals appearâthat is, how many unique
            element-boundary rows have non-zero values in that
            column.</p>
            <p>The result is a sequence of signal counts, which we can
            arrange as a <span class="math inline">\(1 \times T\)</span>
            row vector:</p>
            <figure id="fig:incidence-count" class="subfigures figure">
            <img src="../assets/placeholder.png"
            id="fig:incidence-count" style="display: none;" alt="a" />
            <table>
            <thead>
            <tr>
            <th>
            Time
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>
            Signal Count
            </td>
            <td>
            3
            </td>
            <td>
            3
            </td>
            <td>
            3
            </td>
            <td>
            3
            </td>
            <td>
            3
            </td>
            <td>
            3
            </td>
            <td>
            4
            </td>
            <td>
            4
            </td>
            <td>
            4
            </td>
            <td>
            4
            </td>
            </tr>
            </tbody>
            </table>
            <figcaption><p>Figure 26: Signal incidence counts over the
            observation window.. </p></figcaption>
            </figure>
            <p>where <span class="math inline">\(n_j\)</span> is the
            total number of distinct signals that have appeared at or
            before time <span class="math inline">\(j\)</span>. Each
            <span class="math inline">\(n_j\)</span> counts the number
            of signals with support intersecting the interval <span
            class="math inline">\([0, j]\)</span>.</p>
            <p>We can chart this row to visualize how the cumulative
            number of observed signals grows over time. If <span
            class="math inline">\(N(0,T)\)</span> grows linearly in
            <span class="math inline">\(T\)</span>, then the incidence
            rate <span class="math inline">\(\iota(T) =
            N(0,T)/T\)</span> should converge to a finite value <span
            class="math inline">\(I\)</span>. On the other hand, if
            <span class="math inline">\(N(0,T)\)</span> grows faster
            than linearly, the incidence rate will divergeâand if it
            grows sublinearly, the rate will decay toward zero.</p>
            <figure id="fig:signal-incidence-rate">
            <img src="../assets/pandoc/avg_incidence_rate.png"
            alt="Figure 27: Signal incidence rate" />
            <figcaption aria-hidden="true">Figure 27: Signal incidence
            rate</figcaption>
            </figure>
            <p>FigureÂ 27 shows the incidence rate <span
            class="math inline">\(\iota(T) = N(0,T)/T\)</span> over
            time. In this example, the rate initially decreases and then
            stabilizes, since the number of distinct signals <span
            class="math inline">\(N(0,T)\)</span> stops increasing after
            a point. In general, the incidence rate will converge to a
            finite limit if <span class="math inline">\(N(0,T)\)</span>
            grows no faster than linearly with <span
            class="math inline">\(T\)</span>. If <span
            class="math inline">\(N(0,T)\)</span> grows <em>faster</em>
            than <span class="math inline">\(T\)</span>, the ratio <span
            class="math inline">\(\iota(T)\)</span> will
            divergeâindicating that signals are being activated at an
            unbounded rate. Conversely, if <span
            class="math inline">\(N(0,T)\)</span> grows <em>slower</em>
            than <span class="math inline">\(T\)</span>, the incidence
            rate will decay toward zero. Thus, convergence of <span
            class="math inline">\(\iota(T)\)</span> requires that <span
            class="math inline">\(N(0,T)\)</span> grows approximately
            linearly in <span class="math inline">\(T\)</span>.</p>
            <p>This kind of divergence typically arises in systems where
            the <em>onset rate</em>â the rate at which new signals are
            activatedâexceeds the <em>reset rate</em>, which closes
            those signals. While transient imbalances between onsets and
            resets are common during transitions between equilibrium
            states, divergence only occurs if this imbalance is
            sustained indefinitely. In that case, <span
            class="math inline">\(N(0,T)\)</span> grows without bound
            relative to <span class="math inline">\(T\)</span>, and the
            system exhibits an asymptotically increasing incidence rate.
            So, divergence of <span
            class="math inline">\(\iota(T)\)</span> directly reflects a
            persistent structural imbalance between signal onsets and
            resets over time.</p>
            <div
            style="border: 1px solid #ccc; border-radius: 6px; padding: 1em; background-color: #f9f9f9; margin: 2em 0;">
            <p><b>Boundedness of incidence rate</b></p>
            <p><i> In a convergent system of presences, the long-run
            rate of signal onsets does not exceed the long-run rate of
            resets, when observed over a sufficiently long time
            interval. </i></p>
            </div>
            <h4 data-number="8.3.3" id="recap"><span
            class="header-section-number">8.3.3</span> Recap</h4>
            <p>We began by defining convergence in terms of the
            existence of a long-run limit for presence density.</p>
            <p>We then showed how the existence of this global limit
            depends on the existence of two other measurable limits: the
            incidence rate of signals and their mass contribution.</p>
            <p>Next, we traced each of these limits back to the local
            behavior of individual signalsâspecifically, the presence or
            absence of well-behaved signal onsets and resets.</p>
            <p>With this connection in place, we now have a principled
            way to reason about the global convergence or divergence of
            a system by analyzing the patterns of local signal behavior
            over time.</p>
            <h4 data-number="8.3.4"
            id="formal-proof-of-convergence-and-littles-law"><span
            class="header-section-number">8.3.4</span> Formal proof of
            convergence and Littleâs Law</h4>
            <p>In this document, we have presented a somewhat
            simplifiedâ account of the criteria required to ensure that
            a system of presences is convergent. Specifically, based on
            the definitions above, we assert that for a given system of
            presences, if the limits <span
            class="math inline">\(I\)</span> and <span
            class="math inline">\(\bar{M}\)</span> exist and are finite,
            then the limit <span class="math inline">\(\Delta\)</span>
            also exists and is finite. Furthermore, we claim that</p>
            <p><span class="math display">\[
            \Delta = I \times \bar{M}
            \]</span></p>
            <p>Technically, this relationship does not follow
            automatically from the arguments we have presented so far.
            In fact, the statement above is a restatement of a
            generalized form of Littleâs Law originally proven by
            Brumelle <span class="citation" data-cites="brumelle71"><a
            href="#ref-brumelle71" role="doc-biblioref">[4]</a></span>,
            and later by using sample path techniques by Heyman and
            Stidham <span class="citation" data-cites="heyman80"><a
            href="#ref-heyman80" role="doc-biblioref">[5]</a></span>.
            The full proofâalong with the additional technical
            conditions required to ensure that the limit of the product
            equals the product of the limitsâis a discussed in a
            separate document on our theory track.</p>
            <p>For our purposes, it is safe to state that this
            relationship, and the conditions under which it holds,
            <em>constitute</em> a general form of Littleâs Law for a
            system of presences.</p>
            <p>This general form of Littleâs Law is usually presented in
            the form <span class="math inline">\(H = \lambda \cdot
            G.\)</span> In our notation <span class="math inline">\(H =
            \Delta\)</span>, <span class="math inline">\(\lambda =
            I\)</span> and <span class="math inline">\(\bar{M} =
            G\)</span> We have chosen to develop a new consistent
            notation to describe these terms as limiting values of
            measure-theoretic parameters <span
            class="math inline">\(\delta, \iota, \text{ and }
            \bar{m}\)</span> of the presence invariant, but the
            underlying terms can be shown to be equivalent to each
            other.</p>
            <p>For a more detailed explanation of the correspondence
            between the two, please see the document <a
            href="./generalized_littles_law.html">Convergence of systems
            of presence</a> on our Theory Track.</p>
            <h4 data-number="8.3.5"
            id="convergence-coherence-and-littles-law"><span
            class="header-section-number">8.3.5</span> Convergence,
            coherence, and Littleâs Law</h4>
            <p>One important point to note is that âLittleâs Lawâ is not
            a single law, but rather a family of related laws that apply
            at different time scales, in different forms, and with
            different interpretations. The presence invariant is the
            most general version of this law, as it holds at all time
            scales.</p>
            <p>In this document, we stated it as a relationship between
            presence density, signal incidence rate, and mass
            contribution per signal over any finite observation window.
            This relationship holds at all scales, <em>including</em>
            those sufficiently long windows where the parameters
            approach their limits: <span
            class="math inline">\(\Delta\)</span>, <span
            class="math inline">\(I\)</span>, and <span
            class="math inline">\(\bar{M}\)</span>.</p>
            <p>It is natural to ask: what is special, if anything, about
            those limiting values?</p>
            <p>Without going too deeply into technical arguments here,
            we note that the limits are indeed meaningful. When a system
            is observed over a non-convergent interval, the quantities
            in the presence invariant are dominated by <em>partial</em>
            mass contributions from signals. The first interval
            FigureÂ 28 shows an example of this behavior.</p>
            <figure id="fig:presence-invariant-continuous-2">
            <img
            src="../assets/pandoc/convergent-divergent-intervals.png"
            alt="Figure 28: Convergent and non-convergent intervals" />
            <figcaption aria-hidden="true">Figure 28: Convergent and
            non-convergent intervals</figcaption>
            </figure>
            <p>The system may <em>appear</em> convergent when the window
            is long enough for <em>complete</em> signals to dominate the
            presence density. For example, the interval <span
            class="math inline">\(T&#39;\)</span> in FigureÂ 28 includes
            full support for <em>nearly</em> all the signals, and in
            this case the system would appear convergent over that
            interval.</p>
            <p>The main difference between the two intervals in
            FigureÂ 28 is that in the latter, the mass contributions from
            the signals include nearly the entire presence mass of each
            each signal.</p>
            <blockquote>
            <p>In other words, if the observation window is long enough
            that most signal contributions equal the masses of the
            signals in the window, the system will appear convergent
            over the interval and <span
            class="math inline">\(\bar{m}\)</span> as measured for that
            interval will be close to its limit value <span
            class="math inline">\(\bar{M}\)</span> for the system as
            whole.</p>
            </blockquote>
            <p>The key point here is that in such situations, the
            presence invariant is not just a relationship among mass
            <em>contributions</em>, but <em>also</em> implicitly a
            relationship among the <em>masses</em> of the signals
            involved. This distinction has direct operational
            implications.</p>
            <p>For instance, if the signals in FigureÂ 28 represent
            customer service times, then over a convergent interval,
            mass contributions equal signal masses and reflect what the
            customer actually experiences. But over shorter,
            non-convergent intervals, those same contributions primarily
            reflect partial massesâwhat a <em>system operator</em> might
            observe on a day-to-day basis.</p>
            <p>In FigureÂ 28, the second interval represents the state
            where the mass contribution per signal is near its limit
            value <span class="math inline">\(\bar{M}\)</span>. When we
            measure presence density, signal masses, and incidence rates
            over this longer window, we are implicitly aligning the
            customerâs perspective with the operatorâs.</p>
            <p>In this way, convergence brings these two
            perspectivesâthe customerâs and the operatorâsâinto
            alignment. More generally, if a system is convergent over a
            long enough observation window, the invariant over those
            intervals expresses a relationship between presence density
            in the time domain and mass per signal ( which also equals
            mass contribution per signal) in the element-boundary
            domain.</p>
            <p>We will also note that in these cases, there is a
            parallel argument that can be made about the incidence rate
            <span class="math inline">\(I\)</span> and signal onset
            rates and reset rates. When the observation window is long
            enough that it includes complete signal masses, this implies
            will also have fewer signals where onsets are not matched
            with resets.</p>
            <p>So over those intervals the incidence rate, onset rates
            and reset rates will all converge to the same limiting value
            <span class="math inline">\(I\)</span> over the interval.
            These are the well known equilibrium conditions we call the
            âconservation of flowâ under the classic treatments of
            Littleâs Law, but now generalized to arbitrary systems of
            presences.</p>
            <blockquote>
            <p>When the parameters of the presence invariant over an
            interval are at or close to their limit values, the system
            is in a state of <em>epistemic coherence</em>: multiple
            observers, using different vantage points, arrive at a
            consistent <em>interpretation</em> of parameters in the
            presence invariant.</p>
            </blockquote>
            <p>This coherent state occurs only when the system is
            operating at or near equilibrium.</p>
            <p>We will return to this idea in future posts, particularly
            in the context of flow measurement in systems that operate
            far from equilibrium. But for now, it is enough to recognize
            that identifying whether a system is operating in a
            convergent or divergent mode is fundamental to making
            meaningful decisions when reasoning about a system of
            presences.</p>
            <h4 data-number="8.3.6" id="a-note-on-determinism"><span
            class="header-section-number">8.3.6</span> A note on
            determinism</h4>
            <p>A final point we emphasize in this section is that the
            form of Littleâs Law derived here is entirely deterministic.
            It does not depend on any probabilistic or stochastic
            assumptions about the behavior of the system. In fact, the
            notion of a sample path used here originates in a
            deterministic proof of Littleâs Law by Stidham in 1972. The
            presence invariant, as we have introduced it, is a direct
            analogue of the finite-window constructs used in that
            proof.</p>
            <p>It is important to recognize that Littleâs Law is not a
            statistical artifact. It is a structural property deeply
            woven into the behavior of of <em>any</em> system of
            presences. Convergence and divergence are deterministic
            features of how signals evolve and interact over
            timeâregardless of whether the underlying signals are random
            or not.</p>
            <p>Even when the signals have randomness, the
            <em>observed</em> evolution of presence density is
            deterministic<a href="#fn20" class="footnote-ref"
            id="fnref20" role="doc-noteref"><sup>20</sup></a> and
            governed by the law of conservation of presence massâthat
            is, the presence invariant. This determinism extends to any
            functional quantity that depends on presence density. As we
            will see, a large and operationally useful class of system
            behaviors can be characterized in terms of the presence
            density of domain signals. That is why the machinery
            developed here is more than just a theoretical
            curiosity.</p>
            <h3 data-number="8.4"
            id="the-presence-invariant-and-rate-conservation-laws"><span
            class="header-section-number">8.4</span> The presence
            invariant and rate conservation laws</h3>
            <p>This is our main point of departure in the presence
            calculus: we treat equilibrium not as a precondition for
            Littleâs Law, but as a special case of a more general
            principle.</p>
            <p>We place the <em>finite</em> version of Littleâs Lawâin
            the general form of the presence invariantâat the center of
            our analysis, because it continues to hold and yield
            meaningful insight even when the system is far from
            equilibrium. These are precisely the operating modes that
            traditional queueing theory and other classical approaches
            de-emphasizeâbut where most real-world systems actually
            live.</p>
            <p>In fact, a broader principle is at work. Miyazawa <span
            class="citation" data-cites="miyazawa94"><a
            href="#ref-miyazawa94" role="doc-biblioref">[8]</a></span>
            was among the first to identify a general class of <em>rate
            conservation laws</em> that allow relationships like the
            generalized Littleâs Law to be derived in wider settings.
            Sigman <span class="citation" data-cites="sigman91"><a
            href="#ref-sigman91" role="doc-biblioref">[9]</a></span>
            showed that the generalized form of Littleâs Law can be seen
            as an instance of such rate conservation principles.</p>
            <p>The document <a
            href="./presence_invariant_and_rate_conservation_laws.html">The
            Presence Invariant and Rate Conservation Laws</a> in our
            theory track explores this connection in detail.</p>
            <p>From this perspective, the presence calculus offers
            constructive tools to <em>discover</em> and formalize
            conservation laws within a given domain.</p>
            <blockquote>
            <p>Every system of presences may be considered to generate a
            rate conservation law when presence is interpreted according
            to the semantics of the domain.</p>
            </blockquote>
            <p>The interactions of element-boundary signals in any
            system of presences naturally give rise to rate conservation
            laws based on the principle of conserved signal mass.
            Mapping these back into the language of the domain appears
            to be a fruitful path for uncovering the mechanisms by which
            systems in that domain evolve.</p>
            <h2 data-number="9" id="visualizing-signal-dynamics"><span
            class="header-section-number">9</span> Visualizing signal
            dynamics</h2>
            <p>Convergence, as discussed in the last section, is a
            fundamental concept in the presence calculus. We now have
            the tools to detect convergence or divergence in the
            long-run behavior of a system of presencesâspecifically, the
            evolution of presence density and its underlying drivers:
            signal mass contributions and signal incidence rates.</p>
            <p>As noted earlier, <em>whenever we can model the
            meaningful behaviors of a system as interactions between
            element-boundary signals within a system of presences</em>,
            the presence calculus gives a constructive analytical
            framework for studying signal dynamics of the system.</p>
            <p>This framework applies to a wide range of operational
            problems across many domains, and should be viewed as an
            alternate analytical lens to statistical analysis of
            operational data.</p>
            <p>We also observed that much of the structure in the
            presence calculus is<br />
            deterministic. That is, given the observed behavior of a
            system of presences, we can deterministically explain the
            evolution of presence density in terms of its drivers,
            signal mass and incidence rateâacross time and across
            different timescales.</p>
            <p>In this section, we elaborate on this deterministic
            structure and introduce<br />
            new tools that help us analyze how local behaviors evolve
            into global patterns, and help us understand the dynamics of
            a system of presence.</p>
            <p>First we will develop fundamental tools that help us
            establish a uniform sense of place and direction of the
            system evolution, that scales from the local to global
            timescales.</p>
            <p>While convergence and divergence establish this at the
            macro scale, these concepts are somewhat unwieldy and
            inapplicable when the system is operating far from
            equilibrium, and we need some better tools to navigate in
            this complementary region.</p>
            <p>We will show how to detect emerging patterns in this
            evolution that reveal the <em>direction</em> in which the
            system is movingâand just as importantly, how to detect when
            that direction is <em>changing</em>.</p>
            <p>These are essential capabilities for building mechanisms
            that can steer system behavior in a desired directionâtoward
            convergence or divergence of presence density.</p>
            <h3 data-number="9.1" id="phase-space"><span
            class="header-section-number">9.1</span> The presence
            invariant in phase space</h3>
            <p>In the previous section, we showed that the accumulation
            of presence density across time and timescales follows a
            deterministic ruleâthe <em>presence accumulation
            recurrence</em>. That rule described how presence density
            evolves, but only in terms of <em>magnitude</em> and
            <em>scale</em>.</p>
            <p>We now introduce machinery that allows us to understand
            the <em>direction</em> in which the system is evolving. To
            do this, we recast the entries in the accumulation matrix
            using our central tool: the <em>presence invariant</em>.</p>
            <p>Since the presence invariant holds at each cell of the
            accumulation matrix, we can write:</p>
            <p><span class="math display">\[
            \delta_{i,j} = \iota_{i,j} \cdot \bar{m}_{i,j}
            \]</span></p>
            <p>This is just the familiar identity <span
            class="math inline">\(\delta = \iota \cdot \bar{m}\)</span>
            applied at each interval <span
            class="math inline">\((i,j)\)</span> in the matrix.</p>
            <blockquote>
            <p>Changes in presence density are thus driven by changes in
            the product of incidence rate and signal mass
            contribution.</p>
            </blockquote>
            <p>Because products are more difficult to reason about
            directly than sums, we analyze the system in <em>logarithmic
            space</em>, where the invariant becomes additive:</p>
            <p><span class="math display">\[
            \log \delta = \log \iota + \log \bar{m}
            \]</span></p>
            <p>This additive form reveals how changes in rate and mass
            contribute linearly (in log space) to changes in presence
            density.</p>
            <p>We now introduce a compact coordinate system for
            visualizing these dynamics by embedding the two terms into
            the complex plane:</p>
            <p><span class="math display">\[
            z = \log \iota + i \log \bar{m}
            \]</span></p>
            <p>Here, <span class="math inline">\(\Re(z) = \log
            \iota\)</span> and <span class="math inline">\(\Im(z) = \log
            \bar{m}\)</span>. This maps each interval to a point in
            <span class="math inline">\(\mathbb{C}\)</span> representing
            the <em>logarithmic decomposition</em> of the observed
            presence density.</p>
            <p>This representation has powerful interpretive value:</p>
            <ul>
            <li>The <em>magnitude</em> of <span
            class="math inline">\(z\)</span> reflects the
            <em>intensity</em> of presence accumulation.</li>
            <li>The <em>phase angle</em> (or phase) of <span
            class="math inline">\(z\)</span> reflects whether changes in
            <span class="math inline">\(\delta\)</span> are primarily
            driven by <span class="math inline">\(\iota\)</span> (rate)
            or <span class="math inline">\(\bar{m}\)</span> (mass).</li>
            </ul>
            <p>Letâs define these explicitly:</p>
            <ul>
            <li><p>The <em>norm</em> of <span
            class="math inline">\(z\)</span>:</p>
            <p><span class="math display">\[
            \|z\| = \sqrt{(\log \iota)^2 + (\log \bar{m})^2}
            \]</span></p>
            <p>This gives a scale-invariant measure of the strength of
            presence density, combining incidence and mass into a single
            quantity.</p></li>
            <li><p>The <em>phase angle</em> of <span
            class="math inline">\(z\)</span>:</p>
            <p><span class="math display">\[
            \theta = \arg(z) = \mathrm{atan2}(\log \bar{m}, \log \iota)
            \]</span></p>
            <p>This describes the balance between rate and mass:</p>
            <ul>
            <li>Positive <span class="math inline">\(\theta\)</span>:
            mass-dominant behavior (fewer, more massive signals).</li>
            <li>Negative <span class="math inline">\(\theta\)</span>:
            rate-dominant behavior (many small signals),</li>
            </ul></li>
            </ul>
            <p>Together, the polar representation</p>
            <p><span class="math display">\[
            z = \|z\| \cdot e^{i\theta}
            \]</span></p>
            <p>allows us to interpret each intervalâs presence dynamics
            in terms of both <em>intensity</em> and
            <em>direction</em>.</p>
            <blockquote>
            <p>We now have a formal notion of <em>flow</em> for presence
            in log-spaceâone that combines incidence and mass into a
            single complex number that represents intensity and
            direction of presence accumulations.</p>
            </blockquote>
            <p>FigureÂ 29 shows this mapping of the presence accumulation
            matrix into polar co-ordinates in the complex plane. We will
            call this the <em>presence accumulation dual.</em></p>
            <figure id="fig:complex-plane-table"
            class="subfigures figure">
            <img src="../assets/placeholder.png"
            id="fig:complex-plane-table" style="display: none;"
            alt="a" />
            <table style="border-collapse: collapse; margin: auto; font-family: serif; font-size: 0.95em;">
            <thead>
            <tr>
            <th>
            i
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>
            <b>1</b>
            </td>
            <td style="background-color: #eef;">
            1.34 â  -0.61
            </td>
            <td style="background-color: #ddf;">
            0.98 â  1.14
            </td>
            <td style="background-color: #eef;">
            1.70 â  1.57
            </td>
            <td style="background-color: #ddf;">
            1.99 â  1.72
            </td>
            <td style="background-color: #eef;">
            2.27 â  1.80
            </td>
            <td style="background-color: #ddf;">
            2.55 â  1.85
            </td>
            <td style="background-color: #eef;">
            2.43 â  1.80
            </td>
            <td style="background-color: #ddf;">
            2.60 â  1.84
            </td>
            <td style="background-color: #eef;">
            2.74 â  1.87
            </td>
            <td style="background-color: #ddf;">
            2.80 â  1.90
            </td>
            </tr>
            <tr>
            <td>
            <b>2</b>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            1.29 â  0.55
            </td>
            <td style="background-color: #ddf;">
            1.67 â  1.32
            </td>
            <td style="background-color: #eef;">
            1.90 â  1.57
            </td>
            <td style="background-color: #ddf;">
            2.18 â  1.70
            </td>
            <td style="background-color: #eef;">
            2.47 â  1.78
            </td>
            <td style="background-color: #ddf;">
            2.36 â  1.74
            </td>
            <td style="background-color: #eef;">
            2.54 â  1.79
            </td>
            <td style="background-color: #ddf;">
            2.68 â  1.83
            </td>
            <td style="background-color: #eef;">
            2.74 â  1.87
            </td>
            </tr>
            <tr>
            <td>
            <b>3</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            1.57 â  0.80
            </td>
            <td style="background-color: #ddf;">
            1.61 â  1.32
            </td>
            <td style="background-color: #eef;">
            1.90 â  1.57
            </td>
            <td style="background-color: #ddf;">
            2.24 â  1.70
            </td>
            <td style="background-color: #eef;">
            2.19 â  1.67
            </td>
            <td style="background-color: #ddf;">
            2.38 â  1.74
            </td>
            <td style="background-color: #eef;">
            2.53 â  1.79
            </td>
            <td style="background-color: #ddf;">
            2.60 â  1.84
            </td>
            </tr>
            <tr>
            <td>
            <b>4</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            1.21 â  0.44
            </td>
            <td style="background-color: #ddf;">
            1.35 â  1.27
            </td>
            <td style="background-color: #eef;">
            1.81 â  1.57
            </td>
            <td style="background-color: #ddf;">
            1.87 â  1.57
            </td>
            <td style="background-color: #eef;">
            2.11 â  1.68
            </td>
            <td style="background-color: #ddf;">
            2.29 â  1.75
            </td>
            <td style="background-color: #eef;">
            2.36 â  1.81
            </td>
            </tr>
            <tr>
            <td>
            <b>5</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            1.28 â  1.00
            </td>
            <td style="background-color: #ddf;">
            1.55 â  1.31
            </td>
            <td style="background-color: #eef;">
            1.68 â  1.40
            </td>
            <td style="background-color: #ddf;">
            1.93 â  1.57
            </td>
            <td style="background-color: #eef;">
            2.12 â  1.68
            </td>
            <td style="background-color: #ddf;">
            2.20 â  1.76
            </td>
            </tr>
            <tr>
            <td>
            <b>6</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            1.43 â  0.70
            </td>
            <td style="background-color: #ddf;">
            1.50 â  1.09
            </td>
            <td style="background-color: #eef;">
            1.72 â  1.40
            </td>
            <td style="background-color: #ddf;">
            1.92 â  1.57
            </td>
            <td style="background-color: #eef;">
            1.99 â  1.68
            </td>
            </tr>
            <tr>
            <td>
            <b>7</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            1.53 â  0.43
            </td>
            <td style="background-color: #ddf;">
            1.44 â  1.07
            </td>
            <td style="background-color: #eef;">
            1.62 â  1.39
            </td>
            <td style="background-color: #ddf;">
            1.68 â  1.57
            </td>
            </tr>
            <tr>
            <td>
            <b>8</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            1.35 â  0.62
            </td>
            <td style="background-color: #ddf;">
            1.45 â  1.29
            </td>
            <td style="background-color: #eef;">
            1.53 â  1.57
            </td>
            </tr>
            <tr>
            <td>
            <b>9</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            1.23 â  0.97
            </td>
            <td style="background-color: #ddf;">
            1.28 â  1.57
            </td>
            </tr>
            <tr>
            <td>
            <b>10</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            0.71 â  -0.23
            </td>
            </tr>
            </tbody>
            </table>
            <figcaption><p>Figure 29: The presence accumulation dual:
            the presence accumulation matrix in polar co-ordinates on
            the complex plane.. </p></figcaption>
            </figure>
            <h3 data-number="9.2" id="flow-fields"><span
            class="header-section-number">9.2</span> Flow fields</h3>
            <p>FigureÂ 29 is not particularly insightful, so letâs
            visualize this as a field of vectors as in FigureÂ 30. We
            will call this the flow field for the system.</p>
            <figure id="fig:flow-field">
            <img src="../assets/pandoc/flow_field.png"
            alt="Figure 30: Flow field visualization" />
            <figcaption aria-hidden="true">Figure 30: Flow field
            visualization</figcaption>
            </figure>
            <p>To understand its construction, letâs go back and start
            with FigureÂ 29. The grid represents rows and columns of the
            accumulation matrix and represents timescales.</p>
            <ul>
            <li>Each cell is represented by the log-space vector of the
            presence density over an observation window in polar
            coordinates.</li>
            <li>The length of the vector encodes magnitude <span
            class="math inline">\(\|z\|\)</span> and the orientation of
            the vector encodes <span
            class="math inline">\(\theta\)</span> in radians.</li>
            </ul>
            <p>Since the matrix in FigureÂ 29 is upper triangular, there
            is no significant information encoded in the bottom half of
            the matrix. So as a convention, when we visualize it we will
            drop the bottom half of the matrix and only show the entries
            in the upper diagonal.</p>
            <p>Now imagine this matrix rotated by 90 degrees so that
            entries on the diagonal are on the bottom row, the entries
            on the next diagonal are laid out above it, and so on until
            the we get to the top left entry in the matrix. The result
            in the pyramid shape of the flow field diagram in
            FigureÂ 30.</p>
            <p>We can also think of this as a compact visualization of
            the pyramid in FigureÂ 14 represented in polar-coordinates.
            Each vector in the flow field is drawn with a proportional
            magnitude and theta.</p>
            <p>FigureÂ 30 compresses a tremendous amount information
            about the dynamics of a system into a very compact
            representation that is easy to scan and interpret visually
            as well as analytically.</p>
            <h3 data-number="9.3" id="interpreting-flow-fields"><span
            class="header-section-number">9.3</span> Interpreting flow
            fields</h3>
            <p>When interpreting flow fields, we are not interested in
            absolute magnitudes or angles for the most part.</p>
            <p>Rather we focus on the <em>relative change in magnitude
            and direction</em> of these vectors as we sweep left to
            right in time along a row and from bottom to top in time
            scales across the rows.</p>
            <blockquote>
            <p>That is, the flow field encodes the <em>dynamics</em> of
            the system across time and time scales.</p>
            </blockquote>
            <p>The phase angle <span
            class="math inline">\(\theta\)</span>, provides a concise,
            single-value representation of the systemâs directional
            flow. Its values range from <span
            class="math inline">\(-\pi\)</span> to <span
            class="math inline">\(\pi\)</span> radians, spanning all
            four quadrants of the complex log-space. In this log-space,
            a positive value signifies the original quantity is greater
            than 1, a negative value indicates itâs less than 1, and a
            zero value means itâs exactly 1.</p>
            <p>The table in FigureÂ 31 interprets that range, showing how
            the combination of incidence rate and mass per signal
            contributes to the systemâs overall dynamics, indicating
            whether growth or decline is primarily driven by rate, mass,
            or a balanced interplay between them.</p>
            <figure id="fig:flow-interpretation"
            class="subfigures figure">
            <p><img src="../assets/placeholder.png" id="fig:"
            style="display: none;" alt="a" /></p>
            <div style="text-align: center; margin: 2em">
            <table style="border-collapse: collapse; margin: auto; font-family: sans-serif; font-size: 0.75em;">
            <thead>
            <tr>
            <th style="padding: 0.2em 0.3em;">
            Î¸ (rad)
            </th>
            <th style="padding: 0.2em 0.3em;">
            Dir.
            </th>
            <th style="padding: 0.2em 0.3em;">
            Dynamics
            </th>
            <th style="padding: 0.2em 0.3em;">
            Conditions
            </th>
            <th style="padding: 0.2em 0.3em;">
            Interpretation
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\approx \pi/2\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <svg width="18" height="18" viewBox="0 0 24 24" overflow="visible">
            <line x1="12" y1="20" x2="12" y2="4" stroke="black" stroke-width="2" marker-end="url(#arrow)"/>
            </svg>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <em>Mass-Driven Growth</em>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\log \iota \approx 0\)</span>,
            <span class="math inline">\(\log \bar{m} &gt; 0\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\iota \approx 1\)</span>, <span
            class="math inline">\(\bar{m} &gt; 1\)</span>. Density
            increases from growing mass per signal; incidence stable.
            </td>
            </tr>
            <tr>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\in (0, \pi/2)\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <svg width="18" height="18" viewBox="0 0 24 24" overflow="visible">
            <line x1="4" y1="20" x2="20" y2="4" stroke="black" stroke-width="2" marker-end="url(#arrow)"/>
            </svg>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <em>General Growth/Expansion</em>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\log \iota &gt; 0\)</span>,
            <span class="math inline">\(\log \bar{m} &gt; 0\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\iota &gt; 1\)</span>, <span
            class="math inline">\(\bar{m} &gt; 1\)</span>. Both signal
            incidence rate and mass per signal are increasing. (<span
            class="math inline">\(\theta = \pi/4\)</span> for balanced
            growth).
            </td>
            </tr>
            <tr>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\approx 0\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <svg width="18" height="18" viewBox="0 0 24 24" overflow="visible">
            <line x1="4" y1="12" x2="20" y2="12" stroke="black" stroke-width="2" marker-end="url(#arrow)"/>
            </svg>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <em>Rate-Driven Growth (Mass Stable)</em>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\log \iota &gt; 0\)</span>,
            <span class="math inline">\(\log \bar{m} \approx 0\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\iota &gt; 1\)</span>, <span
            class="math inline">\(\bar{m} \approx 1\)</span>. Density
            increases from growing signal incidence; mass per signal
            stable.
            </td>
            </tr>
            <tr>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\in (-\pi/2, 0)\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <svg width="18" height="18" viewBox="0 0 24 24" overflow="visible">
            <line x1="4" y1="4" x2="20" y2="20" stroke="black" stroke-width="2" marker-end="url(#arrow)"/>
            </svg>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <em>Mass Dilution</em>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\log \iota &gt; 0\)</span>,
            <span class="math inline">\(\log \bar{m} &lt; 0\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\iota &gt; 1\)</span>, <span
            class="math inline">\(\bar{m} &lt; 1\)</span>. Many signals
            entering, but each contributes less mass. (<span
            class="math inline">\(\theta = -\pi/4\)</span> for
            proportional change).
            </td>
            </tr>
            <tr>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\approx -\pi/2\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <svg width="18" height="18" viewBox="0 0 24 24" overflow="visible">
            <line x1="12" y1="4" x2="12" y2="20" stroke="black" stroke-width="2" marker-end="url(#arrow)"/>
            </svg>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <em>Mass-Driven Decline</em>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\log \iota \approx 0\)</span>,
            <span class="math inline">\(\log \bar{m} &lt; 0\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\iota \approx 1\)</span>, <span
            class="math inline">\(\bar{m} &lt; 1\)</span>. Density
            declines due to decreasing mass per signal; incidence
            stable.
            </td>
            </tr>
            <tr>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\in (-\pi, -\pi/2)\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <svg width="18" height="18" viewBox="0 0 24 24" overflow="visible">
            <line x1="20" y1="4" x2="4" y2="20" stroke="black" stroke-width="2" marker-end="url(#arrow)"/>
            </svg>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <em>General Decline/Contraction</em>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\log \iota &lt; 0\)</span>,
            <span class="math inline">\(\log \bar{m} &lt; 0\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\iota &lt; 1\)</span>, <span
            class="math inline">\(\bar{m} &lt; 1\)</span>. Both signal
            incidence rate and mass per signal are decreasing. (<span
            class="math inline">\(\theta = -3\pi/4\)</span> for balanced
            proportional decline).
            </td>
            </tr>
            <tr>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\approx \pm \pi\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <svg width="18" height="18" viewBox="0 0 24 24" overflow="visible">
            <line x1="20" y1="12" x2="4" y2="12" stroke="black" stroke-width="2" marker-end="url(#arrow)"/>
            </svg>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <em>Rate-Driven Decline (Mass Stable)</em>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\log \iota &lt; 0\)</span>,
            <span class="math inline">\(\log \bar{m} \approx 0\)</span>
            </td>
            <td style="padding: 0.2em 0.3em;">
            <span class="math inline">\(\iota &lt; 1\)</span>, <span
            class="math inline">\(\bar{m} \approx 1\)</span>. Density
            declines primarily due to decreasing signal incidence; mass
            per signal stable.
            </td>
            </tr>
            </tbody>
            </table>
            <svg height="0" width="0">
            <defs>
            <marker id="arrow" markerWidth="6" markerHeight="6" refX="0" refY="3" orient="auto" markerUnits="strokeWidth">
            <path d="M0,0 L0,6 L6,3 z" fill="#000"/> </marker> </defs>
            </svg>
            </div>
            <figcaption><p>Figure 31: Interpretation of Î¸ as directional
            flow in log-space between incidence rate and signal mass..
            </p></figcaption>
            </figure>
            <p>Letâs review the flow field in FigureÂ 30 with these
            interpretations in mind. While this example is somewhat
            simplified, many of the key features of a flow field are
            already observable.</p>
            <p>Here are some important observations from the flow field
            in FigureÂ 30:</p>
            <ul>
            <li>Each row of vectors represents the dominant
            characteristic of flow across observation windows of the
            same length, i.e, at the same timescale.</li>
            <li>Vectors with similar length and direction within a row
            indicate convergence toward a common flow pattern at that
            timescale.</li>
            <li>Changes in direction or magnitude of vectors along a row
            indicate non-convergence at that timescale.</li>
            <li>We can see that this system starts to converge at
            relative small intervals.</li>
            <li>The bottom row (shortest intervals) shows the greatest
            variation in vector direction, reflecting more volatile
            behavior at finer time scales.</li>
            <li>In this bottom row, the leftmost and rightmost vectors
            transition from incidence-driven to mass-driven growth and
            then back again. This pattern typically arises when onset
            and reset behaviors vary significantly across signals over
            those intervalsâas is the case here, where all signals had
            presences during that span.</li>
            <li>Vector alignment begins to emerge by the second row
            (intervals of length 1), which is expected in a small
            dataset. More generally, the lowest <em>level</em> at which
            such alignment in magnitude and direction becomes visible is
            an important system property.</li>
            </ul>
            <h3 data-number="9.4" id="attractors"><span
            class="header-section-number">9.4</span> Sample path
            trajectories and attractors</h3>
            <p>In the last section, we showed how we could
            mathematically define the concept of <em>flow</em> in a
            system of presences via a mapping of the right-hand product
            in the presence invariant to a complex number and then
            visualizing the entries in the accumulation matrix as a
            vector field using the polar coordinates of the
            corresponding complex numbers.</p>
            <p>There is much more that can be done with this complex
            plane mapping besides visualization, but these applications
            are beyond the scope of this gentle introduction.</p>
            <p>While the flow field representation focused on the
            evolution of incidence rate and presence mass, an equally
            important set of visualizations directly examines the
            presence density matrix. Recall that this matrix represents
            the presence density for each entry in the accumulation
            matrix. There are many straightforward and direct
            visualizations of the matrix using heatmaps and contour
            plots, which we will describe later elsewhere, but in this
            document, we want to describe a less obvious visualization
            that is very useful for studying<br />
            the dynamics of the system when it operates far from
            equilibrium.</p>
            <p>In dynamical systems theory, a standard object of study
            is the path trajectory of a system through some parameter
            space. For a system of presences, the key input parameters
            that drive the systemâs dynamics are the incidence rate and
            mass contribution per signal and their changes over
            time.</p>
            <p>Since the product of these values determines presence
            density over an interval, a sample path in the accumulation
            matrix is a good candidate to study the evolution of the
            system in this parameter space. FigureÂ 32 shows this matrix
            for our running example.</p>
            <figure id="fig:presence-density-matrix"
            class="subfigures figure">
            <img src="../assets/placeholder.png" id="fig:"
            style="display: none;" alt="a" />
            <table style="border-collapse: collapse; margin: auto; font-family: serif; font-size: 0.95em;">
            <thead>
            <tr>
            <th>
            i\j
            </th>
            <th>
            1
            </th>
            <th>
            2
            </th>
            <th>
            3
            </th>
            <th>
            4
            </th>
            <th>
            5
            </th>
            <th>
            6
            </th>
            <th>
            7
            </th>
            <th>
            8
            </th>
            <th>
            9
            </th>
            <th>
            10
            </th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>
            <b>1</b>
            </td>
            <td style="background-color: #eef;">
            1.40
            </td>
            <td style="background-color: #eef;">
            3.65
            </td>
            <td style="background-color: #eef;">
            5.50
            </td>
            <td style="background-color: #eef;">
            5.38
            </td>
            <td style="background-color: #eef;">
            5.48
            </td>
            <td style="background-color: #eef;">
            5.82
            </td>
            <td style="background-color: #eef;">
            6.07
            </td>
            <td style="background-color: #eef;">
            6.14
            </td>
            <td style="background-color: #eef;">
            6.07
            </td>
            <td style="background-color: #eef;">
            5.63
            </td>
            </tr>
            <tr>
            <td>
            <b>2</b>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            5.90
            </td>
            <td style="background-color: #eef;">
            7.55
            </td>
            <td style="background-color: #eef;">
            6.70
            </td>
            <td style="background-color: #eef;">
            6.50
            </td>
            <td style="background-color: #eef;">
            6.70
            </td>
            <td style="background-color: #eef;">
            6.85
            </td>
            <td style="background-color: #eef;">
            6.81
            </td>
            <td style="background-color: #eef;">
            6.65
            </td>
            <td style="background-color: #eef;">
            6.10
            </td>
            </tr>
            <tr>
            <td>
            <b>3</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            9.20
            </td>
            <td style="background-color: #eef;">
            7.10
            </td>
            <td style="background-color: #eef;">
            6.70
            </td>
            <td style="background-color: #eef;">
            6.90
            </td>
            <td style="background-color: #eef;">
            7.04
            </td>
            <td style="background-color: #eef;">
            6.97
            </td>
            <td style="background-color: #eef;">
            6.76
            </td>
            <td style="background-color: #eef;">
            6.13
            </td>
            </tr>
            <tr>
            <td>
            <b>4</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            5.00
            </td>
            <td style="background-color: #eef;">
            5.45
            </td>
            <td style="background-color: #eef;">
            6.13
            </td>
            <td style="background-color: #eef;">
            6.50
            </td>
            <td style="background-color: #eef;">
            6.52
            </td>
            <td style="background-color: #eef;">
            6.35
            </td>
            <td style="background-color: #eef;">
            5.69
            </td>
            </tr>
            <tr>
            <td>
            <b>5</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            5.90
            </td>
            <td style="background-color: #eef;">
            6.70
            </td>
            <td style="background-color: #eef;">
            7.00
            </td>
            <td style="background-color: #eef;">
            6.90
            </td>
            <td style="background-color: #eef;">
            6.62
            </td>
            <td style="background-color: #eef;">
            5.80
            </td>
            </tr>
            <tr>
            <td>
            <b>6</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            7.50
            </td>
            <td style="background-color: #eef;">
            7.55
            </td>
            <td style="background-color: #eef;">
            7.23
            </td>
            <td style="background-color: #eef;">
            6.80
            </td>
            <td style="background-color: #eef;">
            5.78
            </td>
            </tr>
            <tr>
            <td>
            <b>7</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            7.60
            </td>
            <td style="background-color: #eef;">
            7.10
            </td>
            <td style="background-color: #eef;">
            6.57
            </td>
            <td style="background-color: #eef;">
            5.35
            </td>
            </tr>
            <tr>
            <td>
            <b>8</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            6.60
            </td>
            <td style="background-color: #eef;">
            6.05
            </td>
            <td style="background-color: #eef;">
            4.60
            </td>
            </tr>
            <tr>
            <td>
            <b>9</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            5.50
            </td>
            <td style="background-color: #eef;">
            3.60
            </td>
            </tr>
            <tr>
            <td>
            <b>10</b>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td>
            </td>
            <td style="background-color: #eef;">
            1.70
            </td>
            </tr>
            </tbody>
            </table>
            <figcaption><p>Figure 32: Presence Density Matrix for our
            running example. </p></figcaption>
            </figure>
            <p>This matrix encodes presence density along sample paths
            at different observation time scales and each one can be
            plotted as a trajectory.</p>
            <p>In visualizing these trajectories, we are focusing on the
            <em>values</em> of the presence density that the system
            inhabitsâhow close they are to each other, how often the
            system returns to the same values, etc.</p>
            <p>These are useful in identifying values to which the
            system converges when the behavior is not fully convergent
            or divergent, and help identify attractors of the system as
            well as the movement of the system between them.</p>
            <p>We can construct this visualization as follows:</p>
            <ul>
            <li>A sample path at a given timescale is a non-overlapping
            sequence of half-open time intervals that lies on one of the
            diagonals of the presence density matrix.</li>
            <li>At some time scales the non-overlapping requirement
            means we can have many sample paths. For example, along the
            second diagonal we have two sample paths: <span
            class="math inline">\([1,3), [3,5)\)</span> and <span
            class="math inline">\([2,4), [4,6)\)</span>.</li>
            <li>Each of these paths represents a trajectory of the
            evolution of the system when continuously observed over
            windows of that length. Multiple paths along the same
            diagonal are shown in different colors.</li>
            <li>We visualize these paths by laying out their values on a
            line and showing curved arrows between successive
            values.</li>
            <li>The size of a point representing a value is proportional
            to the number of times that value gets visited by some
            trajectory.</li>
            <li>The resulting set of trajectories is laid out along the
            vertical axis, with paths along the same diagonal on the
            same horizontal line.</li>
            </ul>
            <p>The resulting diagram is shown in FigureÂ 33.</p>
            <figure id="fig:attractors">
            <img src="../assets/pandoc/trajectories.png"
            alt="Figure 33: Sample path trajectories" />
            <figcaption aria-hidden="true">Figure 33: Sample path
            trajectories</figcaption>
            </figure>
            <p>While this example is again fairly simple, here are some
            common characteristics we can see:</p>
            <ul>
            <li><p><em>Concentration of Trajectories at Lower
            Scales</em>: The lower diagonals (small interval lengths)
            exhibit rich trajectory structures, indicating high
            variability and recurrence across short timescales.</p></li>
            <li><p><em>Convergence with Increasing Interval
            Length</em>:</p>
            <ul>
            <li>As interval length increases, fewer trajectories appear,
            and values cluster more tightly.</li>
            <li>Some points are revisited by multiple trajectories,
            suggesting possible attractor behavior.</li>
            </ul></li>
            <li><p><em>Dominant Density Bands</em>:</p>
            <ul>
            <li>Several horizontal bands (e.g.Â around Î´ â 6â7) persist
            across multiple levels, possibly indicating stable
            attractors.</li>
            <li>Narrowing spread at higher diagonals supports
            convergence interpretation.</li>
            </ul></li>
            <li><p><em>Visual Asymmetry in Curves</em>:</p>
            <ul>
            <li>Arc shapes suggest non-symmetric evolution: some paths
            curve back or repeat similar values.</li>
            </ul></li>
            <li><p><em>Unlinked Points at Higher Levels</em>:</p>
            <ul>
            <li>Some single-point trajectories appear on upper diagonals
            with no visible arc.</li>
            <li>This is expected as fewer non-overlapping intervals
            exist at those scales.</li>
            </ul></li>
            </ul>
            <p>Overall, the diagram is complementary to the information
            shown in the flow field: how presence densities evolve and
            concentrate across temporal scales, revealing both
            volatility at finer scales and convergence toward attractor
            bands at coarser scales.</p>
            <h3 data-number="9.5" id="feedback-loops-and-steering"><span
            class="header-section-number">9.5</span> Feedback loops and
            steering</h3>
            <p>We should also note that while flow fields and trajectory
            maps are useful to a get a quick high level signature of
            flow dynamics in a system of presences as above, they shine
            when we use them reason closely about the local and global
            changes in the field, and trace these back to the specific
            drivers of the change - incidence rate, mass contributions,
            or both.</p>
            <p>The flow field representation, in particular, lets us
            reliably attribute a <em>proximate cause</em> to a local
            change in presence density, and this is the key to being
            able to build feedback loops into the system to steer it in
            a desired direction.</p>
            <p>However, to act on this insight effectively, we need to
            drop back down into the domain and what presence density,
            incidence rate, and mass contribution mean for a
            <em>specific</em> system of presences. It also requires
            careful choices about timescale at which we sample presence
            density, the timescales at which we sense changes, and
            timescales at which we intervene in the system to change
            direction.</p>
            <p>What is important to note here, is that underlying
            machinery to <em>detect and respond to change</em> is signal
            and timescale agnostic and this is the key contribution of
            the presence calculus in this regard.</p>
            <p>We will have much more say about using the tools of the
            presence calculus to create such feedback loops and steer
            systems in future posts.</p>
            <h2 data-number="10" id="taking-stock"><span
            class="header-section-number">10</span> Taking stock</h2>
            <p>The last few chapters have introduced many concepts that
            build on the primitive idea of presence and systems of
            presence. Letâs pause and take stock of these ideas to build
            some intuition for what the presence calculus <em>is</em>
            and what it tells us.</p>
            <h3 data-number="10.1" id="why-should-i-care"><span
            class="header-section-number">10.1</span> Why should I
            care?</h3>
            <p>First and foremost, the presence calculus is not a
            modeling framework, a measurement system, or a methodology.
            It is a mathematical and analytical <em>substrate</em>âa
            foundation on which such frameworks or methodologies can be
            constructed.</p>
            <p>It is not a replacement for existing mathematical tools.
            Rather, it is a tool that can operate <em>alongside</em>
            techniques such as statistical and probabilistic analyses.
            Its particular strength lies in analyzing a class of
            problems that are not easily addressed using those
            techniques.</p>
            <p>The presence calculus is well suited to reasoning about
            system properties that satisfy the following mathematical
            criteria, stated here in plain English <a href="#fn21"
            class="footnote-ref" id="fnref21"
            role="doc-noteref"><sup>21</sup></a>:</p>
            <ul>
            <li>The measured value is a <em>non-negative</em> real
            number that varies continuously over timeâi.e., we are
            measuring the <em>presence</em> of a property in the
            system.</li>
            <li>The âsignalâ in the property is tied to the
            <em>accumulation</em> of its valueâ both the magnitude and
            the duration for which it is held are meaningful.</li>
            <li>The value at any given time can be expressed as the
            <em>sum</em> of <em>non-negative</em> values from a
            countable set of time-varying functions (signals) over a
            fixed domain.</li>
            <li>Each contributing function is measurable over finite
            intervals, meaning its Lebesgue integral exists and is
            finite over any bounded interval.</li>
            </ul>
            <p>For practical applications, we also require:</p>
            <ul>
            <li>Each signalâs contribution can be <em>directly</em>
            instrumented and measured over the domain.</li>
            </ul>
            <p>While this may sound restrictive, a surprisingly large
            class of operational and business-critical
            propertiesâespecially those related to delays, costs, risks,
            and rewardsâfall squarely within this domain. This makes the
            presence calculus a powerful tool for reasoning about such
            properties in a robust and principled way.</p>
            <p>Because these properties often underpin sound
            decision-making in business contexts, the presence calculus
            has the potential to significantly improve the quality of
            those decisions.</p>
            <h3 data-number="10.2"
            id="where-can-i-apply-this-and-how"><span
            class="header-section-number">10.2</span> Where can I apply
            this and how?</h3>
            <p>Intuitively, when a system property has a âtime-value,â
            the presence calculus offers tools to frame decisions in
            terms of managing that propertyâs accumulation over time. We
            categorize signals into those we want to accumulate (e.g.,
            revenue, profits, satisfied customers) and those we want to
            constrain (e.g., costs, risks, debts, delays).</p>
            <p>For an example of a signal where presence calculus would
            <em>not</em> be useful, consider the price of a stock over
            time. Although this is a time-varying function, there is
            little use in analyzing it the using presence calculus.</p>
            <p>The signal here lies in the <em>instantaneous value</em>
            of the function at a given point in time, and in the
            <em>difference</em> between values at two points in time.
            The <em>accumulated</em> stock price over an interval, does
            not convey much meaningful information for decision
            making.</p>
            <blockquote>
            <p>By contrast, if we consider the <em>time-value of a
            deployed pool of capital</em>, we can use presence calculus
            quite effectively to analyze the <em>efficiency</em> of a
            capital allocation strategy <a href="#fn22"
            class="footnote-ref" id="fnref22"
            role="doc-noteref"><sup>22</sup></a>.</p>
            </blockquote>
            <p>When applied to a suitable class of signals, the calculus
            provides better tools to accurately measure and reason about
            how the property accumulates over time and across
            time-scales within a defined system of presences.</p>
            <figure id="fig:pcalc-machinery">
            <img src="../assets/pandoc/pcalc_machinery.png"
            alt="Figure 34: The presence calculus machinery" />
            <figcaption aria-hidden="true">Figure 34: The presence
            calculus machinery</figcaption>
            </figure>
            <p>The machinery of the presence calculus is shown in
            FigureÂ 34</p>
            <ul>
            <li>The <em>presence invariant</em> states that for any
            finite interval, the presence density in the system equals
            the product of the incidence rate and the mass contribution
            per signal.</li>
            <li>Since presence density measures the rate of accumulation
            over time, these two parametersâincidence rate and mass
            contributionâfully determine how presence evolves in the
            system. They become your two control knobs for presence
            density.</li>
            <li>The <em>presence matrix</em> captures a discrete set of
            presence samples using a fixed sampling granularity. This
            defines the smallest time interval over which presence mass
            is recorded. This normalization allows presence accumulation
            to be compared on a consistent time scale.</li>
            <li>The <em>presence accumulation matrix</em> then
            aggregates these samples across time and time-scales.
            Because presence is finitely additive, the matrix supports
            arbitrary aggregation without losing the semantics of
            accumulated mass.</li>
            <li>This lets us reason about sample paths at multiple
            scales, detect convergence behaviors, and identify
            attractors within the system.</li>
            <li>Mapping the invariant to the complex plane gives us a
            visual representation of presence flow across time-scales.
            It becomes a steering vector that shows <em>where</em> you
            want to move the system using the control knobs of incidence
            rate and signal mass. This is a <em>deterministic</em>
            steering mechanism <em>even when the underlying system is
            non-linear, stochastic or chaotic</em>!</li>
            </ul>
            <p>All of this are significantly more powerful tools than
            analyzing isolated point samples using standard statistical
            or probabilistic techniques. The key advantage is that it
            allows us to <em>manage</em> a system of presences even when
            the underlying signals lack a stable statistical or
            probability distribution and the system is far from
            equilibrium <a href="#fn23" class="footnote-ref"
            id="fnref23" role="doc-noteref"><sup>23</sup></a>.</p>
            <p>So for this very broad category of signals, the presence
            calculus is a much more powerful mathematical substrate on
            which to build decision support systems.</p>
            <h3 data-number="10.3" id="towards-systems-of-systems"><span
            class="header-section-number">10.3</span> Towards Systems of
            Systems</h3>
            <p>When managing a single operational propertyâlike cost,
            risk, delay, or a market-facing reward such as revenueâa
            <em>single presence system</em> is often sufficient to model
            the relevant variables. Many useful operational questions
            can be answered with such models, though these are typically
            local optimizations.</p>
            <p>Real-world decision-making environments, however, involve
            <em>many such systems</em>, each representing distinct
            tradeoffs between aspects of work, interaction, or
            constraint.</p>
            <p>A general theory for how these systems of presence
            interact would be valuable but quite challenging to develop.
            Much work remains in understanding how to combine individual
            presence systems into larger, interacting <em>systems of
            systems</em>, and it seems naive to hope this can be done
            for arbitrarily complex systems.</p>
            <p>The mathematics of the presence calculus are well-suited
            for modeling local interactions. When tackling measurement
            in a larger complex system, I believe their power lies in
            modeling the interactions <em>within and between smaller
            subsystems and assemblages</em>. This remains an active and
            largely unexplored application area for the calculus.</p>
            <p>Still, as weâve seen in concrete applications, modeling
            within specific domains already offers substantial value.
            <em>Domain assumptions</em> often allow us to prove
            properties that might not hold in the general case.</p>
            <p>For instance, in the domain where the presence calculus
            was first developedâoperations management in software
            product developmentâsystems of presence are a powerful
            unifying concept.</p>
            <p>Today, operational improvement in software largely
            remains in a pre-scientific phase. It relies on a fragmented
            mix of metrics, developed using gut instinct and folk
            theories, and awkwardly transplanted models from unrelated
            domains, many designed under assumptions that provably donât
            hold in the software context.</p>
            <p>The result is a patchwork of measurement systemsâDORA
            metrics, Flow Metrics, project metrics, customer experience
            measuresâeach operating independently and largely without an
            underlying theory that is provably correct in the software
            domain. Yet each can be reframed as a <em>system of
            presences</em> and analyzed uniformly with the tools
            developed here.</p>
            <p>We will finally have measurement systems that start with
            software product development concepts and operating
            principles, <em>and yet have mathematically provable
            relationships between key measurements</em>. This is what
            the presence calculus brings to the table. Our first set of
            application articles will explore what this looks like in
            practice.</p>
            <p>The next step is to construct <em>structural causal
            models</em> that link these systems to understand their
            local interactions <em>in a specific company context</em>.
            The presence calculus gives us tools to steer presence
            density in individual systems of presence and explain
            observed behavior in a linked system. This means we can make
            a local change to one system and explain the changes in a
            linked system. The interactions are local, but now they span
            directly linked systems.</p>
            <p>This gives us the ability to build such causal models
            iteratively through experimentation on the system. A value
            stream is a promising example of such a system of systems
            where an approach like this appears tractable.</p>
            <blockquote>
            <p>This idea is very similar to statistical causal modeling,
            but with a crucial difference: weâre not inferring causal
            relationships through statistical correlations between proxy
            variables on large heterogeneous datasets. Instead, weâre
            <em>proving causal relationships between local interactions
            on directly observed presence in a specific complex
            system</em>.</p>
            </blockquote>
            <p>In my view, the most productive path forward lies not in
            seeking a fully general theory of interacting presence
            systems, but in building <em>domain-specific measurement
            frameworks</em> grounded in presence systems as primitives
            that can be effectively tuned through experimentation for a
            <em>specific</em> complex system, like a single value stream
            in a company. This approach is both more tractable and more
            likely to yield practical insights.</p>
            <p>Thatâs where weâll beginâfocusing on specific, bounded,
            real-world applications. Perhaps a larger theory will emerge
            from this, perhaps not, but in either case, we expect to see
            tremendous value created from taking a careful, measured
            mathematically rigorous approach to understanding the
            capabilities and limits of this formalism.</p>
            <h2 data-number="11" id="a-personal-note"><span
            class="header-section-number">11</span> A personal note</h2>
            <p>This work represents the culmination of nearly ten years
            of research, product development, and field work in
            operations management in software product development in <a
            href="https://exathink.com">The Polaris Advisor
            Program</a>.</p>
            <p>The foundations were laid between 2017 and 2022 while I
            was developing <a
            href="https://www.exathink.com/why-us">Polaris</a>, a
            proprietary measurement platform for software product
            development that powers our program. Butâas is the nature of
            evolving software platformsâideas, theories, and practice
            all blended together over the years into a somewhat
            incoherent jumble, making it hard to distinguish what was
            fundamental from what was anecdotal practice.</p>
            <p>I was also deeply frustrated with the inadequacy of
            existing measurement techniques to realistically model the
            systems we <em>actually</em> work in. Personally, as someone
            who has been a software developer for nearly 30 years,
            building measurements systems in software over the last
            decade has highlighted, in vivid detail, the shortcomings in
            our ability to measure <em>anything</em> meaningful and
            actionable about software development work.</p>
            <p>The presence calculus is a foundational step in
            addressing this problem, but it is only a starting point. It
            makes no grand claims beyond what is stated mathematically
            here. Much work still remains to explore its applications,
            and its practical limits will become clear as we test its
            applicability in various contexts.</p>
            <blockquote>
            <p>That said, in my view, the calculus represents a
            fundamental departure in how we approach measurement in
            software, distinguishing it both from the purely empirical
            and statistical techniques that are in the mainstream today
            and also from ideas imported from other domains like
            manufacturingâanother significant branch.</p>
            </blockquote>
            <p>It allows us to model and measure software development as
            it isâ bespoke, messy, irregular, highly variable, deeply
            time- and history-dependent work involving humans, machines,
            and codified knowledge, forming complex, interconnected
            systemsâwhere being able to move beyond proxies and observe
            what <em>is</em> in rich detail is a prerequisite to
            reasoning quantitatively about the system.</p>
            <p>It formally identifies the mathematical foundations
            behind how we can do thisâhow to better <em>measure</em>
            properties of such real-world systems without compromise,
            adapt the ideas that are fundamental to this messy world,
            and build measurement models that reflect the reality of
            software development.</p>
            <p>In developing the presence calculus, I drew upon and
            re-contextualized many existing ideas in the field. For
            example, Littleâs Law plays a foundational role in the
            presence calculus. But this is because, mathematically,
            Littleâs Law encodes some deep and general structural
            properties of <em>arbitrary time-varying signals</em> in
            <em>any</em> domain.</p>
            <p>The version we use commonly in the software industry is a
            rather trivial <em>special case</em> of the underlying
            mathematical concepts. This version is based on nearly
            50-year-old ideas developed in the context of studying
            queueing systems and their application to manufacturing. It
            does not reflect the modern mathematical developments in the
            theories underpinning the law. In fact, those developments
            are materially important to apply it correctly in the
            software development context.</p>
            <p>The presence calculus starts from this up-to-date view
            but goes beyond this to position this law within a more
            general, measure-theoretic mathematical framework.</p>
            <p>When we start from the presence calculus framing, we can
            derive a <em>provably correct</em> version of Littleâs Law
            from first principles <em>for any measurement context</em>
            involving time-varying signals. It is no longer simply a
            formula copied from Lean manufacturing.</p>
            <blockquote>
            <p>It may be surprising to many that such a derivation is
            even possibleâmuch less in a mathematically provable way. It
            certainly was, to me!</p>
            </blockquote>
            <p>This has fundamental implications on our ability to
            reason about these measurementsâwell beyond what is possible
            with statistical or probabilistic techniques alone.</p>
            <p>Those statistical and probabilistic techniques, too,
            require much more careful treatment when modeling the
            non-stationary, time-varying behaviors that are the default
            in real-world software systems. However, that care is very
            often missing in the mainstream applications of these
            techniques in industryâfor example, in the field of
            developer experience and in mainstream developer
            productivity measurement products, where statistical
            measures are applied to time-varying signals with nary a
            concern about such technicalities.</p>
            <p>The presence calculus offers a much more careful and
            rigorous foundation for bridging this gapâquite different
            from standard statistical approaches to dealing with
            time-varying data.</p>
            <p>In this way, I believe the presence calculus has deep and
            fundamental applications for the software domain and
            beyond.</p>
            <p>Even though the concepts require a bit of effort to
            understand and apply, the good news is that there are
            relatively straightforward points of departure between how
            we would model problems using the calculus and how we would
            typically model them using conventional techniques.</p>
            <p>By comparing and contrasting these approaches, we can
            better understand where it brings new benefits.</p>
            <h3 data-number="11.1" id="the-road-ahead"><span
            class="header-section-number">11.1</span> The road
            ahead</h3>
            <p>The presence calculus formalizes and generalizes the
            theory behind many of the ideas developed and remixed from
            existing practices and techniques in the industry and that
            we have used in The Polaris Advisor Program over the
            years.</p>
            <p>I am now embarking on a new phase: rebuilding a mix of
            open-source and proprietary tooling grounded in the
            principles laid out here.</p>
            <p>If the concepts in this document and <a
            href="https://github.com/krishnaku/pypcalc">the project</a>
            are of interest, I welcome collaborators who can help
            pressure test and apply these ideasâand explore and
            understand what their limits are.</p>
            <p>This document, along with others on this site, are
            intended to provide a broad and deep foundation to support
            anyone who finds this prospect intriguing or exciting.</p>
            <p>My objective here is to develop a robust substrate for
            many common modeling and measurement problems in software,
            so that more people can extend and apply these ideas in
            real-world environments with greater precision, and with
            confidence in the mathematical validity of their measurement
            systems.</p>
            <p>I welcome constructive feedback and thoughtful
            skepticismâespecially from those who can help surface areas
            where the approach needs refinement or may even be the wrong
            fit. That kind of scrutiny is essential if we want these
            ideas to be broadly useful.</p>
            <hr />
            <h2 class="unnumbered" id="references">References</h2>
            <div id="refs" class="references csl-bib-body"
            data-entry-spacing="0" role="list">
            <div id="ref-forsgren2018" class="csl-entry"
            role="listitem">
            <div class="csl-left-margin">[1] </div><div
            class="csl-right-inline">N. Forsgren, J. Humble, and G. Kim,
            <em>Accelerate: The science of lean software and DevOps:
            Building and scaling high performing technology
            organizations</em>. Portland, OR: IT Revolution Press,
            2018.</div>
            </div>
            <div id="ref-muller2018" class="csl-entry" role="listitem">
            <div class="csl-left-margin">[2] </div><div
            class="csl-right-inline">J. Z. Muller, <em>The tyranny of
            metrics</em>. Princeton, NJ: Princeton University Press,
            2018.</div>
            </div>
            <div id="ref-little2011" class="csl-entry" role="listitem">
            <div class="csl-left-margin">[3] </div><div
            class="csl-right-inline">J. Little, <span>âLittleâs law as
            viewed on its 50th anniversary,â</span> <em>Operations
            Research</em>, vol. 59, no. 3, pp. 536â549, 2011.</div>
            </div>
            <div id="ref-brumelle71" class="csl-entry" role="listitem">
            <div class="csl-left-margin">[4] </div><div
            class="csl-right-inline">S. Bruemelle, <span>âOn the
            relation between customer and time averages in
            queues.â</span> <em>Journal of Applied Probability</em>,
            vol. 8, no. 1â4, pp. 508â520, 1971.</div>
            </div>
            <div id="ref-heyman80" class="csl-entry" role="listitem">
            <div class="csl-left-margin">[5] </div><div
            class="csl-right-inline">D. Heyman and S. Stidham,
            <span>âThe relation between customer and time averages in
            queues,â</span> <em>Operations Research</em>, vol. 28, no.
            4, pp. 983â994, 1980.</div>
            </div>
            <div id="ref-stidham72" class="csl-entry" role="listitem">
            <div class="csl-left-margin">[6] </div><div
            class="csl-right-inline">S. Stidham, <span>âA last word on
            <span class="math inline">\(L=\lambda W\)</span>,â</span>
            <em>Operations Research</em>, vol. 22, no. 1â4, pp. 417â421,
            1972.</div>
            </div>
            <div id="ref-eltaha1999" class="csl-entry" role="listitem">
            <div class="csl-left-margin">[7] </div><div
            class="csl-right-inline">M. El-Taha and S. Jr. Stidham,
            <em>Sample-path analysis of queueing systems</em>, vol. 11.
            in International series in operations research &amp;
            management science, vol. 11. Boston, MA: Springer Science
            &amp; Business Media, 1999, p. 295.</div>
            </div>
            <div id="ref-miyazawa94" class="csl-entry" role="listitem">
            <div class="csl-left-margin">[8] </div><div
            class="csl-right-inline">M. Miyazawa, <span>âRate
            conservation laws: A survey,â</span> <em>Queueing
            Systems</em>, vol. 15, no. 1â4, pp. 1â58, 1994.</div>
            </div>
            <div id="ref-sigman91" class="csl-entry" role="listitem">
            <div class="csl-left-margin">[9] </div><div
            class="csl-right-inline">K. Sigman, <span>âA note on a
            sample path rate conservation law and itâs relationship to
            <span class="math inline">\(H=\lambda G\)</span>,â</span>
            <em>Advanced App. Probability</em>, vol. 23, no. 1â4, pp.
            662â665, 1991.</div>
            </div>
            </div>
            <section id="footnotes"
            class="footnotes footnotes-end-of-document"
            role="doc-endnotes">
            <hr />
            <ol>
            <li id="fn1"><p>As of writing, the toolkit is still in an
            alpha state, and is under active development and evolution.
            It is not ready for production use, but can be used as
            reference to understand how the concepts we define are
            implemented in code.<a href="#fnref1" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn2"><p>If integration signs in a âgentleâ
            introduction feels like a bait-and-switch, rest assured, for
            the purposes of this document you just need to think of them
            as a way to add up presence masses, in a way that the ideas
            we use for binary presences will generalize when we apply
            them to arbitrary functions.<a href="#fnref2"
            class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn3"><p>The way weâve defined signals and mass is
            directly<br />
            analogous to how mass is defined for matter occupying space
            in physics.</p>
            <p>A binary signal can be thought of as defining a
            one-dimensional interval over time. For a fixed element and
            boundary, this gives us an area under the curve in two
            dimensions: time vs.Â amount of signal.</p>
            <p>If we treat elements and boundaries as additional
            independent dimensions, then the signal defines a
            <em>volume</em> in three dimensions, with time as one
            axis.</p>
            <p>This interpretationâpresence as a physical manifestation
            of density over timeâis a powerful way to reason intuitively
            and computationally about duration, overlap, and
            accumulation in time.</p>
            <p>And when we allow multiple signals to interact over the
            same time periods, we begin to model complex,
            higher-dimensional effects of presenceâexactly the kind of
            generality weâll need when we move beyond simple binary
            presences.<a href="#fnref3" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn4"><p>The details here depend on ideas we will
            develop later in the document. The key point is that the
            presence calculus gives us tools to accurately represent the
            time-varying behavior of the actual system property we care
            about, rather than relying on statistical proxies. This also
            gives us stronger protection against Goodhartâs Law <span
            class="citation" data-cites="muller2018"><a
            href="#ref-muller2018"
            role="doc-biblioref">[2]</a></span>.<a href="#fnref4"
            class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn5"><p>The use of a rough curve here is an example
            of how presences can encode continuous inputs more
            effectively than discrete techniques, thanks to their
            explicit model of time. Forcing a developer to rank their
            productivity on a Likert scale often loses valuable
            nuanceâwhereas a fine-grained presence captures temporal
            variation with ease, making it available for downstream
            analysis.<a href="#fnref5" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn6"><p>It is equally valid to define <span
            class="math inline">\(N\)</span> as the number of distinct
            <em>presences</em> in the observation window. For example
            for the signal <span class="math inline">\(P2\)</span> in
            FigureÂ 5, this corresponds to asking if <span
            class="math inline">\(N=5\)</span> (if we count the disjoint
            presences individually) or <span
            class="math inline">\(N=3\)</span> (if we count the
            signals). These give different values for <span
            class="math inline">\(\bar{m}\)</span> and <span
            class="math inline">\(\iota\)</span> but their product
            <em>still equals</em> <span
            class="math inline">\(\delta\)</span>, as long as a single
            consistent definition of N is used.</p>
            <p>As we show in <a
            href="./generalized_presence_invariant.html">The Generalized
            Presence Invariant</a> <span
            class="math inline">\(N\)</span> can be a dicrete counting
            measure on the signal domain.</p>
            <p>This is ultimately a modeling decision that depends on
            what you are trying to measure. By default we will assume
            that <span class="math inline">\(N\)</span> is measured at
            the signal granularity.<a href="#fnref6"
            class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn7"><p><span class="math inline">\(\delta\)</span>
            may also be considered the rate at which presence density
            <em>accumulates</em> over the interval. This latter view
            will be useful when interpreting the dynamics of the
            system.<a href="#fnref7" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn8"><p>As discussed in <a
            href="./generalized_presence_invariant.html"><em>The
            Presence Invariant â a Measure-Theoretic
            Generalization</em></a>, the presence invariant admits a
            formal generalization: whenever two orthogonal measure
            spaces are involved, and accumulated presence mass resides
            in their product space, a similar invariant emerges. This
            reveals that the presence invariant is not a superficial
            constraint, but a deep structural property rooted in the
            measurability of systems of presence.<a href="#fnref8"
            class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn9"><p>We note that the residence time represents
            only the portion of the duration of the task in some
            arbitrary observation window. This is a different quantity
            from the overall duration of the task from start to finish
            (or from signal onset to reset in our terminology). This is
            the more familiar metric typically called the cycle time.<a
            href="#fnref9" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn10"><p>We introduce Littleâs Law here as a special
            case of the presence invariant. This is a deliberate
            decision: we aim to contextualize classical treatments of
            Littleâs Law through the lens of what we consider the more
            foundational, measure-theoretic constructs of the presence
            calculus.</p>
            <p>In fact, the ideas in presence calculus may be considered
            <em>generalizations</em> of the concepts developed to prove
            Littleâs Law. For an excellent overview of those techniques,
            see <span class="citation" data-cites="little2011"><a
            href="#ref-little2011" role="doc-biblioref">[3]</a></span>.
            Many of the concepts surveyed there will reappear in this
            documentâtransformed, but recognizableâunder the
            measure-theoretic definitions of the presence calculus.</p>
            <p>If you are not familiar with Littleâs Law and wish to
            understand it better, Dr.Â Littleâs survey paper remains the
            best starting point.<a href="#fnref10" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn11"><p>The sampling granularity is typically
            coarser than time resolution of the underlying signals. For
            example, the signals themselves may be timestamped at
            millisecond granularity, while the presence matrix may be
            constructed by sampling at hourly, daily, or weekly
            intervals. Many different presence matrices can be
            constructed from the same underlying set of signals.
            Depending on the observation window and sampling
            granularity, we may arrive at very different matrices. A
            presence matrix is therefore an observer-relative analytical
            construct, derived from a system of presencesânot a direct
            representation of the underlying signals.<a href="#fnref11"
            class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn12"><p>It is equally valid to define <span
            class="math inline">\(N\)</span> as the number of distinct
            <em>presences</em> in the observation window. For example
            for the signal <span class="math inline">\(P2\)</span> in
            FigureÂ 5, this corresponds to asking if <span
            class="math inline">\(N=5\)</span> (if we count the disjoint
            presences individually) or <span
            class="math inline">\(N=3\)</span> (if we count the
            signals). These give different values for <span
            class="math inline">\(\bar{m}\)</span> and <span
            class="math inline">\(\iota\)</span> but their product
            <em>still equals</em> <span
            class="math inline">\(\delta\)</span>, as long as a single
            consistent definition of N is used.</p>
            <p>As we show in <a
            href="./generalized_presence_invariant.html">The Generalized
            Presence Invariant</a> <span
            class="math inline">\(N\)</span> can be a dicrete counting
            measure on the signal domain.</p>
            <p>This is ultimately a modeling decision that depends on
            what you are trying to measure. By default we will assume
            that <span class="math inline">\(N\)</span> is measured at
            the signal granularity.<a href="#fnref12"
            class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn13"><p>Assuming <span
            class="math inline">\(N\)</span> and <span
            class="math inline">\(T\)</span> can be mapped to row and
            column counts of the presence matrix is a simplification. It
            relies on the assumption that each sampling interval is
            equal sized, and also that <span
            class="math inline">\(N\)</span> represents signal counts.
            It is entirely possible to apply the machinery and
            techniques here to irregular observation windows, as well as
            for other measures for N on the signal dimension. These
            generalizations merely lead more complicated mappings to
            compute <span class="math inline">\(N\)</span> and <span
            class="math inline">\(T\)</span> from the presence matrix
            structure, and need to be specified as part of the modeling
            process. To avoid complicating matters, we will use this
            simple mapping, which covers a large number of practical use
            cases, as the default.<a href="#fnref13"
            class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn14"><p>Recall that we required signals to be
            measurable functions. This implies presence masses are
            measures over time intervals. Given intervals <span
            class="math inline">\(A\)</span> and <span
            class="math inline">\(B\)</span>, a measure <span
            class="math inline">\(\mu\)</span> satisfies the property of
            finite additivity <span class="math display">\[\mu(A \cup B)
            = \mu(A) + \mu(B) - \mu(A \cap B),\]</span> <span
            class="math inline">\(A[i,j]\)</span> is a measure over the
            union of two time intervals, so the<br />
            recurrence follows from this property of finite additivity
            where <span class="math display">\[A = [i, j{-}1] \text{ and
            } B = [i{+}1, j].\]</span> When <span
            class="math inline">\(A\)</span> and <span
            class="math inline">\(B\)</span> intersect, the subtraction
            removes the presence mass of overlap <span
            class="math inline">\([i{+}1, j{-}1]\)</span> from the sum
            to avoid double-counting it.<a href="#fnref14"
            class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn15"><p>Weâll note at this point, that the
            determinism is <em>retrospective</em>. Since it is based on
            <em>observed</em> presence, this recurrence in no way
            implies we can predict how the presences will evolve in the
            future. We will have more to say about this in the next
            section.<a href="#fnref15" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn16"><p>Though we should hasten to add that this is
            just a loose analogyâwe do not imply any conceptual
            equivalence.<a href="#fnref16" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn17"><p>This concept is equivalent to the concept
            of a sample path developed by Stidham in <span
            class="citation" data-cites="stidham72"><a
            href="#ref-stidham72" role="doc-biblioref">[6]</a></span> to
            provide the first deterministic proof of Littleâs Law. This
            concept has been extensively studied in the context queueing
            systems with <span class="citation"
            data-cites="eltaha1999"><a href="#ref-eltaha1999"
            role="doc-biblioref">[7]</a></span> being the comprehensive
            reference for applications of sample path techniques.<a
            href="#fnref17" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn18"><p>Specifically, the Riemann sum approximation
            of the integral <span class="math display">\[
            A(1, j) = \sum_{k=1}^j A(k,k)
            \approx \int_0^j \left( \sum_{(e,b)} P_{(e,b)}(t) \right) dt
            \]</span> where each <span
            class="math inline">\(P_{(e,b)}(t)\)</span> represents the
            presence density function of an underlying element-boundary
            signal.<a href="#fnref18" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn19"><p>While itâs tempting to assume that the
            limit of a product is simply the product of the limits, this
            doesnât automatically hold here. The long-run value of
            presence density, <span
            class="math inline">\(\Delta\)</span>, is defined as a
            time-based density, while the signal mass contribution,
            <span class="math inline">\(\bar{M}\)</span>, is defined
            over the number of signals. Since these limits are taken
            over different denominators, additional technical conditions
            are required to ensure that their product equals the limit
            of the product.<a href="#fnref19" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn20"><p>It is worth emphasizing the word
            <em>observed</em> in this statement. Even though the
            evolution is deterministic, this does not mean the future
            behavior of the system is predictable based on past
            behavior. That depends entirely on the nature of the signals
            involved, which may or may not be predictable. What we can
            say is that, given a sufficiently complete history of the
            system, we can deterministically reconstruct the current
            state of presence density from any starting point. This
            explanatory power is useful in its own right, as we will
            soon see.<a href="#fnref20" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn21"><p>This is not to imply that all these
            criteria are strictly required to apply the presence
            calculus. There are lots of theoretical reasons to believe
            that the ideas here generalize well beyond this, but
            starting with these criteria gives us a well defined set of
            problems for which we can <em>provably guarantee</em> that
            the machinery here will provide useful insights.
            Understanding what happens as we relax one or more of these
            requirements is an ongoing research activity - both in
            practical application contexts and in developing the
            mathematical theory further.<a href="#fnref21"
            class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn22"><p>There is even a version of <em>Littleâs
            Law</em> that can be derived in this setting! Weâll discuss
            these and other applications in upcoming posts.<a
            href="#fnref22" class="footnote-back"
            role="doc-backlink">â©ï¸</a></p></li>
            <li id="fn23"><p>This is a key differentiation between the
            presence calculus and standard statistical and probabilistic
            techniques. The latter struggle with non-stationary
            systemsâi.e., systems where the underlying distributions of
            values continuously change over time.<a href="#fnref23"
            class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
            </ol>
            </section>
          </div>
  </div>
</body>
</html>
