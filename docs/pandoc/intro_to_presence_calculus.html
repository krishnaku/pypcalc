<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>intro_to_presence_calculus</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h1 id="the-presence-calculus-a-gentle-introduction">The Presence
Calculus, <br> A Gentle Introduction</h1>
<p><strong>Dr. Krishna Kumar</strong><br />
<em>The Polaris Advisor Program</em></p>
<h2 id="what-is-the-presence-calculus">1. What is The Presence
Calculus?</h2>
<p>The Presence Calculus is a new approach for reasoning quantitatively
about the<br />
relationships between signals in a domain over time.</p>
<p>We aim to better support rigorous modeling and principled<br />
decision-making with operational data in complex, business-critical
domains.</p>
<p>A key objective was to ensure that the use of data in such decisions
rests on a mathematically precise, logically coherent, and epistemically
grounded<br />
foundation.</p>
<p>The presence calculus emerged from a search for better quantitative
tools to<br />
reason about software product development and engineering, where
current<br />
approaches leave much to be desired in all three aspects.</p>
<p>Minimally, the foundational constructs of the calculus bring
mathematical<br />
precision to widely used—but poorly defined—concepts such as flow,
stability,<br />
equilibrium, and coherence in a domain. More importantly, it allows us
to<br />
relate them to business-oriented concerns like delay, cost, revenue,
and<br />
user experience.</p>
<p>As you’ll see, however, the ideas behind the calculus are far more
general,<br />
with potential applications well beyond the software product
development<br />
context it emerged from.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pcalc/presence_calculus.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 1: Key constructs—presences, element trajectories, presence matrix,  
and basis topology</code></pre>
</div>
</div>
<h3 id="the-pitch">The pitch</h3>
<p>We introduce the simple but powerful concept of a
<em>presence</em>.</p>
<p>This lets us reason about time, history and evolution of complex
systems using techniques from measure theory, topology and complex
analysis.</p>
<p>Classical statistics and probability theory often struggle here.
<em>History</em>—the sequence and structure of changes in the domain
over time— is usually fenced off under assumptions like ergodicity,
stationarity, and independence.</p>
<p>Our thesis is that to move beyond simple descriptive statistics,
statistical inference and probabilistic models, and start reasoning
about global and long run behavior of complex systems, we need models
that treat time and history as first-class concepts we can reason and
compute with.</p>
<p>This, in turn, lets us apply techniques from disciplines such as
stochastic process dynamics, queueing theory, and of course, complex
systems science, to reason holistically about global emergent behavior
of such systems.</p>
<p>We claim the presence calculus is a novel, constructive approach to
do this - a new and powerful reasoning tool for anyone working with
complex systems.</p>
<p>But this is a bold claim, and it deserves further scrutiny and
validation, and so we invite anyone interested in doing so to
collaborate with us on this open source project.</p>
<h3 id="learning-more-about-the-presence-calculus">Learning more about
The Presence Calculus</h3>
<p>While the calculus was developed with mathematical rigor, an equally
important<br />
goal was not to let mathematics get in the way of understanding the
simple but very powerful and general ideas the calculus embodies<a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>.</p>
<p>In this document, we’ll motivate and introduce the key ideas in the
calculus<br />
informally, with lots of evocative examples and simplifications to<br />
illustrate concepts.</p>
<p>While it is aimed at conveying the main concepts intuitively, in
order to understand the ideas, some basic mathematical notation is used
in key sections. We augment these with examples to build intuition
throughout. However, given the nature of the material we have opted to
stay on the side of rigor rather than dilute the concepts, even in this
“gentle” introduction.</p>
<p>If you are inclined to skim over anything with mathematical notation
in it, working through the examples should be sufficient to grasp the
key ideas and claims. However, for those who are comfortable with it,
the mathematical notation gives precise and rigorous definitions that
are easy to understand and verify.</p>
<p>We recommend reading and understanding all the main ideas here before
jumping deeper into the rest of the documentation at this site, which
does get a fair bit more dense and technical.</p>
<p>If that deeper dive is not your cup of tea, we’ll continue with
ongoing informal exposition on our blog <a
href="https://www.polaris-flow-dispatch.com">The Polaris Flow
Dispatch</a>, where we will focus mostly on applications of the
ideas.</p>
<p>So this document can be thought as the middle ground: detailed enough
to understand the concepts and ideas clearly and precisely, but just a
starting point if you want to really dig deeper.</p>
<p>The next level of detail is in the API docs for <a
href="https://py.pcalc.org">The Presence Calculus<br />
Toolkit</a>.</p>
<p>The toolkit is an open source python library that implements all the
core concepts in the presence calculus. In the API documentation, we go
into the concepts at a level of rigor that you’ll need to work with the
pcalc API and apply the concepts. Some mathematical background will be
useful here if you want to apply the concepts and extend beyond the
core.</p>
<p>The toolkit is reference implementation of the presence calculus. It
is designed as an analytical middleware layer suitable for interfacing
real world operational systems and complex system simulation, to the
analytical machinery of the presence calculus.</p>
<p>If reading code is more your style than reading mathematical
notation, then we recommend jumping into the code and running the
examples (or writing your own) to understand how things work. If you
find bugs, please raise them!</p>
<p>Finally, for those who want to dive deeper into the formal
mathematical<br />
underpinnings of the calculus, we have the theory track, which perhaps
goes<br />
into more detail than most people will need to read or understand, but
is<br />
useful for the mathematically trained to connect the ideas to their
roots in<br />
mainstream mathematics.</p>
<p>Let’s jump in…</p>
<h3 id="why-presence">Why Presence?</h3>
<p>Presence is how reality reveals itself.</p>
<p>We do not perceive the world as disjointed events in time, but rather
as an unfolding—things come into being, endure for a time, and slip
away.</p>
<p>Permanence is just a form of lasting presence. What we call
<em>change</em> is the<br />
movement of presences in and out of our awareness, often set against
that<br />
permanence.</p>
<p>The sense of something being present—or no longer present—is our
most<br />
immediate way of detecting change.</p>
<p>This applies to both the tangible—people, places, and things—and
the<br />
intangible—emotions, feelings, and experiences.</p>
<p>Either way, we reason about the world by reasoning about the
presences and<br />
absences in our environment over time.</p>
<p>The presence calculus begins here. Before we count, measure, compare,
or<br />
optimize, we observe what <em>is</em>.</p>
<p>And what we model is presence.</p>
<h3 id="an-example">An example</h3>
<p>Imagine you see a dollar bill on the sidewalk on your way to get
coffee.<br />
Later, on your way back home, you see it again—still lying in the same
spot.<br />
It would be reasonable for you to assume that the dollar bill was
present<br />
there the whole time.</p>
<p>Of course, that may not be true. Someone might have picked it up in
the<br />
meantime, felt guilty, and quietly returned it. But in the absence of
other<br />
information, your assumption holds: it was there before, and it’s there
now,<br />
so it must have been there in between.</p>
<p>This simple act of inference is something we do all the time. We fill
in gaps,<br />
assume continuity, and reason about what must have been present based on
what<br />
we know from partial glimpses of the world.</p>
<p>The presence calculus gives formal shape to this kind of
inference—and shows<br />
how we can build upon it to <em>reason</em> about presence and
<em>measure</em> its<br />
effects in an environment.</p>
<h3 id="a-software-example">A software example</h3>
<p>Since the ideas here emerged from the software world, let’s begin
with a<br />
mundane, but familiar example: task work in a software team.</p>
<p>By looking closely at how we reason about tasks, we can see how a
subtle shift<br />
to a presence-centered perspective changes not just what we observe, but
what<br />
we measure, and thus can reason about.</p>
<p>We usually reason about task work using <em>events</em> and
<em>snapshots</em> of the state<br />
of a process in time. A task “starts” when it enters development,
and<br />
“finishes” when it’s marked complete. We track “cycle time” by measuring
the<br />
elapsed time between events, “throughput” by counting finish events,
and<br />
“work-in-process” by counting tasks that have started but not yet
finished.</p>
<p>When we look at a Kanban board, we see a point-in-time snapshot of
where tasks<br />
are at that moment—but not how they got there. And by the time we read
a<br />
summary report of how many tasks were finished and how long they took to
go through the process on average, much of the history of the system
that produced those measurements has been lost. That makes it hard to
reason about <em>why</em> those measurements are the way they are.</p>
<p>In complex knowledge work, each task often has a distinct
history—different<br />
from other tasks present at the same time. Losing history makes it hard
to<br />
reason about their interactions and how they impact the global behavior
of the<br />
process.</p>
<p>This problem is not unique to task work. Similar problems exist in
almost all areas of business analysis that rely primarily on descriptive
statistics as the primary measurement tool for analyzing system
behavior.</p>
<p>We are reduced to trying to make inferences from local
descriptive<br />
statistics —things like cycle times, throughput, and work-in-process
levels- over a rapidly changing process.</p>
<p>We try to reason about a process which is shaped by its history,
whose behavior emerges from non-uniform interactions of individual
signals, with measurement techniques that struggle to represent or
reason about that history or the interactions.</p>
<p>This is difficult to do, and we have no good tools right now that are
fit for this purpose. This is where the presence calculus begins.</p>
<p>The calculus focuses on the time <em>in between</em> snapshots of
history: when a task was present, where it was present, for how long,
and whether its presence shifted across people, tools, or systems.</p>
<p>The connective tissue is no longer the task itself, or the process
steps it<br />
followed, or who was working on it, but a continuous, observable
<em>thread of<br />
presence</em>—through all of them, moving through time, interacting,
crossing boundaries—a mathematical representation of history.</p>
<p>With the presence calculus, these threads and their interactions
across time<br />
and space can now be measured, dissected, and analyzed as
first-class<br />
constructs—built on a remarkably simple primitive—the presence.</p>
<h3 id="the-heart-of-the-matter">The heart of the matter</h3>
<p>At its core, the calculus exploits the difference between the two
independent<br />
statements—“The task started development on Monday” and “The task
completed<br />
development on Friday”—and a single, unified assertion: “The task was
present<br />
in development from Monday through Friday.”</p>
<p>The latter is called a <em>presence</em>, and it is the foundational
building block<br />
of the calculus.</p>
<p>At first glance, this might not seem like a meaningful
difference.</p>
<p>But treating the presence as the primary object of reasoning—as a
<em>first-class</em> construct—opens up an entirely new space of
possibilities.</p>
<p>Specifically, it allows us to apply powerful mathematical tools that
exploit the continuity of time and the algebra of time intervals to
reason about the interactions and emergent configurations of presences
in a rigorous and structured, and more importantly, computable way.</p>
<h2 id="what-is-a-presence">2. What is a Presence?</h2>
<p>Let’s start by building intuition for the concept of presence.
Consider the statement: “The task <span class="math inline">\(X\)</span>
was in Development from Monday to Friday.”</p>
<p>In the presence calculus, this would be expressed as a statement of
the form:<br />
“The element <span class="math inline">\(X\)</span> was in boundary
<span class="math inline">\(Y\)</span> from <span
class="math inline">\(t_0\)</span> to <span
class="math inline">\(t_1\)</span> with mass 1.”</p>
<p>Presences are statements about elements (from some domain) being
present in a<br />
boundary (from a defined set of boundaries) over a <em>continuous</em>
period of time, measured using some timescale.</p>
<p>So why do we say “with mass 1”?</p>
<p>The presence calculus treats time as a physical dimension, much like
space.<br />
Just as matter occupies space, presences occupy time. Just as mass
quantifies<br />
<em>how</em> matter occupies space, the mass of a presence quantifies
<em>how</em> a<br />
presence occupies time.</p>
<p>The statement “The task <span class="math inline">\(X\)</span> was in
Development from Monday through Friday” is<br />
a <strong>binary presence</strong> with a uniform mass of 1 over the
entire duration. The<br />
units of this mass are element-time—in this case, task-days.</p>
<p>Binary presences are sufficient to describe the <em>fact</em> of
presence or absence<br />
of things in places in a domain. These presences always have mass 1 in
whatever units we use for elements and time.</p>
<h3 id="presence-mass">Presence mass</h3>
<p>Let’s consider a more detailed set of statements:</p>
<blockquote>
<p>“Task <span class="math inline">\(X\)</span> had 2 developers working
on it from Monday to Wednesday,<br />
3 developers on Thursday, and 1 developer on Friday.”</p>
</blockquote>
<p>These are no longer about just presence, but about the
<em>effects</em> of presence.<br />
They describe the <strong>load</strong> that task <span
class="math inline">\(X\)</span> placed on the Development
boundary<br />
over time.</p>
<p>The units of this presence are developer-days - potentially in a
completely different space from the task, but grounded over the same
time interval as the task.</p>
<p>Here we are saying: “the task being in this boundary over this time
period, had this effect.”</p>
<p>We will describe such presences using a <em>presence density
function,</em> called <span class="math inline">\(\mathsf{load}\)</span>
in this case:</p>
<ul>
<li><span class="math inline">\((\mathsf{load}, X, \text{Development},
\text{Monday}, \text{Wednesday}, 2)\)</span></li>
<li><span class="math inline">\((\mathsf{load}, X, \text{Development},
\text{Thursday}, \text{Thursday}, 3)\)</span></li>
<li><span class="math inline">\((\mathsf{load}, X, \text{Development},
\text{Friday}, \text{Friday}, 1)\)</span></li>
</ul>
<p>Here, <span class="math inline">\(\mathsf{load}(e, b, t)\)</span> is
a time-varying function that takes an<br />
element <span class="math inline">\(e\)</span>, a boundary <span
class="math inline">\(b\)</span>, and a time <span
class="math inline">\(t\)</span>, and returns a real-valued number<br />
describing how much presence is concentrated at that point in time.</p>
<p>The <em>presence mass</em> of such a presence is the total presence
over the<br />
interval <span class="math inline">\([t_0, t_1]\)</span>, defined
as:</p>
<p><span class="math display">\[ \text{mass} = \int_{t_0}^{t_1}
\mathsf{load}(e, b, t)\, dt \]</span></p>
<p>where <span class="math inline">\(\mathsf{load}\)</span> is the
presence density function <a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>Binary presences are much easier to understand intuitively, but the
real power of the presence calculus comes from generalizing to
<em>presence density functions</em>.</p>
<h3 id="domain-signals-and-presence-density-functions">Domain Signals
and Presence Density Functions</h3>
<p>In our earlier example, we showed a presence that described the
<em>load</em> placed on an element at a boundary, and this has a
real-valued presence mass. More generally, we can think of defining a
presence over an arbitrary time varying function with real numbers as
values.</p>
<p>Such functions are called <em>presence density functions</em>. In
general, these presence density function represent some underlying
signal from the domain that we are interested in measuring. So, in what
follows, we will use the terms signal and presence density functions
interchangeably in what follows, opting for the latter only those cases
where we want to focus specifically on the fact that what we are
representing about the signal is the “amount” of the signal ( its
presence) over time.</p>
<p>As shown in Figure 2, the mass of a presence density function, over
any given time <em>interval</em> <span class="math inline">\([t_0,
t1)\)</span> is the integral over the interval, which is also the area
under the signal over that interval<a href="#fn3" class="footnote-ref"
id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/presence_definition.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 2: Signals, Presence,  and Presence Mass</code></pre>
</div>
</div>
<p>The only requirement for a function to be a presence density function
(signal) is that it is <em>measurable</em>, and that you can interpret
<em>presence mass</em>—defined as the integral of the function over a
finite interval—as a meaningful <em>measure</em> of the effect of
presence in your domain.</p>
<p>This is where measure theory enters the picture. It’s not essential
to understand the full technical details, but at its core, measure
theory tells us which kinds of functions are measurable—in other words,
which functions can support meaningful accumulation, comparison, and
composition of values via integration.</p>
<p>When a presence density functions (signal) is measurable,it gives us
the confidence to do things like compute statistics, aggregate over
elements or boundaries, and compose presence effects—while preserving
the semantics of the domain.</p>
<p>Informally, when a presence density function is a measurable we can
treat its values like any other real number and do math over them, as
long as we carefully respect the units involved.</p>
<p>From our perspective, a presence density function is a domain signal
whose value that can be <em>accumulated</em> across time and across
presences.</p>
<p>This lets us reason mathematically about presences with
confidence—and since most of this reasoning will be performed by
algorithms, we need technical<br />
constraints that ensure those calculations are both mathematically valid
and<br />
semantically sound.</p>
<h3 id="more-examples">More examples</h3>
<p>Let’s firm up our intuition about what presences can describe with a
few more<br />
examples of presence density functions.</p>
<h4 id="work-in-software">“Work” in software</h4>
<p>If you’ve ever written a line of code in your life, you’ve heard the
question:<br />
“When will it be done?” Work in software can be a slippery, fungible
concept—<br />
and the presence calculus offers a useful way to describe it.</p>
<p>We can express the work on a task using a presence density function
whose<br />
value at time <span class="math inline">\(t\)</span> is the
<em>remaining</em> work on the task at <span
class="math inline">\(t\)</span>.</p>
<p>This lets us model tasks whose duration is uncertain in general, but
whose<br />
remaining duration can be estimated, subject to revision, at any given
time—a common scenario in software contexts.</p>
<p>A series of presences, where the (non-zero) mass of each presence
corresponds<br />
to the total remaining work over its interval (interpreting the integral
as a<br />
sum), gives us a way to represent <em>work as presence</em>.</p>
<p>Such presences can represent estimates, forecasts, or
confidence-weighted<br />
projections—and as we’ll see shortly, they can be reasoned about and
computed<br />
with just like any other kind of presence.</p>
<h4 id="the-effects-of-interruptions">The effects of interruptions</h4>
<p>Another useful example from the software world illustrates a
different<br />
application of a presence. Let’s assume the boundary in this case is
a<br />
developer, the element is an interruption (defined appropriately in
the<br />
domain), and the presence density function captures the <em>context
switching<br />
cost</em>—measured in lost time—associated with that interruption.</p>
<p>The key insight here is that the <em>effects</em> of the presence
extend beyond the<br />
interval of the interruption itself. This is a classic case of a delayed
or<br />
<em>decaying effect</em>—a pattern that appears frequently in real-world
systems.</p>
<p>The presence density function can be modeled in different ways:</p>
<ul>
<li>As a constant cost: for example, each interruption causes a
fixed<br />
15-minute recovery period, regardless of its duration.</li>
<li>As a decaying function: the cost is highest at the moment of
interruption<br />
and gradually decreases to zero over a defined recovery window
(e.g.<br />
15 minutes), representing a return to full focus.</li>
</ul>
<p>This approach gives us a precise way to model and reason about
the<br />
<em>aftereffects</em> of events—effects that outlast the events
themselves and<br />
accumulate in subtle but measurable ways over longer timeframes.</p>
<p>In this case, we measured an effect that decayed from a peak, but a
similar<br />
approach can be taken, for example, with a presence density function
that<br />
grows from zero and plateaus over the duration of the presence—such as
the net increase in revenue due to a released feature.</p>
<p>Used this way, presence density functions give us a powerful tool for
modeling the impact of binary presences—capturing their downstream or
distributed effects over time, and reasoning about their relationship
over a shared timeline.</p>
<p>Another important use case in the same vein is modeling the cost of
delay for a portfolio-level element—and analyzing its cascading impact
across the<br />
portfolio.</p>
<p>These use cases show that it is possible to analyze not just binary
presences,<br />
but entire chains of influence they exert across a timeline—a key
prerequisite<br />
for reasoning about causality.</p>
<h4 id="self-reported-developer-productivity">Self-reported developer
productivity</h4>
<p>Imagine a developer filling out a simple daily check-in:<br />
“How productive did you feel today?”—scored from 1 to 5, or sketched out
as a<br />
rough curve over the day<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>.</p>
<p>Over a week, this forms a presence density function—not of the
developer in a<br />
place, but of their <em>sense</em> of productivity over time.</p>
<p>These types of presences, representing perceptions, are
powerful—helping<br />
teams track experience, spot early signs of burnout, or correlate
perceived<br />
flow with meetings, environment changes, build failures, or
interruptions.</p>
<p>Now, let’s look at some examples outside software development.</p>
<h4 id="browsing-behavior-on-an-e-commerce-site">Browsing behavior on an
e-commerce site</h4>
<p>Imagine a shopper visiting an online store. They spend 90 seconds
browsing<br />
kitchen gadgets, then linger for five full minutes comparing
high-end<br />
headphones, before briefly glancing at a discounted blender.</p>
<p>Each of these interactions can be modeled as a presence: the
shopper’s<br />
(element) attention occupying different parts of the site (boundaries)
over<br />
time. The varying durations reflect interest, and the shifting presence
reveals<br />
patterns of engagement.</p>
<p>By analyzing these presences—where and for how long attention
dwells—we can<br />
begin to understand preferences, intent, and even the likelihood of
conversion<br />
(modeled as a different presence density function).</p>
<h4 id="patient-movement-in-a-hospital">Patient movement in a
hospital</h4>
<p>Consider a patient navigating a hospital stay. They spend the morning
in<br />
radiology, move to a recovery ward for several hours, then are
briefly<br />
transferred to the ICU overnight.</p>
<p>Each location records a presence—when and where the patient was, and
for how<br />
long. These presences can reveal bottlenecks, resource utilization,
and<br />
potential risks.</p>
<p>Over time, analyzing patient presences helps surface patterns in
care<br />
delivery, delays in treatment, and opportunities for improving patient
flow.</p>
<p>These are examples of classic operations management problems
expressed in the<br />
language of the presence calculus. The calculus is well-suited to
modeling<br />
scenarios like these as a base case.</p>
<h2 id="systems-of-presences">3. Systems of Presences</h2>
<p>Let’s summarize what we’ve described so far.</p>
<p>With signals and presences, we now have a framework for describing
and measuring the behavior of time-varying domain signals—each
representing how a specific element behaves within a given boundary.</p>
<p>The key feature of a presence is that it abstracts these behaviors
into a uniform representation—one that we can reason about and compute
with.</p>
<p>To summarize, a general presence is defined by:</p>
<ul>
<li>a density function <span class="math inline">\(f(e, b,
t)\)</span>,</li>
<li>an element <span class="math inline">\(e\)</span>,</li>
<li>a boundary <span class="math inline">\(b\)</span>,</li>
<li>and a time interval <span class="math inline">\([t_0,
t_1]\)</span>.</li>
</ul>
<p>Its <strong>mass</strong> is the integral of <span
class="math inline">\(f\)</span> over the interval:</p>
<p><span class="math display">\[ \text{mass}(e, b, [t_0, t_1]) =
\int_{t_0}^{t_1} f(e, b, t)\, dt \]</span></p>
<p>This mass captures both <em>that</em> the element was present, and
<em>how</em> it was<br />
present—uniformly, variably, or intermittently—over the time interval of
the presence.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/pdf_examples.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 3: Signals and Presences</code></pre>
</div>
</div>
<h3 id="presence-as-a-sample-of-a-signal">Presence as a sample of a
signal</h3>
<p>A domain signal, in general, describes the continuous behavior of an
element within a boundary over time. This continuous signal may have one
or more disjoint periods where its value is non-zero. These non-zero
periods are called the <em>support</em> of the signal.</p>
<p>As we see in Figure 4, a single signal (for a given element and
boundary) might have multiple support intervals. These may correspond to
episodic behavior in the underlying domain, for example, user sessions
in an e-commerce context, or rework loops in software development when a
task “returns” to development many times over its lifecycle.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/multiple_support.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 4: A presence mass as a sample of a signal over an interval.</code></pre>
</div>
</div>
<p>A <em>presence</em> (the 5-tuple <span class="math inline">\(p = (e,
b, t_0, t_1, m)\)</span> that we work with in the calculus) is generated
by taking an <em>observation</em> of this underlying signal over a
specific time interval <span class="math inline">\([t_0, t_1)\)</span>,
and computing its mass. This interval can be chosen in many ways:</p>
<ul>
<li>It might perfectly align with a single support interval (a ‘hill’ in
the signal).</li>
<li>It might span multiple disjoint support intervals, including the
“zero” regions in between.</li>
<li>It might capture only a portion of a single support.</li>
</ul>
<p>All we require is that the interval chosen for the presence
calculation intersects a region of non-zero area from the signal that
can be reduced to a non-zero presence mass.</p>
<h3 id="presence-assertions">Presence Assertions</h3>
<p>Thus presence is best thought of as a <em>sampled measurement</em> of
the underlying signal, taken by an <em>observer</em> over a specific
time interval, which yields a total presence mass for that interval.</p>
<p>A given observer may not even “see” the full underlying signal—only
the <em>mass</em><br />
of the presence they experience over the interval they observed.</p>
<p>Different observers may observe different intervals of the same
signal and derive different presence values, depending on what part of
the function they<br />
encounter.</p>
<p>This brings us to the concept of <em>presence assertions</em>, which
formalize the act of recording a presence based on an observer’s local
“view” of the underlying density function.</p>
<p>A <em>presence assertion</em> is simply a presence augmented with
metadata:</p>
<ul>
<li><em>who</em> the observer was</li>
<li>and an <em>assertion timestamp</em>—the time at which the
observation was made.</li>
</ul>
<p>The assertion time doesn’t need to align with the time interval of
the presence. This allows assertions to refer to the past, reflect the
present, or even anticipate the future behavior of a signal.</p>
<h4 id="the-open-world-assumption">The open world assumption</h4>
<p>Time in the presence calculus is explicitly defined to be over
<em>extended</em> reals <span
class="math inline">\(\overline{\mathbb{R}}\)</span>: the real line
<span class="math inline">\(\mathbb{R}\)</span> extended with the
symbols <span class="math inline">\(-\infty\)</span> and <span
class="math inline">\(+\infty\)</span>.</p>
<p>This is a mathematical representation of an open world assumption,
which holds that the history of a system of presences extends
indefinitely into the past and future.</p>
<p>An observer will typically only see a finite portion of this history
and has to make inferences on the basis of those observations, but in
general, we need to make inferences with partial information about the
past and contingent assumptions about the future.</p>
<p>A presence with <span class="math inline">\(t_0 = -\infty\)</span>
represents a presence whose beginning is unknown, and <span
class="math inline">\(t_1 = +\infty\)</span> represents a presence whose
end is unknown.</p>
<p>Presences with both start and end unknown are valid constructs and
represent eternal presences.</p>
<p>Many of the most interesting questions in the presence calculus
involve reasoning about the dynamics of a domain under the epistemic
uncertainty introduced by such presences.</p>
<h4 id="a-note-on-epistemology">A note on epistemology</h4>
<p>Presence assertions give us the ability to assign <em>provenance</em>
to a presence—<br />
not just <em>what</em> we know, but <em>how</em> we know it. This is
essential in<br />
representing complex systems where the observer and the act of
observation<br />
are first-class concerns.</p>
<p>Further, since reasoning about time and history is a primary focus of
the calculus, presences with unknown beginnings or endings provide a way
to explicitly model what is known—and unknown—about that history. This
will prove more valuable than it might initially seem.</p>
<p>We won’t go too deeply into the epistemological aspects of the
presence<br />
calculus in this document—this remains an active and open area of
research and development and complements much of what we discuss
here.</p>
<p>But it’s important to acknowledge that this layer exists, that
modeling and interpreting the output of the presence calculus requires
an explicit treatment of how observations are made and by whom, and the
fact that this has a huge impact on the validity of the inferences one
makes using the machinery of the calculus.</p>
<p>With this caveat in place, once we’ve represented a problem domain as
a<br />
<em>system of presences</em>, much of the machinery of the presence
calculus (which<br />
we’ll introduce next) can be applied uniformly.</p>
<p>In the next sections, where we introduce this machinery, we will
operate under the assumption that there exists an observable
<em>signal</em>, and that what is observed reflects what an observer
knows about the domain. We don’t presume anything about the “truth” of
the observations, we treat them uniformly as signals.</p>
<p>One thing we will see is that from the perspective of this machinery,
there are no fundamental differences in behavior between binary signals
and arbitrary signals once they’ve been reduced to a canonical,
presence-mass oriented representation<a href="#fn5" class="footnote-ref"
id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>This greatly increases the scope of the problem domains where the
presence calculus can be applied, and our examples in the previous
section only begin to hint at the possibilities.</p>
<p>This, in the end, is the source of the generality and power of the
presence calculus.</p>
<h4 id="a-note-on-path-dependence">A note on path dependence</h4>
<p>By representing a presence at the granularity of an element in a
boundary, we explicitly recognize the path dependent nature of the
domain signals.</p>
<p>In the presence calculus, signals represent some behavior of domain
elements in boundaries. The calculus itself is agnostic to what counts
as an “element” or a “boundary”— this is a domain modeling decision.</p>
<p>Even if they represent the same underlying quantity, we recognize
that system behavior emerges from the interactions between
<em>individual</em> signals at the element-boundary granularity—each
potentially with distinct presence density functions. We are interested
in studying <em>how</em> system-level behavior arises from the
<em>interactions</em> between these signals over time.</p>
<p>Modeling boundaries are crucial because, for a given domain element,
the boundary typically determines both which signals are relevant and
how we wish to analyze system behavior using them.</p>
<p>For example, a software feature (an element) may be modeled using one
set of signals during development (one boundary), another during
production (a second boundary), and yet another when customers begin
using it (a third boundary).</p>
<p>All three signals are part of the feature’s history. Each feature
follows a unique path through these boundaries, producing its own
independent set of signals with a distinct history and evolution. These
signals interact in complex ways—over time, within boundaries, and with
each other. The boundary is what brings coherence to the analysis—it
defines which signals and interactions we choose to focus on, and
why.</p>
<p>In summary, the boundary allows us to bring a coherent set of
elements and related signals together for analysis. That analysis
focuses on how these signals interact in <em>time</em>.</p>
<p>Constructing an appropriate set of element-boundary signals is
<em>the</em> key modeling decision. But once these are defined, much of
the machinery of the presence calculus can be applied without regard to
the semantics of the specific element-boundaries involved.</p>
<p>Semantics are, of course, crucial in <em>interpreting</em> the
inferences one draws<br />
using the machinery of the calculus.</p>
<p>When we refer to a “system” in the presence calculus we are
explicitly defining it as an <em>evolving</em> set of presence
assertions—i.e., the “system” is what we can assert about a domain using
presence assertions at a given time.</p>
<p>In summary, this is what we refer to as a “system of presences” - a
time-indexed collection of presence assertions derived from an
underlying set of path dependent element-boundary signals.</p>
<p>What we choose to model fundamentally shapes the system we are able
to reason about—and the presence calculus provides a powerful foundation
for doing so.</p>
<h2 id="co-presence-and-the-presence-invariant">4. Co-Presence and The
Presence Invariant</h2>
<p>In the last section, we introduced systems of presences as
collections of presence assertions, which are derived from observable
signals.</p>
<p>Figure 5 illustrates an example of such a system, where we focus on
the subset of presences defined over a <em>shared observation
interval</em>.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/presence_invariant_continuous.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 5: The Presence Invariant</code></pre>
</div>
</div>
<p>These presences are called <em>co-present</em>—they represent an
observer making simultaneous measurements of presence mass across
multiple signals over a common interval of time.</p>
<p>Co-presence is a necessary (but not sufficient) condition for
interaction between one or more signals. This section introduces a key
construct: the presence invariant. It expresses a general and powerful
relationship that holds for any co-present subset of presences within a
finite observation window, and it is a fundamental relationship that
governs how the masses of co-present signals interact.</p>
<p>Let’s establish this relationship.</p>
<p>Given, any finite observation interval, we’ve already shown that each
presence density function has a <em>presence mass</em>, defined as the
integral of the density over the observation interval.</p>
<p>These can be thought of as mass <em>contributions</em> from those
presences to that interval. The sum of these individual mass
contributions gives the total presence mass observed across the system
in that interval.</p>
<p>In our example from Figure 5,</p>
<p><span class="math display">\[ A = M_0 + M_1 + M_3 \]</span></p>
<p>is the total mass contribution from the signals that have non-zero
mass over the interval <span class="math inline">\([t_0, t_1)\)</span>.
The length of this interval is <span class="math inline">\(T = t_1 -
t_0\)</span>.</p>
<p>Since the mass comes from integrating a density function over time,
the quantity <span class="math inline">\(\delta = \frac{A}{T}\)</span>
represents the <em>average presence density</em> over the observation
interval <span class="math inline">\(T\)</span>.</p>
<p>We can now decompose this as:</p>
<p><span class="math display">\[ \delta = \frac{A}{T} = \frac{A}{N}
\times \frac{N}{T} \]</span></p>
<p>Here <span class="math inline">\(N\)</span> is the number of
<em>active signals</em>: distinct signals with a presence in the
observation window<a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a>. This separates the average presence
density into two interpretable components:</p>
<ul>
<li><span class="math inline">\(\bar{m} = \frac{A}{N}\)</span>: the
<em>average mass contribution</em> per active signal,</li>
<li><span class="math inline">\(\iota = \frac{N}{T}\)</span>: the signal
<em>incidence rate</em>—i.e., the number of active signals per unit
time.</li>
</ul>
<p>This leads to the <em>presence invariant</em>:</p>
<p><span class="math display">\[ \text{Average Presence Density} =
\text{Signal Incidence Rate} \times \text{Average Mass Contribution per
Signal} \]</span> or in our notation</p>
<p><span class="math display">\[ \delta = \iota \cdot \bar{m}
\]</span></p>
<p>This identity holds for <em>any</em> co-present subset of signals
over <em>any</em> finite time interval.</p>
<p>The key insight here is that the presence invariant establishes a
fundamental relationship between the mass contributions of individual
signals and what is observable about their interactions at a boundary
over time—that is, their presence density. This is precisely the kind of
relationship the presence calculus is designed to capture.</p>
<p>The fact that this invariant holds for any finite observation
interval is a powerful constraint—one we will exploit in reasoning about
system behavior with the calculus.</p>
<h3 id="an-example-1">An example</h3>
<p>To build intuition for these abstract terms, let’s look at a
practical example.</p>
<p>For example, suppose our signals represent revenues from customer
purchase over some time period. If we look at a system of presences,
across an interval of time, say a week, the total presence mass <span
class="math inline">\(A\)</span> represents the total revenues across
all customers who contributed to that revenue. <span
class="math inline">\(T\)</span> is the time period measured in some
unit of time (say days) and <span class="math inline">\(N\)</span> is
the number of paying customers in that period.</p>
<p>The average presence density is the daily revenue rate, the signal
mass contribution for each signal is revenue for each customer, the
average signal mass contribution is the average revenue per customer for
that week, and the incidence rate represents the average daily rate of
active customers over the week.</p>
<p>So the presence invariant is stating that the revenue rate for the
week is the product of the average revenue per customer and the average
number of active customers over the week.</p>
<h3 id="why-it-matters">Why it matters</h3>
<p>While algebraically, the presence invariant is a tautology, it
imposes a powerful constraint on system behavior—one that is independent
of the specific system, semantics, or timescale. Think of it as the
generalization of our intuitive revenue example to any arbitrary system
of presences.</p>
<p>Indeed, it forms a foundational conservation law of the presence
calculus: the <em>conservation of mass (contribution)</em>.</p>
<p>Just as the conservation of energy or mass constrains the evolution
of physical systems—regardless of the specific materials or forces
involved—the conservation of presence mass constrains how observable
activity is distributed over time in a system of presences. This
conservation law applies to how presence mass is conserved across
time.</p>
<p>While independent of the semantics of what is being observed, like
energy, presence mass can shift, accumulate, or redistribute, but its
total balance across presences within a finite time interval remains
invariant.</p>
<p>Thus, the conservation of mass plays a role in the presence calculus
similar to that of other conservation laws in physics: it constrains the
behavior of three key observable, measurable parameters of any system of
presences.</p>
<p>It is important to note that this makes the “averages” in the
presence invariant much more than descriptive statistics. While they may
be interpreted as such, these are not simply measures of centrality on a
set of observed presences, but quantities with a concrete physical
interpretation, expressed via the invariant, that directly govern how a
system of presences behaves in time.</p>
<p>Exploiting this constraint allows us to study and characterize the
long-run behavior of a system.</p>
<h3 id="binary-presences-and-littles-law">Binary Presences and Little’s
Law</h3>
<p>At this stage, the presence invariant may still feel a bit abstract.
Let’s make it more concrete by interpreting this identity in the special
case of <em>binary</em> presences.</p>
<p>Recall that a <em>binary</em> signal is a function whose density is
either <span class="math inline">\(0\)</span> or <span
class="math inline">\(1\)</span>. That is, we are modeling the presence
or absence of an underlying signal in the domain.</p>
<p>In this case, the <em>mass contribution</em> of a signal becomes an
<em>element-time duration</em>. For example, if the signal represents
the time during which a task is present in development, the mass
contribution of that task over an observation interval is the portion of
its duration that intersects the interval. This is also called the
<em>residence time</em><a href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a> for the task in the observation
window.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/presence_invariant_binary.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 6: The Presence Invariant for binary signals</code></pre>
</div>
</div>
<p>Figure 6 shows possible configurations of binary signals intersecting
a finite observation interval. Suppose the unit of time is days.</p>
<p>The total presence mass accumulation <span
class="math inline">\(A\)</span> is <span
class="math inline">\(11\)</span> task-days. The number <span
class="math inline">\(N\)</span> of tasks that intersect the observation
interval is <span class="math inline">\(4\)</span>. The length of the
observation window is <span class="math inline">\(T = 4\)</span> days.
It is straightforward to verify that the presence invariant holds.</p>
<p>Now, let’s interpret its meaning.</p>
<p>Since each task contributes <span class="math inline">\(1\)</span>
unit of mass for each unit of time it is present, the average presence
density <span class="math inline">\(\delta=\frac{A}{T}\)</span>
represents the <em>average number of tasks</em> present per unit time in
the interval—denoted <span class="math inline">\(L\)</span>.</p>
<p>Conversely, since each unit of mass corresponds to a unit of time
associated with a task, the average mass per active signal, <span
class="math inline">\(\bar{m} = \frac{A}{N}\)</span>, is the average
time a task spends in the observation window. This value is typically
called the <em>residence time</em> <span
class="math inline">\(w\)</span> of a task in the observation
window.</p>
<p>The incidence rate <span class="math inline">\(\iota =
\frac{N}{T}\)</span> may be interpreted as the <em>activation rate</em>
of tasks in the interval—a proxy for the rate at which tasks start
(onset) or finish (reset) within the window.</p>
<p>For example, <span class="math inline">\(N\)</span> may be counted as
the number of tasks that start inside the interval, plus the number that
started before but are still active. Thus, <span
class="math inline">\(\frac{N}{T}\)</span> is a <em>cumulative onset
rate</em> <span class="math inline">\(\Lambda\)</span>.</p>
<p>The presence invariant can now be rewritten as:</p>
<p><span class="math display">\[ L = \Lambda \times w \]</span></p>
<p>which you may recognize as <em>Little’s Law</em> in its finite-window
form.</p>
<p>Thus, the presence invariant serves as a <em>generalization of
Little’s Law</em>—extending it to arbitrary systems of presence density
functions (signals) measured over finite observation windows.</p>
<p>It is important to note that we are referring to <em>Little’s Law
over a finite observation window</em>, rather than the much more
familiar, steady-state equilibrium form of Little’s Law.</p>
<p>Unlike the equilibrium form of the law, this version holds
<em>unconditionally</em>. The key is that the quantities involved are
<em>observer-relative</em>: the time tasks spend <em>within a finite
observation window</em>, and the <em>activation rate</em> of tasks
<em>over the window</em>, rather than the task-relative durations or
long-run arrival/departure rates assumed in the equilibrium form.</p>
<p>Indeed, the difference between these two forms of the identity will
serve as the basis for how we <em>define</em> whether a system of
presences is in equilibrium or not. The idea is that the system of
presences is at equilibrium when observed over sufficiently long
observation windows such that the observer-relative and task-relative
values of average presence density, incidence rate and average presence
mass converge.</p>
<p>Since complex systems often operate far from equilibrium—and since
the presence invariant holds <em>regardless</em> of equilibrium—the
finite-window form becomes far more valuable for analyzing the long-run
behavior of such systems as they <em>move into and between</em>
equilibrium states.</p>
<p>All this is the focus of section 6, where we will formally make the
connections between the presence invariant and equilibrium states in
systems of presence with the help of a very general form of Little’s Law
originally proven by Stidham.</p>
<p>For now, we will note that for any arbitrary signal, we can always
define a binary presence corresponding to the intervals over which the
value of the density function is non-zero (the support interval). In
general then, we can say the finite window version of Little’s Law, with
the above definitions, always applies to <em>any</em> signal under this
interpretation.</p>
<p><em>In addition,</em> the general presence invariant also applies to
the full signal. Yet another version applies in equilibrium states—but
all are, in effect, different manifestations of the same underlying
relationship captured in the presence invariant.</p>
<p>We will return to this important topic shortly. But first, let’s
understand the implications of the presence invariant.</p>
<h3 id="causal-reasoning">Causal Reasoning</h3>
<p>The presence invariant gave us an important constraint that applies
to the behavior of three key parameters of a system of presences when
measured over any finite time interval: the average presence density,
the signa incidence rate, and the average mass contribution per
signal.</p>
<p>This means that if we observe the behavior of a system of presences
over a continuous sequence of non-overlapping time intervals, the
presence invariant holds in <em>each</em> such interval, and given the
value of any two of the parameters, the third is completely
determined.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/system_presences_discrete.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 7: Sampling a system of presence across uniform intervals. </code></pre>
</div>
</div>
<p>The requirement that presence mass is conserved across each interval
means that there are only two degrees of freedom between three
variables.</p>
<p>Furthermore, within the structure of this identity, presence density
is the resultant variable, while the incidence rate and signal mass
contribution act as its fundamental, independent variables.</p>
<p>Thus, the invariant explicitly states that any change in presence
density is an <em>outcome</em> that is necessarily accounted for by a
change in incidence rate or signal mass contribution.</p>
<p>This means that if you observe a shift in presence density, you know
it must originate from changes in either how frequently signals appear
(incidence rate) or how much ‘mass’ each signal contributes on average
(average mass contribution), or both.</p>
<p>Since the invariant holds across <em>any</em> finite interval, this
also implies that if we study how these parameters change in concert as
we move <em>across</em> time intervals, we will get unique insights into
how a <em>particular</em> system of presences evolves over time.</p>
<p>This is a powerful tool in causal reasoning: being able to analyze
<em>why</em> a system of presences behaves the way it does.
Specifically, it provides a powerful framework for structural causal
attribution when analyzing the changes in behavior of a system or
presences.</p>
<p>Specifically, if we think of these three parameters of a system as
defining the unique co-ordinates of the state of the system over a small
finite interval, we can “trace” the movement of the system by following
these co-ordinates. But since there are only two degrees of freedom
these coordinates will always lie on a two dimensional manifold<a
href="#fn8" class="footnote-ref" id="fnref8"
role="doc-noteref"><sup>8</sup></a>, in a 3 dimensional space.</p>
<p>If fact, remarkably, the state of <em>every</em> possible system of
presences, no matter how general, always has a trajectory in time that
lies on this <em>same</em> manifold. This is a powerful constraint and
insight that we can use to study the behavior of a system of presences
over time.</p>
<p>We will now introduce a tool called the presence matrix that makes it
easier to visualize and manage the computations involved in doing
so.</p>
<h2 id="the-presence-matrix">5. The Presence Matrix</h2>
<p>A <em>presence matrix</em> records the presence mass distribution
obtained by sampling a set of presence density functions over a fixed
set of time intervals. Specifically, if we fix a time granularity—such
as hours, days or weeks—we can construct a matrix in which:</p>
<ul>
<li><p><em>Rows</em> correspond to individual signals (e.g., for each
<span class="math inline">\((
e, b)\)</span> pair),</p></li>
<li><p><em>Columns</em> correspond to non-overlapping time intervals at
that fixed time granularity <em>that cover the time axis</em>,</p></li>
<li><p><em>Entries</em> contain the <em>presence mass</em>, i.e., the
integral of the density function over the corresponding interval:</p>
<p><span class="math display">\[ M_{(e,b),j} = \int_{t_j}^{t_{j+1}}
f_{(e,b)}(t) \, dt \]</span></p></li>
</ul>
<p>The resulting matrix provides a discrete, temporally-aligned
representation of this system of presences.</p>
<p>Since we are accumulating presence masses over an interval, the value
of presence mass in a matrix entry is always a a real number. Figure 8
shows the presence matrix for the system in Figure 7<a href="#fn9"
class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a>.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/presence_matrix.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 8: Presence Matrix for a system of presences. </code></pre>
</div>
</div>
<p>The presence matrix encodes some deep structural properties of a
system of presences. Many of key concepts we want to highlight are
easier to define and understand in terms of this representation.</p>
<p>The Presence Calculus Toolkit documentation has more details on the
mechanics of its construction and the computations it enables. For now,
these details are not critical, and we will focus on the insights it
surfaces.</p>
<h3 id="the-presence-invariant-and-the-presence-matrix.">The presence
invariant and the presence matrix.</h3>
<p>Let’s revisit Figure 3, reproduced below, which introduced the idea
of thinking of presence mass as a sample of an underlying signal.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/multiple_support.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 3: A presence as a sample of a signal over an interval.</code></pre>
</div>
</div>
<p>When we observe a system of presences across a finite observation
window, as we do in deriving the presence invariant, we are looking at
presence mass across a “vertical” slice of time across all signals.</p>
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
E
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
e1_b1
</td>
<td style="background-color: #c6f6c6">
0.3
</td>
<td style="background-color: #c6f6c6">
2.3
</td>
<td style="background-color: #c6f6c6">
3.4
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td style="background-color: #c6f6c6">
2.9
</td>
<td style="background-color: #c6f6c6">
3.2
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<td>
e1_b2
</td>
<td style="background-color: #c6f6c6">
0.3
</td>
<td style="background-color: #c6f6c6">
2.3
</td>
<td style="background-color: #c6f6c6">
3.4
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td>
0.0
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td style="background-color: #c6f6c6">
2.2
</td>
<td style="background-color: #c6f6c6">
2.4
</td>
<td style="background-color: #c6f6c6">
2.3
</td>
<td style="background-color: #c6f6c6">
0.8
</td>
</tr>
<tr>
<td>
e2_b2
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td style="background-color: #c6f6c6">
0.9
</td>
<td style="background-color: #c6f6c6">
1.8
</td>
<td style="background-color: #c6f6c6">
3.2
</td>
<td style="background-color: #c6f6c6">
0.9
</td>
</tr>
<tr>
<td>
e2_b1
</td>
<td style="background-color: #c6f6c6">
0.8
</td>
<td style="background-color: #c6f6c6">
1.3
</td>
<td style="background-color: #c6f6c6">
2.4
</td>
<td style="background-color: #c6f6c6">
2.8
</td>
<td style="background-color: #c6f6c6">
3.0
</td>
<td style="background-color: #c6f6c6">
3.2
</td>
<td style="background-color: #c6f6c6">
3.4
</td>
<td style="background-color: #c6f6c6">
2.4
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 9: The presence matrix from Figure 8, reproduced</code></pre>
</div>
</div>
<p>The presence matrix, shown in figure 9, makes this notion explict -
assuming some fixed granularity of observation window, a vertical slice
in time corresponds to a column in the presence matrix.</p>
<p>Looking at the sums of the presence masses along the rows of the
matrix gives us the mass contributions per signal (or presence) and the
row sums give us cumulative presence mass per signal across all
presences.</p>
<p>Given a sequence of columns of the presence matrix, the signal
incidences are simply the number of rows that have non-zero values those
rows. It’s straightforward now to interpret the presence invariant in
terms of the presence matrix.</p>
<p>Further, you can see that we can construct a presence matrix from any
subset of the rows of another presence matrix, and it would still be a
presence matrix, for which the presence invariant applies. What is more,
the presence invariant applies for any <em>consecutive</em> sequence of
columns of the presence matrix.</p>
<p>Thus the presence invariant encodes very strong <em>local</em>
constraints in how presence mass distributes in time across signals, and
we can use these to derive meaningful constraints on the global
accumulation of presence mass across a system of presences.</p>
<h3 id="the-presence-accumulation-matrix">The Presence Accumulation
Matrix</h3>
<p>To visualize and reason about both the micro and macro behaviors of a
system of presences in a natural way, we will need the help of a data
structure called the Presence Accumulation matrix. It compresses a lot
of information about the interaction between local and global behavior
of a system of presences across time scales and relies heavily on the
fact that the presence invariant is scale-independent.</p>
<p>We’ll continue with the presence matrix of Figure 9, to illustrate
how it is constructed.</p>
<p>Recall that the columns of the presence matrix represent time
intervals at the finest level of granularity at which an underlying
system of signals and presences is sampled. If we have an <span
class="math inline">\(MxN\)</span> of <span
class="math inline">\(M\)</span> signal sampled at <span
class="math inline">\(N\)</span> consecutive intervals, the value in the
presence matrix at row <span class="math inline">\(i\)</span> and column
<span class="math inline">\(j\)</span> represents the sampled presence
mass of signal <span class="math inline">\(i\)</span> at time interval
<span class="math inline">\(j\)</span>.</p>
<p>Consider the matrix <span class="math inline">\(A\)</span> that we
accumulates presences masses across consecutive time intervals of
various length between 1 and <span class="math inline">\(N\)</span>.
This is a square matrix of size <span class="math inline">\(NxN\)</span>
and is constructed as follows. For our example, this is a 10x10
matrix.</p>
<p>The diagonal of the matrix contains the accumulated presence mass
across each interval of length 1. This corresponds to the sum of each
column of the presence matrix.</p>
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td style="background-color:#e6ffe6">
1.4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
9.2
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.0
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.6
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
6.6
</td>
<td>
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.5
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
1.7
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 10: Cumulative presence mass along intervals of length 1. </code></pre>
</div>
</div>
<p>The next diagonal row contains the cumulative presence mass for
intervals of length 2 - ie <span class="math inline">\(A(1,2)\)</span>
contains the sum of all entries in columns 1 and 2. <span
class="math inline">\(A(2,3)\)</span> contains the sum of all entries
column 2 and 3 and so on..</p>
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td style="background-color:#e6ffe6">
1.4
</td>
<td style="background-color:#e6f0ff">
7.3
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td style="background-color:#e6f0ff">
15.1
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
9.2
</td>
<td style="background-color:#e6f0ff">
14.2
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.0
</td>
<td style="background-color:#e6f0ff">
10.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td style="background-color:#e6f0ff">
13.4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.5
</td>
<td style="background-color:#e6f0ff">
15.1
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.6
</td>
<td style="background-color:#e6f0ff">
14.2
</td>
<td>
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
6.6
</td>
<td style="background-color:#e6f0ff">
12.1
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.5
</td>
<td style="background-color:#e6f0ff">
7.2
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
1.7
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 11: Cumulative presence mass along intervals of length 2. </code></pre>
</div>
</div>
We can continue filling the matrix out in diagonal order this way until
we get the presence accumulation matrix shown below.
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<!-- Row 1 -->
<tr>
<td>
1
</td>
<td style="background-color:#e6ffe6">
1.4
</td>
<td style="background-color:#e6f0ff">
7.3
</td>
<td style="background-color:#e6ffe6">
16.5
</td>
<td style="background-color:#e6f0ff">
21.5
</td>
<td style="background-color:#e6ffe6">
27.4
</td>
<td style="background-color:#e6f0ff">
34.9
</td>
<td style="background-color:#e6ffe6">
42.5
</td>
<td style="background-color:#e6f0ff">
49.1
</td>
<td style="background-color:#e6ffe6">
54.6
</td>
<td style="background-color:#e6f0ff">
56.3
</td>
</tr>
<!-- Row 2 -->
<tr>
<td>
2
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td style="background-color:#e6f0ff">
15.1
</td>
<td style="background-color:#e6ffe6">
20.1
</td>
<td style="background-color:#e6f0ff">
26.0
</td>
<td style="background-color:#e6ffe6">
33.5
</td>
<td style="background-color:#e6f0ff">
41.1
</td>
<td style="background-color:#e6ffe6">
47.7
</td>
<td style="background-color:#e6f0ff">
53.2
</td>
<td style="background-color:#e6ffe6">
54.9
</td>
</tr>
<!-- Row 3 -->
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
9.2
</td>
<td style="background-color:#e6f0ff">
14.2
</td>
<td style="background-color:#e6ffe6">
20.1
</td>
<td style="background-color:#e6f0ff">
27.6
</td>
<td style="background-color:#e6ffe6">
35.2
</td>
<td style="background-color:#e6f0ff">
41.8
</td>
<td style="background-color:#e6ffe6">
47.3
</td>
<td style="background-color:#e6f0ff">
49.0
</td>
</tr>
<!-- Row 4 -->
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.0
</td>
<td style="background-color:#e6f0ff">
10.9
</td>
<td style="background-color:#e6ffe6">
18.4
</td>
<td style="background-color:#e6f0ff">
26.0
</td>
<td style="background-color:#e6ffe6">
32.6
</td>
<td style="background-color:#e6f0ff">
38.1
</td>
<td style="background-color:#e6ffe6">
39.8
</td>
</tr>
<!-- Row 5 -->
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td style="background-color:#e6f0ff">
13.4
</td>
<td style="background-color:#e6ffe6">
21.0
</td>
<td style="background-color:#e6f0ff">
27.6
</td>
<td style="background-color:#e6ffe6">
33.1
</td>
<td style="background-color:#e6f0ff">
34.8
</td>
</tr>
<!-- Row 6 -->
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.5
</td>
<td style="background-color:#e6f0ff">
15.1
</td>
<td style="background-color:#e6ffe6">
21.7
</td>
<td style="background-color:#e6f0ff">
27.2
</td>
<td style="background-color:#e6ffe6">
28.9
</td>
</tr>
<!-- Row 7 -->
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.6
</td>
<td style="background-color:#e6f0ff">
14.2
</td>
<td style="background-color:#e6ffe6">
19.7
</td>
<td style="background-color:#e6f0ff">
21.4
</td>
</tr>
<!-- Row 8 -->
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
6.6
</td>
<td style="background-color:#e6f0ff">
12.1
</td>
<td style="background-color:#e6ffe6">
13.8
</td>
</tr>
<!-- Row 9 -->
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.5
</td>
<td style="background-color:#e6f0ff">
7.2
</td>
</tr>
<!-- Row 10 -->
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
1.7
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 12: Final presence accumulation matrix for the presence matrix of Figure 9. </code></pre>
</div>
</div>
<p>We can see that in practice, this matrix compresses a large amount of
information in a very compact form. For example if the columns represent
weekly samples of signals a 52x52 matrix allows us to analyze a whole
years worth of presence accumulation across every time-scale ranging
from a single week to a whole year in one compact structure. We’ll
shortly see why this is useful.</p>
<p>First lets define the general structure of the presence accumulation
matrix.</p>
<p>Let <span class="math inline">\(P \in \mathbb{R}^{M \times
N}\)</span> be the presence matrix of <span
class="math inline">\(M\)</span> signals sampled at <span
class="math inline">\(N\)</span> consecutive time intervals. The
<strong>Presence Accumulation Matrix</strong> <span
class="math inline">\(A \in \mathbb{R}^{N \times N}\)</span> is defined
by:</p>
<p><span class="math display">\[
A(i, j) = \sum_{k=1}^{M} \sum_{\ell=i}^{j} P(k, \ell)
\quad \text{for all } 1 \leq i \leq j \leq N
\]</span></p>
<p>That is, <span class="math inline">\(A(i,j)\)</span> gives the total
presence mass of all signals across the interval <span
class="math inline">\([i,j]\)</span>.</p>
<h3 id="properties">Properties</h3>
<ul>
<li><span class="math inline">\(A\)</span> is upper triangular: <span
class="math inline">\(A(i,j)\)</span> is defined only when <span
class="math inline">\(i \leq j\)</span>.</li>
<li>The diagonal entries <span class="math inline">\(A(i,i)\)</span>
equal the column sums of <span class="math inline">\(P\)</span>.</li>
<li>Each entry <span class="math inline">\(A(i,j)\)</span> reflects the
cumulative presence mass over the interval <span
class="math inline">\([i,j]\)</span>.</li>
</ul>
<p>As we will see below, this matrix compactly encodes multi-scale
information about system behavior and supports the analysis of both
micro and macro scale behavior of a system of presences.</p>
<h2 id="applying-the-presence-calculus">6. Applying the Presence
Calculus</h2>
<p>The presence calculus might seem like a highly abstract, theoretical
framework, but much of its utility emerges when we <em>interpret</em>
its concepts in a specific, applied context.</p>
<p>Concepts such as presence mass, incidence rates, and density are not
unlike abstract physical notions like force, mass, and acceleration. In
principle, these are measurable quantities that nature constrains to
behave in prescribed ways at a micro scale.</p>
<p>Once we understand the rules governing their micro-scale behavior, we
gain tools to systematically measure, reason about, and explain a vast
range of observable macro-scale phenomena. Much of physics is built on
this principle.</p>
<p>In a similar vein, the presence calculus—and especially the
<em>presence invariant</em> —provides a foundational law that governs
the local, time-based behavior of any system composed of time-varying
signals and the presences they induce.</p>
<p>Once we recognize that such a governing constraint exists, the
presence calculus equips us with tools to describe, interpret, explain,
and, in certain cases,<br />
make verifiable predictions about the macro-scale behavior of these
systems.</p>
<p>Newtonian mechanics, for example, allows us to describe and predict
the motion of physical systems with remarkable precision—such as
planetary orbits or the paths of falling objects. Yet even within this
well-established framework, certain limits remain: the general
three-body problem has no closed-form solution, and systems like the
double pendulum exhibit chaotic behavior that defies long-term
prediction.</p>
<p>Still, we can represent the behavior and evolutions of such systems
as deterministic trajectories through a parameter space, uncovering
structure even where precise global behavior remain unpredictable. In
much the same way, the presence calculus does not seek to forecast the
exact evolution of complex systems. Instead, by explicitly modeling
signal histories and representing element trajectories over time, it
equips us with powerful descriptive and explanatory tools.</p>
<p>In this way, structural constraints and local invariants help us
interpret locally observed dynamics and connect it to system behavior at
the macro scale.</p>
<p>Let’s see how.</p>
<h3 id="sample-paths-and-convergence">Sample Paths and Convergence</h3>
Consider the highlighted portions of the accumulation matrix <span
class="math inline">\(A\)</span> in figure 13.
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td style="background-color:#e6ffe6">
1.4
</td>
<td style="background-color:#ffffcc">
7.3
</td>
<td style="background-color:#ffffcc">
16.5
</td>
<td style="background-color:#ffffcc">
21.5
</td>
<td style="background-color:#ffffcc">
27.4
</td>
<td style="background-color:#ffffcc">
34.9
</td>
<td style="background-color:#ffffcc">
42.5
</td>
<td style="background-color:#ffffcc">
49.1
</td>
<td style="background-color:#ffffcc">
54.6
</td>
<td style="background-color:#ffffcc">
56.3
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
9.2
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.0
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.6
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
6.6
</td>
<td>
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.5
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
1.7
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 13: Diagonal and top row of the accumulation matrix</code></pre>
</div>
</div>
<p>Each diagonal entry represents the total presence mass observed
across all signals in a single time interval. Thus, the diagonal traces
the discrete-time evolution of the system’s observable activity—one step
at a time. We will call the sequence of values on the diagonal a
<em>sample path</em> for the system of presences.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/diagonal_values.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 14: The values on the diagonal of the accumulation matrix.  </code></pre>
</div>
</div>
<p>Recall that each entry on the diagonal represents the sum of the
presence masses of the signals active in a time window. This implies
that each entry on the top row is an approximation of an integral<a
href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a> and equals the <em>area under the
sample path</em> represented by the diagonal.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/top_row_values.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 15: The values on the top row accumulation matrix.  </code></pre>
</div>
</div>
<p>In other words, the diagonal and top row represent fundamentally
different views of the system: the diagonal captures the <em>micro-level
behavior</em>—instantaneous<br />
presence mass across signals in each interval—while the top row encodes
the<br />
<em>macro-level behavior</em>—the cumulative effect of those presences
over time.<br />
Both views are compactly encoded in the structure of the<br />
accumulation matrix.</p>
<p>Figure 16 shows this geometric interpretation of the diagonal and top
rows of the accumulation matrix.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/sample_path_area.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 16: Sample path and the area under it.   </code></pre>
</div>
</div>
<p>If we divide each of the entries in the accumulation matrix by the
length of the time interval it covers, we get the presence density for
each interval. For the diagonal interval has length 1 (time unit) and
for the top row the lengths range from 1 to <span
class="math inline">\(N-1.\)</span></p>
<div style="text-align: center; margin:2em">
<table style="border-collapse: collapse;">
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td style="background-color:#e6ffe6">
1.4
</td>
<td style="background-color:#ffffcc">
3.6
</td>
<td style="background-color:#ffffcc">
5.5
</td>
<td style="background-color:#ffffcc">
5.4
</td>
<td style="background-color:#ffffcc">
5.5
</td>
<td style="background-color:#ffffcc">
5.8
</td>
<td style="background-color:#ffffcc">
6.1
</td>
<td style="background-color:#ffffcc">
6.1
</td>
<td style="background-color:#ffffcc">
6.1
</td>
<td style="background-color:#ffffcc">
5.6
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
9.2
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.0
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.6
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
6.6
</td>
<td>
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.5
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
1.7
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 17: Presence density: diagonal and top row.   </code></pre>
</div>
</div>
<p>So now we have the left-hand side of the presence invariant encoded
in matrix<br />
form for every pair of continuous intervals in the system.</p>
<p>Let’s chart the values in both of these rows in the matrix.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/first_row_vs_main_diagonal.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 18: Convergence of long-run average presence density.</code></pre>
</div>
</div>
<p>We can see that while the values on the sample path are volatile, the
values on the top row converge towards a finite value. We can define
this notion of convergence precisely using the mathematical concept of a
limit. Let:</p>
<p><span class="math display">\[
\Delta = \lim_{T \to \infty} \frac{1}{T} \int_0^T \delta(t) \, dt
\]</span></p>
<p>Here, <span class="math inline">\(\delta(t)\)</span> is the
instantaneous presence density introduced in the presence
invariant.<br />
The quantity <span class="math inline">\(\Delta\)</span>, by contrast,
represents the long-run average presence density —<br />
the time-averaged total presence across all signals in the system.</p>
<p>In the discrete setting, this corresponds to the limiting value of
the top row of the<br />
accumulation matrix — a value toward which the green line in Figure 18
appears to converge:</p>
<p><span class="math display">\[
\Delta = \lim_{j \to \infty} \frac{A(1,j)}{j}
\]</span></p>
<p>where <span class="math inline">\(A(1,j)\)</span> is the total
presence mass accumulated from time 0 through interval <span
class="math inline">\(j\)</span>.</p>
<p>Not every system of presences has such a limit. We call a system
<em>convergent</em> if<br />
<span class="math inline">\(\Delta\)</span> exists and is finite, and
<em>divergent</em> otherwise.</p>
<p>To summarize: in a convergent system, the average presence density
over time<br />
settles toward a finite value. Intuitively, this means that after
observing<br />
enough of the system’s history, additional observation does not
significantly<br />
alter our understanding of its long-term behavior.</p>
<p>Some systems converge rapidly to a single stable limit. Others may
not settle<br />
at all, but instead move among a small number of such limits.
These<br />
represent dominant behavioral modes — quasi-equilibrium states that the
system<br />
can enter and sustain for extended periods. Such behavior is
called<br />
<em>metastable</em> or <em>multi-modal</em>.</p>
<p>Divergence, by contrast, implies the absence of such limiting
behavior. The<br />
presence density in these systems continues to grow without bound,<br />
indicating that no stable long-term pattern emerges.</p>
<p>Figure 19 shows examples of each of these behaviors.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/convergence_divergence.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 19: Convergent, Divergent, and Metastable behavior in systems of presences.</code></pre>
</div>
</div>
<p>If a limit <span class="math inline">\(\Delta\)</span> exists and is
sustained over time, it signifies a stable long-run average presence
density for the system. This value represents a specific pattern of
average behavior toward which the system’s observable presence density
gravitates over extended periods, regardless of short-term
fluctuations.</p>
<p>This concept aligns with the broader notion of attractors in
dynamical systems. While a system’s full, high-dimensional state might
exhibit complex dynamics, the long-run average presence density can
itself stabilize around a particular value or set of values. When the
presence density consistently settles around such a limit, it indicates
that the system’s observable behavior has entered a sustainable
regime.</p>
<p>This provides a powerful way to characterize the system’s overall
operational modes in the long term, even if its micro-level details
remain complex and unpredictable. We will have more to say on this topic
shortly.</p>
<p>It is important to emphasize that convergence and divergence are
properties<br />
of the <em>observed long-run behavior</em> of a system of presences —
not intrinsic<br />
properties of the underlying system itself.</p>
<p>We cannot infer the system’s nature solely from whether it appears
convergent or divergent at any given time. We can only observe the
dynamics of presence<br />
over time and assess whether they exhibit convergence.</p>
<p>The key difference between convergence and the other two modes is
that a<br />
convergent system can effectively <em>forget</em> its history beyond the
point of stabilization.Its future behavior becomes representative of its
past, allowing the system to be characterized by a stable long-run
average.</p>
<p>Among other things, the presence calculus equips us with the
computational tools needed to identify convergent, divergent and
metastable states of a system of presences.</p>
<h4 id="the-semantics-of-convergence">The Semantics of Convergence</h4>
<p>An important point to emphasize is that, depending on how presence
density<br />
is interpreted in a given domain, <em>any</em> of the three behavioral
modes —<br />
convergent, divergent, or metastable — may be desirable.<br />
Convergence is not inherently “good,” nor is divergence necessarily
“bad.”</p>
<p>For example, in repetitive manufacturing or many customer service
domains,<br />
convergence is often desired. In these contexts, presence
typically<br />
represents <em>demand</em> on a constrained resource, and keeping that
demand<br />
stable is essential for ensuring consistent service times,
throughput,<br />
and resource utilization. Traditional operations management and queueing
theory therefore seek out — and emphasize — stability and convergence in
key<br />
operational signals.</p>
<p>By contrast, if presence represents a company’s <em>customer
base</em> or <em>market<br />
share</em>, we <em>want</em> the long-run presence density to look like
the chart on<br />
the right: up and to the right — that is, <em>divergent</em>.</p>
<p>Metastable modes are common and often highly desirable in
software<br />
development, where teams must shift between modes of operation in<br />
response to external demands or changing market conditions. Indeed,
the<br />
ability to transition between such modes effectively is often a hallmark
of a high-functioning, adaptive organization — <em>provided</em> it is
done intentionally and with awareness.</p>
<p>All too often, however, organizations drift from metastable to
chaotic<br />
behavior, losing the capacity to stabilize in any mode. This often
results<br />
from a lack of visibility into — or the inability to reason about —
their<br />
operational state.</p>
<p>One of the major practical applications of the presence calculus is
to<br />
bring new analytical tools to <em>observe</em>, <em>categorize</em>, and
<em>steer</em> the<br />
behavior of such complex systems — aligning them with the desired modes
of<br />
operation in a given domain, <em>before</em> critical tipping points are
reached.</p>
<h3 id="detecting-convergence">Detecting Convergence</h3>
<p>In the last section, we <em>defined</em> convergent behavior in terms
of the<br />
existence of the limit <span class="math inline">\(\Delta\)</span>, the
long-run average presence density<br />
of the system.</p>
<p>Now we ask: under what observable conditions does such a limit
exist?</p>
<p>If we can identify these conditions, we gain levers to begin
<em>steering</em><br />
systems toward desired modes of operation.</p>
<p>It turns out the answer is hiding in plain sight — in the
presence<br />
invariant, which, as we’ve seen, holds for <em>any</em> finite
observation<br />
window. The limit <span class="math inline">\(\Delta\)</span> represents
the asymptotic average of <span
class="math inline">\(\delta(t)\)</span>,<br />
the left-hand side of the invariant, measured over a sequence of
consecutive<br />
overlapping intervals, each one a prefix of the sample path.</p>
<p>For each such prefix interval <span class="math inline">\(t\)</span>
, the <em>presence invariant</em> gives us:</p>
<p><span class="math display">\[
\delta(t) = \iota(t) \times \bar{m}(t)
\]</span></p>
<p>This tells us that each value of <span
class="math inline">\(\delta(t)\)</span> is determined by the<br />
product of two measurable quantities: <span
class="math inline">\(\iota(t)\)</span>, the incidence rate,<br />
and <span class="math inline">\(\bar{m}(t)\)</span>, the average mass
contribution per signal.</p>
<p>To understand when the long-run average of <span
class="math inline">\(\delta(t)\)</span> converges, we can ask<br />
a simpler question: do the corresponding long-run averages of <span
class="math inline">\(\iota(t)\)</span><br />
and <span class="math inline">\(\bar{m}(t)\)</span> converge? If both
do, we should expect that their product —<br />
and hence <span class="math inline">\(\Delta\)</span> — converges as
well,and it does, with some technical conditions in place<a href="#fn11"
class="footnote-ref" id="fnref11"
role="doc-noteref"><sup>11</sup></a>.</p>
<p>So, let’s write down precise definitions for the limits of <span
class="math inline">\(\iota(t)\)</span> and<br />
<span class="math inline">\(\bar{m}(t)\)</span> and examine how these
limits behave.</p>
<h4 id="convergence-of-average-signal-mass-contribution">Convergence of
Average Signal Mass Contribution</h4>
<p>We will derive the limit for <span
class="math inline">\(\bar{m}\)</span>. We will denote this by <span
class="math inline">\(\bar{M}\)</span>.</p>
<p><span class="math display">\[
\bar{M} = \lim_{T \to \infty} \frac{1}{N(0,T)} \sum_{(e,b)} \int_0^T
P_{(e,b)}(t) \, dt
\]</span></p>
<p>This expression means: for each signal <span
class="math inline">\((e,b)\)</span>, accumulate its total presence mass
over time, then sum across all signals, and divide by the total number
of signals active during that window. Each integral in the sum is a row
sum in the original presence matrix - the mass contribution of an
individual signal over the interval.</p>
<p>Thus we can also write this as</p>
<p><span class="math display">\[
\bar{M} = \lim_{j \to \infty} \frac{1}{N(1,j)} \sum_{(e,b)} \sum_{k=1}^j
P_{(e,b)}(k)
\]</span></p>
<p><span class="math inline">\(\bar{M}\)</span> is the limit of average
mass contribution per signal over a sufficiently long observation
interval. Here, <span class="math inline">\(P_{(e,b)}(t)\)</span> is the
presence density function for signal <span
class="math inline">\((e,b)\)</span>, and <span
class="math inline">\(N(0,T)\)</span> is the total number of signals
observed during the interval <span
class="math inline">\([0,T]\)</span>.</p>
<p>Let’s work this out for our running example and see what it means.
We’ll reproduce Figure 9, our starting presence matrix, here for easy
reference.</p>
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
E
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
e1_b1
</td>
<td style="background-color: #c6f6c6">
0.3
</td>
<td style="background-color: #c6f6c6">
2.3
</td>
<td style="background-color: #c6f6c6">
3.4
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td style="background-color: #c6f6c6">
2.9
</td>
<td style="background-color: #c6f6c6">
3.2
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<td>
e1_b2
</td>
<td style="background-color: #c6f6c6">
0.3
</td>
<td style="background-color: #c6f6c6">
2.3
</td>
<td style="background-color: #c6f6c6">
3.4
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td>
0.0
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td style="background-color: #c6f6c6">
2.2
</td>
<td style="background-color: #c6f6c6">
2.4
</td>
<td style="background-color: #c6f6c6">
2.3
</td>
<td style="background-color: #c6f6c6">
0.8
</td>
</tr>
<tr>
<td>
e2_b2
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td style="background-color: #c6f6c6">
0.9
</td>
<td style="background-color: #c6f6c6">
1.8
</td>
<td style="background-color: #c6f6c6">
3.2
</td>
<td style="background-color: #c6f6c6">
0.9
</td>
</tr>
<tr>
<td>
e2_b1
</td>
<td style="background-color: #c6f6c6">
0.8
</td>
<td style="background-color: #c6f6c6">
1.3
</td>
<td style="background-color: #c6f6c6">
2.4
</td>
<td style="background-color: #c6f6c6">
2.8
</td>
<td style="background-color: #c6f6c6">
3.0
</td>
<td style="background-color: #c6f6c6">
3.2
</td>
<td style="background-color: #c6f6c6">
3.4
</td>
<td style="background-color: #c6f6c6">
2.4
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 9: The presence matrix from Figure 8, reproduced</code></pre>
</div>
</div>
<p>The cumulative mass per signal over each interval <span
class="math inline">\([1, j], j \le 10\)</span> is shown below. Each
value in this matrix is the sum of all the values in that row to the
left (inclusive) of the value.</p>
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
E
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
e1_b1
</td>
<td>
0.3
</td>
<td>
2.6
</td>
<td>
6.0
</td>
<td>
7.1
</td>
<td>
10.0
</td>
<td>
13.2
</td>
<td>
14.3
</td>
<td>
14.3
</td>
<td>
14.3
</td>
<td>
14.3
</td>
</tr>
<tr>
<td>
e1_b2
</td>
<td>
0.3
</td>
<td>
2.6
</td>
<td>
6.0
</td>
<td>
7.1
</td>
<td>
7.1
</td>
<td>
8.2
</td>
<td>
10.4
</td>
<td>
12.8
</td>
<td>
15.1
</td>
<td>
15.9
</td>
</tr>
<tr>
<td>
e2_b2
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.9
</td>
<td>
2.7
</td>
<td>
5.9
</td>
<td>
6.8
</td>
</tr>
<tr>
<td>
e2_b1
</td>
<td>
0.8
</td>
<td>
2.1
</td>
<td>
4.5
</td>
<td>
7.3
</td>
<td>
10.3
</td>
<td>
13.5
</td>
<td>
16.9
</td>
<td>
19.3
</td>
<td>
19.3
</td>
<td>
19.3
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 20: The cumulative mass contribution matrix for the presence matrix.</code></pre>
</div>
</div>
<p>Now lets chart each row of this matrix to see how this cumulative
mass grows over time. Here we are showing each row in the matrix as a
line in the chart.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/mass_contribution_per_signal.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 21: Mass contribution per signal</code></pre>
</div>
</div>
<p>Finally figure 22 shows the cumulative average of the mass
contribution. Each point in this chart represents the cumulative average
of the mass contributions for the window <span
class="math inline">\([1,j]\)</span> which is the sum of the value in
column j divided by the number of non-zero rows in the sub-matrix
spanned by the columns in $[1,j]</p>
<p>As we can see, this curve converges to a limit.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/avg_mass_contribution_per_signal.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 22: Average Mass contribution per signal</code></pre>
</div>
</div>
<p>So lets ask, what would make the average mass contribution
<em>not</em> converge to a finite value?</p>
<p>Figure 21 suggests that the mass contribution of every individual
signal is monotonically non-decreasing and it increases continuously
over every non-zero support interval of the signal and flattens out over
every interval where the underlying signal is zero.</p>
<p>Suppose when measured over a sufficient long interval, each signal
remains bounded, that is every onset is matched with a corresponding
reset, then each individual signal contributes a finite mass to the
cumulative average.</p>
<p>Thus, the only way the cumulative average mass can grow without limit
is if <em>some</em> signal grows without limit.</p>
<p>For example, in figure 3 we show some onset-reset patterns for
signals and the last signal in Figure 3, which has an onset but no
apparent reset within the observation window, would grow without limit
in Figure 21 if there was no reset.</p>
<p>This gives us the first condition for convergence of <span
class="math inline">\(\Delta\)</span> :</p>
<div
style="border: 1px solid #ccc; border-radius: 6px; padding: 1em; background-color: #f9f9f9; margin: 2em 0;">
<p><strong>Boundedness of Signal Mass</strong></p>
<p><em>In a convergent system of presences, every signal onset is
eventually followed by a corresponding reset, when observed over a
sufficiently long time interval.</em></p>
</div>
<p>We’ll note, once again, that depending upon the semantics of the
domain, we may or may not want to have this condition hold depending
upon whether we are looking to steer the system towards convergence or
towards divergence.</p>
<p>For example, if a signal represents a new revenue source, a mass
contribution represents incremental revenues and ideally we want many
onsets without matching resets: every reset corresponds to a lost
revenue stream.</p>
<p>If, on the other hand, a signal onset represents a new unfinished
task on your to-do list, then a reset marks its completion — and
convergence becomes desirable, as it indicates tasks are being completed
in a timely manner and that your todo list is not growing without
limit.</p>
<h4 id="convergence-of-incidence-rate">Convergence of Incidence
Rate</h4>
<p>We now define the long-run incidence rate, denoted <span
class="math inline">\(I\)</span>, in exact analogy to how we defined the
average mass contribution per signal <span
class="math inline">\(\bar{M}\)</span>. Recall that <span
class="math inline">\(\iota(t)\)</span> is the incidence rate observed
over the interval <span class="math inline">\([0, t]\)</span>, defined
as the number of signals observed in that interval divided by its
duration. Then we define the long-run incidence rate as the following
limit:</p>
<p><span class="math display">\[
I = \lim_{T \to \infty} \iota(T) = \lim_{T \to \infty} \frac{N(0, T)}{T}
\]</span></p>
<p>where <span class="math inline">\(N(0, T)\)</span> is the number of
signals observed over the interval <span class="math inline">\([0,
T]\)</span>— that is, the number of distinct element-boundary signals
that are active at some point during the interval. The incidence rate
measures how many such signals are activated, on average, per unit
time.</p>
<p>This limit <span class="math inline">\(I\)</span>, when it exists,
represents the asymptotic rate at which signals appear in the system. It
plays a symmetric role to <span class="math inline">\(\bar{M}\)</span>
in the convergence of the presence density, and its existence is the
second key condition we will examine next.</p>
<p>To better understand the behavior of the incidence rate <span
class="math inline">\(\iota(T)\)</span>, let’s now examine the
cumulative signal count <span class="math inline">\(N(0,T)\)</span> over
the interval <span class="math inline">\([0,T]\)</span> for increasing
values of <span class="math inline">\(T\)</span>. This is directly
analogous to how we analyzed the growth of cumulative mass contributions
per signal when analyzing <span
class="math inline">\(\bar{M}\)</span>.</p>
<p>Recall that for a given observation window <span
class="math inline">\([0,T]\)</span>, <span
class="math inline">\(N(0,T)\)</span> counts the number of
element-boundary signals that are active at some point within the
interval. We can compute this by scanning across the presence matrix
and, for each column (from <span class="math inline">\(t=1\)</span> to
<span class="math inline">\(t=T\)</span>), counting how many
<em>new</em> signals appear—that is, how many unique element-boundary
rows have non-zero values in that column.</p>
<p>The result is a sequence of signal counts, which we can arrange as a
<span class="math inline">\(1 \times T\)</span> row vector:</p>
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
Time
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Signal Count
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 23: Cumulative count of distinct signals observed over the interval $[0, T]$.</code></pre>
</div>
</div>
<p>where <span class="math inline">\(n_j\)</span> is the total number of
distinct signals that have appeared at or before time <span
class="math inline">\(j\)</span>. Each <span
class="math inline">\(n_j\)</span> counts the number of signals with
support intersecting the interval <span class="math inline">\([0,
j]\)</span>.</p>
<p>We can chart this row to visualize how the cumulative number of
observed signals grows over time. If <span
class="math inline">\(N(0,T)\)</span> grows linearly in <span
class="math inline">\(T\)</span>, then the incidence rate <span
class="math inline">\(\iota(T) = N(0,T)/T\)</span> should converge to a
finite value <span class="math inline">\(I\)</span>. On the other hand,
if <span class="math inline">\(N(0,T)\)</span> grows faster than
linearly, the incidence rate will diverge—and if it grows sublinearly,
the rate will decay toward zero.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/avg_incidence_rate.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 24: Average signal incidence rate </code></pre>
</div>
</div>
<p>Figure 24 shows the incidence rate <span
class="math inline">\(\iota(T) = N(0,T)/T\)</span> over time. In this
example, the rate initially decreases and then stabilizes, since the
number of distinct signals <span class="math inline">\(N(0,T)\)</span>
stops increasing after a point. In general, the incidence rate will
converge to a finite limit if <span
class="math inline">\(N(0,T)\)</span> grows no faster than linearly with
<span class="math inline">\(T\)</span>. If <span
class="math inline">\(N(0,T)\)</span> grows <em>faster</em> than <span
class="math inline">\(T\)</span>, the ratio <span
class="math inline">\(\iota(T)\)</span> will diverge—indicating that
signals are being activated at an unbounded rate. Conversely, if <span
class="math inline">\(N(0,T)\)</span> grows <em>slower</em> than <span
class="math inline">\(T\)</span>, the incidence rate will decay toward
zero. Thus, convergence of <span class="math inline">\(\iota(T)\)</span>
requires that <span class="math inline">\(N(0,T)\)</span> grows
approximately linearly in <span class="math inline">\(T\)</span>.</p>
<p>This kind of divergence typically arises in systems where the
<em>onset rate</em>— the rate at which new signals are activated—exceeds
the <em>reset rate</em>, which closes those signals. While transient
imbalances between onsets and resets are common during transitions
between equilibrium states, divergence only occurs if this imbalance is
sustained indefinitely. In that case, <span
class="math inline">\(N(0,T)\)</span> grows without bound relative to
<span class="math inline">\(T\)</span>, and the system exhibits an
asymptotically increasing incidence rate. So, divergence of <span
class="math inline">\(\iota(T)\)</span> directly reflects a persistent
structural imbalance between signal onsets and resets over time.</p>
<div
style="border: 1px solid #ccc; border-radius: 6px; padding: 1em; background-color: #f9f9f9; margin: 2em 0;">
<p><b>Boundedness of incidence rate</b></p>
<p><i> In a convergent system of presences, the long-run rate of signal
onsets does not exceed the long-run rate of resets, when observed over a
sufficiently long time interval. </i></p>
</div>
<h4 id="recap">Recap</h4>
<p>We began by defining convergence in terms of the existence of a
long-run limit for average presence density.</p>
<p>We then showed how the existence of this global limit depends on the
existence of two other measurable limits: the average incidence rate of
signals and their average mass contribution.</p>
<p>Next, we traced each of these limits back to the local behavior of
individual signals—specifically, the presence or absence of well-behaved
signal onsets and resets.</p>
<p>With this connection in place, we now have a principled way to reason
about the global convergence or divergence of a system by analyzing the
patterns of local signal behavior over time. In the next section, we’ll
see how to apply this principle in practice.</p>
<h4 id="formal-proof-of-convergence-and-littles-law">Formal Proof of
Convergence and Little’s Law</h4>
<p>In this document, we have presented an accurate—though somewhat
simplified— account of the criteria required to ensure that a system of
presences is convergent. Specifically, based on the definitions above,
we assert that for a given system of presences, if the limits <span
class="math inline">\(I\)</span> and <span
class="math inline">\(\bar{M}\)</span> exist and are finite, then the
limit <span class="math inline">\(\Delta\)</span> also exists and is
finite. Furthermore, we claim that</p>
<p><span class="math display">\[
\Delta = I \times \bar{M}
\]</span></p>
<p>Technically, this relationship does not follow automatically from the
arguments we have presented so far. In fact, the statement above is a
restatement of a generalized form of Little’s Law originally proven by
Brumelle, and later by Heyman and Stidham. The full proof—along with the
additional technical conditions required to ensure that the limit of the
product equals the product of the limits—is beyond the scope of this
document.</p>
<p>For our purposes, it is safe to state that this relationship, and the
conditions under which it holds, <em>constitute</em> the general form of
Little’s Law for a system of presences.</p>
<p>With the exception of certain carefully constructed pathological
cases, the criteria we have outlined—bounded signal mass and balanced
onset/reset rates—will typically suffice to determine whether a system
is convergent or divergent.</p>
<h4 id="convergence-coherence-and-littles-law">Convergence, Coherence,
and Little’s Law</h4>
<p>One important point to note is that “Little’s Law” is not a single
law, but rather a family of related laws that apply at different time
scales, in different forms, and with different interpretations. The
presence invariant is the most general version of this law.</p>
<p>In this document, we stated it as a relationship between average
presence density, signal incidence rate, and average mass contribution
per signal over any finite observation window. This relationship holds
at all time scales, <em>including</em> those sufficiently long windows
where the limits <span class="math inline">\(\Delta\)</span>, <span
class="math inline">\(I\)</span>, and <span
class="math inline">\(\bar{M}\)</span> exist.</p>
<p>It is natural to ask: what is special, if anything, about those
limiting values?</p>
<p>Without going too deeply into technical arguments here, we note that
the limits are indeed special. When a system is observed over a
non-convergent interval, the quantities in the presence invariant are
dominated by <em>partial</em> mass contributions from signals that have
not yet completed. Figure 8 shows an example of this behavior. The
system may <em>appear</em> convergent when the window is long enough for
<em>complete</em> signals to dominate the averages. For example, if we
extended the window in Figure 8 to include the full duration of each
signal, the system would appear convergent over that interval.</p>
<p>The important point here is that in such situations, the presence
invariant is not just a relationship about mass <em>contributions</em>,
but also implicitly a relationship about the <em>masses</em> of the
signals involved. This distinction has direct operational
implications.</p>
<p>For instance, if the signals in Figure 8 represent customer service
times, then over a convergent interval, mass contributions reflect what
the customer experiences. But over shorter, non-convergent intervals,
the same contributions reflect only what a system operator sees day to
day. In this way, convergence can be seen as aligning two perspectives:
the customer’s and the operator’s. When their averages match, we achieve
a state of epistemic coherence—observers agree on what they are
measuring.</p>
<p>We will return to this idea in more detail later, particularly in the
context of flow measurement in systems that operate far from
equilibrium. But for now, it is enough to recognize that identifying
whether a system is operating in a convergent or divergent mode is
fundamental to making meaningful decisions when reasoning about a system
of presences.</p>
<h4 id="a-note-on-determinism">A Note on Determinism</h4>
<p>A final point we emphasize in this section is that the form of
Little’s Law derived here is entirely deterministic. It does not depend
on any probabilistic or stochastic assumptions about the behavior of the
system. In fact, the notion of a sample path used here originates in a
deterministic proof of Little’s Law by Stidham in 1972. The presence
invariant, as we have introduced it, is a direct analogue of the
finite-window constructs used in that proof.</p>
<p>It is important to recognize that Little’s Law is not a statistical
artifact. It is a structural property deeply woven into the behavior of
of <em>any</em> system of presences. Convergence and divergence are
deterministic features of how signals evolve and interact over
time—regardless of whether the underlying signals are random or not.</p>
<p>Even when the system is driven by randomness, the <em>observed</em>
evolution of presence density is deterministic and governed by the law
of conservation of presence mass—that is, the presence invariant. This
determinism extends to any functional quantity that depends on presence
density. As we will see, a large and operationally useful class of
system behaviors can be characterized in terms of the presence density
of domain signals. That is why the machinery developed here is more than
just a theoretical curiosity.</p>
<p>This is our main point of departure in the presence calculus: we
treat equilibrium not as a precondition for Little’s Law, but as a
special case of a more general principle. We place the finite version of
Little’s Law at the center of our analysis, because it continues to hold
and yield meaningful insight even when the system is far from
equilibrium—precisely where classical applications of Little’s Law begin
to break down.</p>
<h3 id="steering-the-system">Steering the System</h3>
<p>Convergence, as discussed in the last section, is a fundamental
concept in the presence calculus, and we now have</p>
<table style="border-collapse: collapse; margin: auto;">
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
0.13 ∠ -0.29
</td>
<td>
0.68 ∠ -0.3
</td>
<td>
0.9 ∠ -0.29
</td>
<td>
1.08 ∠ -0.31
</td>
<td>
1.15 ∠ -0.33
</td>
<td>
1.2 ∠ -0.38
</td>
<td>
1.22 ∠ -0.43
</td>
<td>
1.24 ∠ -0.47
</td>
<td>
1.26 ∠ -0.51
</td>
<td>
1.26 ∠ -0.55
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td>
0.86 ∠ -0.29
</td>
<td>
1.02 ∠ -0.29
</td>
<td>
1.17 ∠ -0.31
</td>
<td>
1.26 ∠ -0.34
</td>
<td>
1.29 ∠ -0.39
</td>
<td>
1.31 ∠ -0.44
</td>
<td>
1.32 ∠ -0.48
</td>
<td>
1.34 ∠ -0.52
</td>
<td>
1.34 ∠ -0.56
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td>
0.71 ∠ -0.23
</td>
<td>
0.94 ∠ -0.26
</td>
<td>
1.14 ∠ -0.3
</td>
<td>
1.2 ∠ -0.35
</td>
<td>
1.26 ∠ -0.41
</td>
<td>
1.31 ∠ -0.46
</td>
<td>
1.34 ∠ -0.5
</td>
<td>
1.36 ∠ -0.54
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.06 ∠ 0.0
</td>
<td>
0.42 ∠ 0.19
</td>
<td>
0.61 ∠ 0.18
</td>
<td>
0.8 ∠ 0.16
</td>
<td>
0.91 ∠ 0.14
</td>
<td>
0.97 ∠ 0.13
</td>
<td>
1.01 ∠ 0.12
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.1 ∠ 0.0
</td>
<td>
0.46 ∠ 0.21
</td>
<td>
0.69 ∠ 0.2
</td>
<td>
0.83 ∠ 0.18
</td>
<td>
0.91 ∠ 0.17
</td>
<td>
0.97 ∠ 0.16
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.19 ∠ 0.0
</td>
<td>
0.56 ∠ 0.23
</td>
<td>
0.75 ∠ 0.22
</td>
<td>
0.88 ∠ 0.21
</td>
<td>
0.97 ∠ 0.2
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.15 ∠ 0.0
</td>
<td>
0.51 ∠ 0.24
</td>
<td>
0.73 ∠ 0.23
</td>
<td>
0.89 ∠ 0.22
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.06 ∠ 0.0
</td>
<td>
0.43 ∠ 0.23
</td>
<td>
0.67 ∠ 0.22
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.08 ∠ 0.0
</td>
<td>
0.44 ∠ 0.25
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.0 ∠ 0.0
</td>
</tr>
</tbody>
</table>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This document is the first step in that direction. We
welcome feedback on how it can be improved,and the concepts clarified.
Please feel free to open a pull request with thoughts, suggestions or
feedback.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>If integration signs in a “gentle” introduction feels
like a bait-and-switch, rest assured, for the puposes of this document
you just need to think of them as a way to add up presence masses, in a
way that the ideas we use for binary presences will generalize when we
apply them to arbitrary functions.<a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The way we’ve defined signals and mass is directly<br />
analogous to how mass is defined for matter occupying space in
physics.</p>
<p>A binary signal can be thought of as defining a one-dimensional
interval over<br />
time. For a fixed element and boundary, this gives us an area under
the<br />
curve in two dimensions: time vs. density.</p>
<p>If we treat elements and boundaries as additional independent
dimensions,<br />
then the signal defines a <em>volume</em> in three dimensions, with time
as one axis.</p>
<p>This interpretation—presence as a physical manifestation of density
over<br />
time—is a powerful way to reason intuitively and computationally about
duration, overlap, and accumulation in time.</p>
<p>And when we allow multiple signals to interact over the same time
periods, we<br />
begin to model complex, higher-dimensional effects of presence—exactly
the<br />
kind of generality we’ll need when we move beyond simple binary
presences.<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The use of a rough curve here is an example of how
presences can encode<br />
continuous inputs more effectively than discrete techniques, thanks to
their<br />
explicit model of time. Forcing a developer to rank their productivity
on a<br />
Likert scale often loses valuable nuance—whereas a fine-grained
presence<br />
captures temporal variation with ease, making it available for
downstream<br />
analysis.<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>There are several technical conditions that must be
satisfied when<br />
mapping signals to a canonical system of presences in order for this
claim to<br />
hold. To avoid getting bogged down in those details, we’ll simply claim
it for<br />
now. The API docs go into more detail about the mechanics of this
canonical<br />
representation, and what’s needed to ensure a “clean” mapping from a
signal to a system of presences—or more precisely, a system of presence
assertions.<a href="#fnref5" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>It is equally valid to define <span
class="math inline">\(N\)</span> as the number of distinct
<em>presences</em> in the observation window. For example for the signal
<span class="math inline">\(P2\)</span> in Figure 5, this corresponds to
asking if <span class="math inline">\(N=5\)</span> (if we count the
disjoint presences individually) or <span
class="math inline">\(N=3\)</span> (if we count the signals). These give
different values for <span class="math inline">\(\bar{m}\)</span> and
<span class="math inline">\(\iota\)</span> but their product <em>still
equals</em> <span class="math inline">\(\delta\)</span>, as long as a
single consistent definition of N is used. This is ultimately a modeling
decision that depends on what you are trying to measure. By default we
will assume that <span class="math inline">\(N\)</span> is measured at
the signal granularity.<a href="#fnref6" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>We note that the residence time represents only the
portion of the duration of the task in some arbitrary observation
window. This is a different quantity from the overall duration of the
task from start to finish (or from signal onset to reset in our
terminology). This is the more familiar metric typically called the
cycle time.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>All solutions to an equation of the form <span
class="math inline">\(x=y \times z,\)</span> which is the form of the
presence invariant, lie on a 2D surface, called a manifold in 3
dimensions: think of the manifold a rubber sheet suspended in a 3
dimensional space. All the values of <span class="math inline">\(x, y,
\text{ and } z\)</span> that satisfy this equation will lie on the
surface of this rubber sheet. This is a powerful geometric constraint
that we can exploit to reason about the possible behavior of a system of
presences over time.<a href="#fnref8" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The alert reader will note the difference between the
first two rows in the matrix. The first signal is represented by a
single presence, while the second is broken up into two disjoint
presences. This is entirely an artifact of the granularity at which the
timeline is divided. At a suitably fine sampling granularity, the first
signal could also be represented by two presences. As discussed in
footnote 6 above, this has no real impact on the behavior of the
invariant. In general we will use signals as the unit at which incidence
is measured.<a href="#fnref9" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Specifically, the Riemann sum approximation of the
integral <span class="math display">\[
A(1, j) = \sum_{k=1}^j A(k,k)
\approx \int_0^j \left( \sum_{(e,b)} P_{(e,b)}(t) \right) dt
\]</span> where each <span class="math inline">\(P_{(e,b)}(t)\)</span>
represents the presence density function of an underlying
element-boundary signal.<a href="#fnref10" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>While it’s tempting to assume that the limit of a
product is simply the product of the limits, this doesn’t automatically
hold here. The long-run average of presence density, <span
class="math inline">\(\Delta\)</span>, is defined as a time-based
average, while the average signal mass contribution, <span
class="math inline">\(\bar{M}\)</span>, is defined over the number of
signals. Since these limits are taken over different denominators,
additional technical conditions are required to ensure that their
product equals the limit of the product.<a href="#fnref11"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
