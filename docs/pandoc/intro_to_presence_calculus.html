<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>The Presence Calculus</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }

    html, body {
      margin: 0;
      padding: 0;
      font-family: Georgia, serif;
      font-size: 19px;
      line-height: 1.75;
      background: #fff;
      color: #111;

      /* flex centering */
      display: flex;
      justify-content: center;
    }

    #page {
      max-width: 880px;
      width: 100%;
      padding: 2.5rem 1.5rem;
      box-sizing: border-box;
    }

    h1, h2, h3, h4 {
      font-family: Georgia, serif;
      font-weight: bold;
      margin-top: 2rem;
      margin-bottom: 1rem;
      line-height: 1.4;
    }

    p {
      margin: 1.25rem 0;
    }

    img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 2rem auto;
    }

    pre {
      background: #f8f8f8;
      padding: 1rem;
      overflow-x: auto;
      border-radius: 6px;
    }

    code {
      font-family: SFMono-Regular, Consolas, monospace;
      background: #f5f5f5;
      padding: 0.2em 0.4em;
      border-radius: 4px;
    }

    blockquote {
      margin: 2rem 0;
      padding-left: 1rem;
      border-left: 4px solid #ddd;
      color: #666;
      font-style: italic;
    }

    .subtitle, .author, .date {
      color: #666;
      font-size: 1rem;
      margin-top: 0.5rem;
    }

    nav#TOC {
      margin: 2rem 0;
      padding: 1rem;
      border: 1px solid #eee;
      background: #fafafa;
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
  <div id="page">
            <header id="title-block-header">
      <h1 class="title"><strong>The Presence Calculus</strong></h1>
      <p class="subtitle"><span style="font-size:1.2em;">A Gentle
Introduction</span></p>
      <p class="author"><p>Dr. Krishna Kumar<br />
<em>The Polaris Advisor Program</em></p></p>
      
          </header>
            <nav id="TOC" role="doc-toc">
      <h2 id="toc-title">Contents</h2>
      <ul>
      <li><a href="#what-is-the-presence-calculus"
      id="toc-what-is-the-presence-calculus"><span
      class="toc-section-number">1</span> What is The Presence
      Calculus?</a>
      <ul>
      <li><a href="#the-pitch" id="toc-the-pitch"><span
      class="toc-section-number">1.1</span> The pitch</a></li>
      <li><a href="#learning-about-the-presence-calculus"
      id="toc-learning-about-the-presence-calculus"><span
      class="toc-section-number">1.2</span> Learning about the presence
      calculus</a></li>
      <li><a href="#why-presence" id="toc-why-presence"><span
      class="toc-section-number">1.3</span> Why presence?</a></li>
      </ul></li>
      <li><a href="#what-is-a-presence"
      id="toc-what-is-a-presence"><span
      class="toc-section-number">2</span> What is a presence?</a>
      <ul>
      <li><a href="#presence-mass-the-manifestation-of-presence"
      id="toc-presence-mass-the-manifestation-of-presence"><span
      class="toc-section-number">2.1</span> Presence mass: the
      manifestation of presence</a></li>
      <li><a href="#presence-density-functions-aka-signals"
      id="toc-presence-density-functions-aka-signals"><span
      class="toc-section-number">2.2</span> Presence density functions
      <em>aka</em> Signals</a></li>
      <li><a href="#more-examples" id="toc-more-examples"><span
      class="toc-section-number">2.3</span> More examples</a></li>
      <li><a href="#presence-a-summary"
      id="toc-presence-a-summary"><span
      class="toc-section-number">2.4</span> Presence: a summary</a></li>
      </ul></li>
      <li><a href="#systems-of-presences"
      id="toc-systems-of-presences"><span
      class="toc-section-number">3</span> Systems of Presences</a>
      <ul>
      <li><a href="#presence-as-a-sample-of-a-signal"
      id="toc-presence-as-a-sample-of-a-signal"><span
      class="toc-section-number">3.1</span> Presence as a sample of a
      signal</a></li>
      <li><a href="#presence-assertions"
      id="toc-presence-assertions"><span
      class="toc-section-number">3.2</span> Presence assertions</a></li>
      </ul></li>
      <li><a href="#co-presence-and-the-presence-invariant"
      id="toc-co-presence-and-the-presence-invariant"><span
      class="toc-section-number">4</span> Co-Presence and the Presence
      Invariant</a>
      <ul>
      <li><a href="#an-example-1" id="toc-an-example-1"><span
      class="toc-section-number">4.1</span> An example</a></li>
      <li><a href="#why-it-matters" id="toc-why-it-matters"><span
      class="toc-section-number">4.2</span> Why it matters</a></li>
      <li><a href="#binary-presences-and-littles-law"
      id="toc-binary-presences-and-littles-law"><span
      class="toc-section-number">4.3</span> Binary presences and
      Little’s Law</a></li>
      <li><a href="#signal-dynamics" id="toc-signal-dynamics"><span
      class="toc-section-number">4.4</span> Signal Dynamics</a></li>
      </ul></li>
      <li><a href="#the-presence-matrix"
      id="toc-the-presence-matrix"><span
      class="toc-section-number">5</span> The Presence Matrix</a>
      <ul>
      <li><a href="#the-presence-invariant-and-the-presence-matrix."
      id="toc-the-presence-invariant-and-the-presence-matrix."><span
      class="toc-section-number">5.1</span> The presence invariant and
      the presence matrix.</a></li>
      <li><a href="#the-presence-accumulation-matrix"
      id="toc-the-presence-accumulation-matrix"><span
      class="toc-section-number">5.2</span> The Presence Accumulation
      Matrix</a></li>
      </ul></li>
      <li><a href="#applying-the-presence-calculus"
      id="toc-applying-the-presence-calculus"><span
      class="toc-section-number">6</span> Applying the Presence
      Calculus</a>
      <ul>
      <li><a href="#equilibrium-convergence-and-divergence"
      id="toc-equilibrium-convergence-and-divergence"><span
      class="toc-section-number">6.1</span> Equilibrium: Convergence and
      Divergence</a></li>
      <li><a href="#detecting-convergence"
      id="toc-detecting-convergence"><span
      class="toc-section-number">6.2</span> Detecting
      Convergence</a></li>
      </ul></li>
      <li><a href="#navigation-defining-place-and-direction"
      id="toc-navigation-defining-place-and-direction"><span
      class="toc-section-number">7</span> Navigation: defining place and
      direction</a>
      <ul>
      <li><a href="#the-presence-accumulation-recurrence"
      id="toc-the-presence-accumulation-recurrence"><span
      class="toc-section-number">7.1</span> The Presence Accumulation
      Recurrence</a></li>
      <li><a
      href="#a-phase-space-representation-of-the-presence-invariant"
      id="toc-a-phase-space-representation-of-the-presence-invariant"><span
      class="toc-section-number">7.2</span> A phase space representation
      of the presence invariant</a></li>
      </ul></li>
      <li><a href="#attractors" id="toc-attractors"><span
      class="toc-section-number">8</span> Attractors</a></li>
      </ul>
    </nav>
        <h2 data-number="1" id="what-is-the-presence-calculus"><span
        class="header-section-number">1</span> What is The Presence
        Calculus?</h2>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pcalc/presence_calculus.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 1: The Presence Calculus - Key Concepts</code></pre>
        </div>
        </div>
        <p>The Presence Calculus is a quantitative model for reasoning
        about signal dynamics in a domain.</p>
        <p>Its purpose is to support principled modeling and rigorous
        decision-making using operational data and signals in
        business-critical contexts. It aims to ensure that such
        decisions rest on a mathematically precise, logically coherent,
        and epistemically grounded foundation.</p>
        <p>The presence calculus emerged from a search for better tools
        to reason about operations management in software product
        development and engineering—domains where prevailing approaches
        to measurement fall short in all three dimensions.</p>
        <p>At a minimum, its foundational constructs bring mathematical
        precision to widely used—but poorly defined—concepts such as
        flow, stability, equilibrium, and coherence for measurable
        signals in a domain.</p>
        <p>More importantly, it offers a set of uniform abstractions and
        computational tools that connect low-level activity signals to
        business-relevant signals such as delay, cost, revenue, and user
        experience.</p>
        <p>For software development, this simplifies the construction of
        bespoke, context-specific measurement models for operational
        improvement, a powerful alternative to one-size-fits all metrics
        frameworks and visibility tools that are the only option
        available today.</p>
        <p>As we’ll see, however, the core ideas of the calculus are
        more general, with potential applications well beyond the
        software domain in which it originated.</p>
        <h3 data-number="1.1" id="the-pitch"><span
        class="header-section-number">1.1</span> The pitch</h3>
        <p>We introduce the simple but powerful concept of a
        <em>presence</em>.</p>
        <p>This lets us reason about the history and evolution of a set
        of <em>signals</em> that measure time-varying properties of
        <em>elements</em> that are present in a defined
        <em>boundary</em> in a domain using techniques from measure
        theory, topology and complex analysis.</p>
        <p>Classical statistics and probability theory often struggle
        here.</p>
        <p><em>History</em>—the sequence and structure of changes in the
        domain over time— is usually fenced off under assumptions like
        ergodicity, stationarity, and independence.</p>
        <p>Our thesis is that to move beyond simple descriptive
        statistics and statistical or probabilistic inference, and
        reason effectively about global and long run behavior of many
        real world systems, we need analytical techniques that treat
        time and the history of signal interactions as first-class
        concepts we can model and calculate with.</p>
        <p>The Presence Calculus is a novel, <em>constructive</em>
        approach to this problem—an analytical framework for modeling
        <em>observed behavior</em> in systems ranging from simple,
        linear, and ordered to non-linear, stochastic, adaptive, and
        complex, all based on a small, uniform set of underlying
        concepts rooted in a primitive notion called
        <em>presence</em>.</p>
        <h3 data-number="1.2"
        id="learning-about-the-presence-calculus"><span
        class="header-section-number">1.2</span> Learning about the
        presence calculus</h3>
        <p>While the calculus was developed with mathematical rigor, an
        equally important goal was not to let mathematics get in the way
        of understanding the simple but very powerful ideas the calculus
        embodies<a href="#fn1" class="footnote-ref" id="fnref1"
        role="doc-noteref"><sup>1</sup></a>.</p>
        <p>In this document, we’ll motivate and introduce the key ideas
        in the calculus informally, with lots of evocative examples and
        simplifications to<br />
        illustrate concepts.</p>
        <p>In order to maintain precision, mathematical definitions are
        used in key sections. We augment these with examples to build
        intuition throughout. However, given the nature of the material
        we have opted to stay on the side of rigor rather than dilute
        the concepts, even in this “gentle” introduction.</p>
        <p>If you are inclined to skim over anything with mathematical
        notation in it, working through the examples should be
        sufficient to grasp the key ideas and claims. However, for those
        who are comfortable with it, the mathematics should be easy to
        understand and verify.</p>
        <p>The presence calculus is constructive - specifically
        everything in the calculus has a computational aspect, and this
        document is designed to lay the framework for describing how to
        perform those computations in a step by step manner.</p>
        <p>This means that it is best to read the sections in order, as
        each section builds on the concepts from the previous sections
        systematically. The concepts are not particularly difficult to
        grasp, but they will make more sense if you take time to
        understand how they fit together in the order they are presented
        here.</p>
        <p>So even though it is a gentle introduction, it is not the
        kind of introduction that you will get as much value from if you
        simply skim this document.</p>
        <p>If that deeper dive is not your cup of tea, we’ll continue
        with ongoing informal exposition on our blog <a
        href="https://www.polaris-flow-dispatch.com">The Polaris Flow
        Dispatch</a>, where we will focus mostly on informal exposition
        and applications of the ideas.</p>
        <p>If you are interested in working with the calculus, we
        recommend reading and understanding all the main ideas here
        before jumping deeper into the rest of the documentation at this
        site, which get a fair bit more dense and technical.</p>
        <p>This document can be thought as the middle ground: detailed
        enough to understand the concepts and even implement and extend
        them yourself if you are so inclined, but just a starting point
        if you want to really dig deeper.</p>
        <p>That next level of detail is in the API docs for <a
        href="https://py.pcalc.org">The Presence Calculus<br />
        Toolkit</a>.</p>
        <p>The toolkit is an open source reference implementation of the
        core concepts in the presence calculus. It is currently situated
        as a middleware layer suitable for interfacing real world
        operational systems and complex system simulation, to the
        analytical machinery of the presence calculus.</p>
        <p>In the API documentation, we go into the concepts at a level
        of rigor that you’ll need to compute with the presence calculus
        and apply the concepts. The background and context from this
        document will be important if you want to apply the concepts
        from first principles and develop new applications.</p>
        <p>Finally, for those who want to dive deeper into the formal
        mathematical<br />
        underpinnings of the calculus, there are links in the references
        and footnotes of this document. These go into more detail than a
        practitioner will need to read or understand, but is useful for
        the mathematically trained to validate the rationale for
        <em>why</em> the presence calculus works.</p>
        <p>Let’s jump in…</p>
        <h3 data-number="1.3" id="why-presence"><span
        class="header-section-number">1.3</span> Why presence?</h3>
        <p>Presence is what we observe in the world.</p>
        <p>We dont experience reality as a sequence of discrete events
        in time, but as a continuous unfolding—things are present, or
        they appear, endure for a while, and then slip away.</p>
        <p>Permanence is simply a form of lasting presence. What we call
        <em>change</em> is the movement of presences into and out of
        awareness, often set against that permanence.</p>
        <p>The sense that something is present—or no longer present—is
        our most immediate way of detecting change. This applies equally
        to the tangible—people, places, and things—and the
        intangible—emotions, feelings, and experiences.</p>
        <p>Either way, reasoning about the presences and absences in our
        environment over time is key to understanding the dynamics of
        the world around us.</p>
        <p>The Presence Calculus begins here.</p>
        <p>Before we count, measure, compare, or optimize, we observe
        what <em>is</em>.</p>
        <p>And what we observe is presence.</p>
        <h4 data-number="1.3.1" id="an-example"><span
        class="header-section-number">1.3.1</span> An example</h4>
        <p>Imagine you see a dollar bill on the sidewalk on your way to
        get coffee.<br />
        Later, on your way back home, you see it again—still lying in
        the same spot. It would be reasonable for you to assume that the
        dollar bill was present there the whole time.</p>
        <p>Of course, that may not be true. Someone might have picked it
        up in the<br />
        meantime, felt guilty, and quietly returned it. But in the
        absence of other<br />
        information, your assumption holds: it was there before, and
        it’s there now, so it must have been there in between.</p>
        <p>This simple act of inference is something we do all the time.
        We fill in gaps, assume continuity, and reason about what must
        have been present based on what we know from partial glimpses of
        the world.</p>
        <p>The presence calculus gives formal shape to this kind of
        inference about <em>things</em>, <em>places</em> and
        <em>time</em>—and shows how we can build upon it to
        <em>reason</em> about presence and <em>measure</em> its effects
        in an environment.</p>
        <h4 data-number="1.3.2" id="a-software-example"><span
        class="header-section-number">1.3.2</span> A software
        example</h4>
        <p>Since the ideas here emerged from the software world, let’s
        begin with a<br />
        mundane, but familiar example: task work in a software team.</p>
        <p>We usually reason about task work using <em>events</em> and
        <em>snapshots</em> of the state<br />
        of a process in time. A task “starts” when it enters
        development, and<br />
        “finishes” when it’s marked complete. We track “cycle time” by
        measuring the elapsed time between events, “throughput” by
        counting finish events, and ” work-in-process” by counting tasks
        that have started but not yet finished.</p>
        <p>When we look at a Kanban board, we see a point-in-time
        snapshot of where tasks are at that moment—but not how they got
        there. And by the time we read a summary report of how many
        tasks were finished and how long they took to go through the
        process on average, much of the history of the system that
        produced those measurements has been lost. They become mere
        descriptive statistics about the system at a point in time. That
        makes it hard to reason about <em>why</em> those measurements
        are the way they are.</p>
        <p>In software development, each task often has a distinct
        history—different<br />
        from other tasks present at the same time. Losing history makes
        it hard to<br />
        reason about the interactions between tasks and how they impact
        the global behavior of the process.</p>
        <p>This problem is not unique to task work. Similar problems
        exist in almost all areas of business analysis that rely
        primarily on event models and descriptive statistics derived
        from events as the primary measurement tool for analyzing system
        behavior.</p>
        <p>We are reduced to trying to make inferences from local
        descriptive<br />
        statistics —things like cycle times, throughput, and
        work-in-process levels- over a rapidly changing process.</p>
        <p>We try to reason about a process which is shaped by its
        history, whose behavior emerges from non-uniform interactions of
        individual tasks, with measurement techniques that lack the
        ability to represent or reason about that history or the
        interactions.</p>
        <p>This is difficult to do, and we have no good tools right now
        that are fit for this purpose. This is where the presence
        calculus begins.</p>
        <p>By looking closely at how we reason about tasks in the
        presence calculus, we can see how a subtle shift from an
        event-centered to a presence-centered perspective changes not
        just what we observe, but what we measure, and thus can reason
        about.</p>
        <p>The calculus focuses on the time <em>in between</em>
        snapshots of history: when a task was present, where it was
        present, for how long, and whether its presence shifted across
        people, tools, or systems.</p>
        <p>The connective tissue is no longer the task itself, or the
        process steps it<br />
        followed, or who was working on it, but a continuous, observable
        <em>thread of<br />
        presence</em>—through all of them, moving through time,
        interacting, crossing boundaries—a mathematical representation
        of history.</p>
        <p>With the presence calculus, these threads and their
        interactions across time and space can now be measured directly,
        dissected, composed, and analyzed as first-class
        constructs—built on a remarkably simple primitive—the
        presence.</p>
        <h4 data-number="1.3.3" id="the-heart-of-the-matter"><span
        class="header-section-number">1.3.3</span> The heart of the
        matter</h4>
        <p>At its core, the calculus exploits the difference between the
        two independent statements—“The task started development on
        Monday” and “The task completed development on Friday”—and a
        single, unified assertion: “The task was present in development
        from Monday through Friday.”</p>
        <p>The latter is called a <em>presence</em>, and it is the
        foundational building block<br />
        of the calculus. At first glance, this might not seem like a
        meaningful difference.</p>
        <p>But treating the presence as the primary object of
        reasoning—as a <em>first-class</em> construct—opens up an
        entirely new space of possibilities.</p>
        <p>Specifically, it allows us to apply powerful mathematical
        tools that exploit the topology of time and the algebra of time
        intervals to reason about the interactions and configurations of
        presences in a rigorous and structured, and more importantly,
        computable way.</p>
        <h2 data-number="2" id="what-is-a-presence"><span
        class="header-section-number">2</span> What is a presence?</h2>
        <p>Let’s start by building intuition for the concept. Consider
        the statement: “The task <span class="math inline">\(X\)</span>
        was in Development from Monday to Friday.”</p>
        <p>In the presence calculus, this would be expressed as a
        statement of the form: “The element <span
        class="math inline">\(X\)</span> was in boundary <span
        class="math inline">\(Y\)</span> from <span
        class="math inline">\(t_0\)</span> to <span
        class="math inline">\(t_1\)</span> with mass 1.”</p>
        <p>Presences are statements about elements (from some domain)
        being present in a boundary (from a defined set of boundaries)
        over a <em>continuous</em> period of time, measured using some
        timescale.</p>
        <p>So why do we say “with mass 1”?</p>
        <blockquote>
        <p>The presence calculus treats time as a physical dimension,
        much like space. Just as matter occupies space, presences occupy
        time. Just as mass quantifies <em>how</em> matter occupies
        space, the mass of a presence quantifies <em>how</em> a presence
        occupies time.</p>
        </blockquote>
        <p>The statement “The task <span
        class="math inline">\(X\)</span> was in Development from Monday
        through Friday” is a <em>binary presence</em> with a uniform
        mass of 1 over the entire duration. The units of this mass are
        element-time—in this case, task-days.</p>
        <p>Binary presences are sufficient to describe the <em>fact</em>
        of presence or absence<br />
        of things in places in a domain. These presences always have
        mass 1 in whatever units we use for elements and time.</p>
        <p>Typically, but not always, these represent the presence or
        absence of activity in a domain and can also be considered
        <em>activity signals</em>.</p>
        <h3 data-number="2.1"
        id="presence-mass-the-manifestation-of-presence"><span
        class="header-section-number">2.1</span> Presence mass: the
        manifestation of presence</h3>
        <p>Let’s consider a different set of statements:</p>
        <blockquote>
        <p>“Task <span class="math inline">\(X\)</span> had 2 developers
        working on it from Monday to Wednesday,<br />
        3 developers on Thursday, and 1 developer on Friday.”</p>
        </blockquote>
        <p>These are no longer about just presence, but about the
        <em>effects</em> of presence.<br />
        They describe the <strong>load</strong> that task <span
        class="math inline">\(X\)</span> placed on the Development
        boundary<br />
        over time.</p>
        <p>The units of this presence are developer-days - potentially
        in a completely different dimension from the task, but grounded
        over the same time interval as the task.</p>
        <p>Here we are saying: “the task being in this boundary over
        this time period, had this effect in a related dimension.”</p>
        <p>We will describe this using a presence with an arbitrary
        <em>real valued mass</em>. We will assume that there is some
        function, that computes this mass. In our example, let’s call
        this function <span
        class="math inline">\(\mathsf{load}.\)</span></p>
        <p>The presence can then be described as</p>
        <ul>
        <li><span class="math inline">\((\mathsf{load}, X,
        \text{Development}, \text{Monday}, \text{Wednesday},
        2)\)</span></li>
        <li><span class="math inline">\((\mathsf{load}, X,
        \text{Development}, \text{Thursday}, \text{Thursday},
        3)\)</span></li>
        <li><span class="math inline">\((\mathsf{load}, X,
        \text{Development}, \text{Friday}, \text{Friday},
        1)\)</span></li>
        </ul>
        <p>Here, <span class="math inline">\(\mathsf{load}(e, b,
        t)\)</span> is a time-varying function that takes an<br />
        element <span class="math inline">\(e\)</span>, a boundary <span
        class="math inline">\(b\)</span>, and a time <span
        class="math inline">\(t\)</span>, and returns a real-valued
        number<br />
        describing how much presence is concentrated at that point in
        time.</p>
        <p>The <em>presence mass</em> of such a presence is the total
        presence over the<br />
        interval <span class="math inline">\([t_0, t_1]\)</span>,
        defined as:</p>
        <p><span class="math display">\[ \text{mass} = \int_{t_0}^{t_1}
        \mathsf{load}(e, b, t)\, dt \]</span></p>
        <p>where <span class="math inline">\(\mathsf{load}\)</span> is
        called a <em>presence density function</em> <a href="#fn2"
        class="footnote-ref" id="fnref2"
        role="doc-noteref"><sup>2</sup></a>.</p>
        <p>Binary presences are much easier to understand intuitively,
        but the real power of the presence calculus comes from
        generalizing to <em>presence density functions</em>.</p>
        <h3 data-number="2.2"
        id="presence-density-functions-aka-signals"><span
        class="header-section-number">2.2</span> Presence density
        functions <em>aka</em> Signals</h3>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/presence_definition.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 2: Signals, Presence,  and Presence Mass</code></pre>
        </div>
        </div>
        <p>We can extrapolate from the example of the load function and
        think of defining a presence over an arbitrary time varying
        function with real numbers as values. We will call these
        <em>presence density functions.</em></p>
        <p>Such functions represent an underlying <em>signal</em> from
        the domain that we are interested in measuring. In what follows,
        we will use the terms signal and presence density functions
        interchangeably, opting for the latter only those cases where we
        want to focus specifically on the fact that what we are
        representing about the signal is the “amount” of the signal (its
        presence) over time.</p>
        <p>As shown in Figure 2, the mass of a presence density
        function, over any given time <em>interval</em> <span
        class="math inline">\([t_0, t1)\)</span> is the <em>integral of
        the function</em> over the interval, which is also the area
        under the signal over that interval<a href="#fn3"
        class="footnote-ref" id="fnref3"
        role="doc-noteref"><sup>3</sup></a>.</p>
        <p>The only requirement for a function to be a presence density
        function (signal) is that it is <em>measurable</em>, and that
        you can interpret <em>presence mass</em>—defined as the integral
        of the function over a finite interval—as a meaningful
        <em>measure</em> of the effect of presence in your domain.</p>
        <p>This is where measure theory enters the picture. It’s not
        essential to understand the full technical details, but at its
        core, measure theory tells us which kinds of functions are
        measurable—in other words, which functions can support
        meaningful accumulation, comparison, and composition of values
        via <em>integration</em>.</p>
        <p>When a presence density functions (signal) is measurable, it
        gives us the confidence to do things like compute statistics,
        aggregate over elements or boundaries, and compose presence
        effects—while preserving the semantics of the domain.</p>
        <p>From our perspective, a presence density function is a domain
        signal whose value that can be <em>accumulated</em> across time
        and across presences.</p>
        <p>This lets us reason mathematically about presences with
        confidence—and since most of this reasoning will be performed by
        algorithms, we need technical constraints that ensure those
        calculations are both mathematically valid and semantically
        sound.</p>
        <h3 data-number="2.3" id="more-examples"><span
        class="header-section-number">2.3</span> More examples</h3>
        <p>Let’s firm up our intuition about what signals in the
        presence calculus can describe with a few more examples of
        presence density functions.</p>
        <h4 data-number="2.3.1" id="work-in-software"><span
        class="header-section-number">2.3.1</span> “Work” in
        software</h4>
        <p>If you’ve ever written a line of code in your life, you’ve
        heard the question:<br />
        “When will it be done?” Work in software can be a slippery,
        fungible concept— and the presence calculus offers a useful way
        to describe it.</p>
        <p>We can express the work on a task using a presence density
        function whose value at time <span
        class="math inline">\(t\)</span> is the <em>remaining</em> work
        on the task at <span class="math inline">\(t\)</span>.</p>
        <p>This lets us model tasks whose duration is uncertain in
        general, but whose<br />
        remaining duration can be estimated, subject to revision, at any
        given time—a common scenario in software contexts.</p>
        <p>A series of presences, where the (non-zero) mass of each
        presence corresponds to the total remaining work over its
        interval (interpreting the integral as a sum), gives us a way to
        represent <em>work as presence</em>.</p>
        <p>Such presences can represent estimates, forecasts, or
        confidence-weighted<br />
        projections—and as we’ll see, they can be reasoned about and
        computed with just like any other kind of presence.</p>
        <h4 data-number="2.3.2" id="the-effects-of-interruptions"><span
        class="header-section-number">2.3.2</span> The effects of
        interruptions</h4>
        <p>Another useful example from the software world illustrates a
        different<br />
        application of a presence. Let’s assume the boundary in this
        case is a<br />
        developer, the element is an interruption (defined appropriately
        in the<br />
        domain), and the presence density function captures the
        <em>context switching<br />
        cost</em>—measured in lost time—associated with that
        interruption.</p>
        <p>The key insight here is that the <em>effects</em> of the
        presence extend beyond the<br />
        interval of the interruption itself. This is a classic case of a
        delayed or<br />
        <em>decaying effect</em>—a pattern that appears frequently in
        real-world systems.</p>
        <p>The presence density function can be modeled in different
        ways:</p>
        <ul>
        <li>As a constant cost: for example, each interruption causes a
        fixed<br />
        15-minute recovery period, regardless of its duration.</li>
        <li>As a decaying function: the cost is highest at the moment of
        interruption<br />
        and gradually decreases to zero over a defined recovery window
        (e.g.<br />
        15 minutes), representing a return to full focus.</li>
        </ul>
        <p>This approach gives us a precise way to model and reason
        about the<br />
        <em>aftereffects</em> of events—effects that outlast the events
        themselves and<br />
        accumulate in subtle but measurable ways over longer
        timeframes.</p>
        <p>In this case, we measured an effect that decayed from a peak,
        but a similar<br />
        approach can be taken, for example, with a presence density
        function that<br />
        grows from zero and plateaus over the duration of the
        presence—such as the net increase in revenue due to a released
        feature.</p>
        <p>Used this way, presence density functions give us a powerful
        tool for modeling the impact of binary presences—capturing their
        downstream or distributed effects over time, and reasoning about
        their relationship over a shared timeline.</p>
        <p>Another important use case in the same vein is modeling the
        cost of delay for a portfolio-level element—and analyzing its
        cascading impact across a portfolio.</p>
        <p>These use cases show that it is possible to analyze not just
        binary presences, but entire chains of influence they exert
        across a timeline—a key prerequisite for causal reasoning.</p>
        <h4 data-number="2.3.3"
        id="self-reported-developer-productivity"><span
        class="header-section-number">2.3.3</span> Self-reported
        developer productivity</h4>
        <p>Imagine a developer filling out a simple daily check-in: “How
        productive did you feel today?”—scored from 1 to 5, or sketched
        out as a rough curve over the day<a href="#fn4"
        class="footnote-ref" id="fnref4"
        role="doc-noteref"><sup>4</sup></a>.</p>
        <p>Over a week, this forms a presence density function—not of
        the developer in a place, but of their <em>sense</em> of
        productivity over time.</p>
        <p>These types of presences, representing perceptions, are
        powerful—helping<br />
        teams track experience, spot early signs of burnout, or
        correlate perceived<br />
        productivity with meetings, environment changes, build failures,
        or interruptions.</p>
        <p>Now, let’s look at some examples outside software
        development.</p>
        <h4 data-number="2.3.4"
        id="browsing-behavior-on-an-e-commerce-site"><span
        class="header-section-number">2.3.4</span> Browsing behavior on
        an e-commerce site</h4>
        <p>Imagine a shopper visiting an online store. They spend 90
        seconds browsing kitchen gadgets, then linger for five full
        minutes comparing high-end headphones, before briefly glancing
        at a discounted blender.</p>
        <p>Each of these interactions can be modeled as a presence: the
        shopper’s<br />
        (element) attention occupying different parts of the site
        (boundaries) over<br />
        time. The varying durations reflect interest, and the shifting
        presence reveals patterns of engagement in a population of
        visitors in a boundary (an area of the site).</p>
        <p>By analyzing these presences across visitors to the site
        —where and for how long attention dwells—we can begin to
        understand population level preferences, intent, and even the
        likelihood of conversion (modeled as a different presence
        density function).</p>
        <h4 data-number="2.3.5"
        id="patient-movement-in-a-hospital"><span
        class="header-section-number">2.3.5</span> Patient movement in a
        hospital</h4>
        <p>Consider a patient navigating a hospital stay. They spend the
        morning in<br />
        radiology, move to a recovery ward for several hours, then are
        briefly<br />
        transferred to the ICU overnight.</p>
        <p>Each location records a presence—when and where the patient
        was, and for how long. These presences can reveal bottlenecks,
        resource utilization, and potential risks.</p>
        <p>Over time, analyzing patient presences helps surface patterns
        in care<br />
        delivery, delays in treatment, and opportunities for improving
        patient flow.</p>
        <p>These are examples of classic operations management problems
        expressed in the language of the presence calculus. The calculus
        is well-suited to modeling scenarios like these as a base
        case.</p>
        <h3 data-number="2.4" id="presence-a-summary"><span
        class="header-section-number">2.4</span> Presence: a
        summary</h3>
        <p>Let’s summarize what we’ve described so far.</p>
        <p>With signals and presences, we now have a framework for
        describing and measuring the behavior of time-varying domain
        signals—each representing how a specific element behaves within
        a given boundary.</p>
        <p>The key feature of a presence is that it abstracts these
        behaviors into a uniform representation—one we can reason about
        and compute with.</p>
        <p>Formally, a general presence is defined by:</p>
        <ul>
        <li>a presence density function (or signal) <span
        class="math inline">\(f(e, b, t)\)</span>,</li>
        <li>an element <span class="math inline">\(e\)</span>,</li>
        <li>a boundary <span class="math inline">\(b\)</span>,</li>
        <li>and a time interval <span class="math inline">\([t_0,
        t_1]\)</span>.</li>
        </ul>
        <p>Its <strong>mass</strong> is the integral of <span
        class="math inline">\(f\)</span> over the interval:</p>
        <p><span class="math display">\[ \text{mass}(e, b, [t_0, t_1]) =
        \int_{t_0}^{t_1} f(e, b, t)\, dt \]</span></p>
        <p>This mass captures both <em>that</em> the element was
        present, and <em>how</em> it was<br />
        present—uniformly, variably, or intermittently—over the
        interval.</p>
        <p>Intuitively, you can think of <em>integration</em> as the
        mathematical process by which we construct a continuous temporal
        model from discrete events in a domain.</p>
        <p>Sensors in the real world often generate both discrete event
        streams and continuous signals. Modeling all signals uniformly
        as presences with temporal mass is the first step toward
        analyzing interactions and dynamics <em>across</em>
        heterogeneous signals within a domain.</p>
        <h2 data-number="3" id="systems-of-presences"><span
        class="header-section-number">3</span> Systems of Presences</h2>
        <p>In this section, we move from individual presence density
        functions to systems of signals—each representing the presence
        behavior of many elements across many boundaries within a
        domain.</p>
        <p>A key aspect of the Presence Calculus is that it represents
        fine-grained signals corresponding to individual elements. We
        study how the presence masses of these signals interact over
        time to produce observable <em>cumulative</em> effects.</p>
        <p>A signal describes the continuous behavior of a specific
        domain element within a boundary over time. The calculus
        emphasizes that each of these signals traces a distinct path
        through time, and seeks to derive insights from their
        <em>interactions</em> over a shared interval.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/pdf_examples.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 3: A System of Presences</code></pre>
        </div>
        </div>
        <p>From the standpoint of the calculus itself, it does not
        matter what the signals represent—we treat them uniformly as
        real-valued presence masses and define a standard set of
        mathematical operations over them.</p>
        <p>From the standpoint of semantics, however, the
        <em>meaningfulness</em> of these operations depends on the
        domain. What it means to combine signals, which ones to combine,
        and how to interpret the result—all of this is context-dependent
        and crucial for producing useful insights.</p>
        <p>This aspect—modeling and model development—is essential for
        applying the machinery effectively. However, our focus here is
        on the calculus itself: what it can say in general about
        arbitrary systems of signals and presence density functions that
        interact in time.</p>
        <p>In what follows, we will treat signals and presence density
        functions as abstract mathematical objects and describe the
        properties and operations of such systems. We will build some
        domain-specific intuition through examples, but will not focus
        on applications.</p>
        <p>Those will be the subject of future posts—an area that is
        both rich and complex, and which will depend heavily on the
        foundational machinery developed in this document.</p>
        <h3 data-number="3.1"
        id="presence-as-a-sample-of-a-signal"><span
        class="header-section-number">3.1</span> Presence as a sample of
        a signal</h3>
        <p>A signal, in general, describes the continuous behavior of an
        element within a boundary over time. This continuous signal may
        have one or more disjoint periods where its value is non-zero.
        These non-zero periods are called the <em>support</em> of the
        signal.</p>
        <p>As we see in Figure 4, a single signal (for a given element
        and boundary) might have multiple support intervals. These may
        correspond to episodic behavior in the underlying domain, for
        example, user sessions in an e-commerce context, or rework loops
        in software development when a task “returns” to development
        many times over its lifecycle.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/multiple_support.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 4: A presence mass as a sample of a signal over an interval.</code></pre>
        </div>
        </div>
        <p>A <em>presence</em> (the 5-tuple <span
        class="math inline">\(p = (e, b, t_0, t_1, m)\)</span> that we
        work with in the calculus) is generated by taking an
        <em>observation</em> of this underlying signal over a specific
        time interval <span class="math inline">\([t_0, t_1)\)</span>,
        and computing its mass. This interval can be chosen in many
        ways:</p>
        <ul>
        <li>It might perfectly align with a single support interval (a
        ‘hill’ in the signal).</li>
        <li>It might span multiple disjoint support intervals, including
        the “zero” regions in between.</li>
        <li>It might capture only a portion of a single support.</li>
        </ul>
        <p>All we require is that the interval chosen for the presence
        calculation intersects a region of non-zero area from the signal
        that can be reduced to a non-zero presence mass.</p>
        <h3 data-number="3.2" id="presence-assertions"><span
        class="header-section-number">3.2</span> Presence
        assertions</h3>
        <p>Thus presence is best thought of as a <em>sampled
        measurement</em> of the underlying signal, taken by an
        <em>observer</em> over a specific time interval, which yields a
        total presence mass for that interval.</p>
        <p>A given observer may not even “see” the full underlying
        signal—only the <em>mass</em> of the presence they experience
        over the interval they observed.</p>
        <p>Different observers may observe different intervals of the
        same signal and derive different presence values, depending on
        what part of the function they encounter.</p>
        <p>So here is the first key assumption of the presence
        calculus:</p>
        <blockquote>
        <p>A system of presences refers to a collection of discrete
        observations drawn from an underlying set of presence density
        functions. In general, we do not assume we have access to the
        “true” underlying functions. Instead, we reason about these
        functions based their observed presence masses over interval and
        treat these observations as the basis for all inference within
        the calculus.</p>
        </blockquote>
        <p>This brings us to the concept of <em>presence
        assertions</em>, which formalize the act of recording a presence
        based on an observer’s local “view” of the underlying density
        function.</p>
        <p>A <em>presence assertion</em> is simply a presence augmented
        with metadata:</p>
        <ul>
        <li><em>who</em> the observer was</li>
        <li>and an <em>assertion timestamp</em>—the time at which the
        observation was made.</li>
        </ul>
        <p>The assertion time doesn’t need to align with the time
        interval of the presence. This allows assertions to refer to the
        past, reflect the present, or even anticipate the future
        behavior of a signal.</p>
        <h4 data-number="3.2.1" id="the-open-world-assumption"><span
        class="header-section-number">3.2.1</span> The open world
        assumption</h4>
        <p>Time in the presence calculus is explicitly defined to be
        over <em>extended</em> reals <span
        class="math inline">\(\overline{\mathbb{R}}\)</span>: the real
        line <span class="math inline">\(\mathbb{R}\)</span> extended
        with the symbols <span class="math inline">\(-\infty\)</span>
        and <span class="math inline">\(+\infty\)</span>.</p>
        <p>This is a mathematical representation of an open world
        assumption, which holds that the history of a system of
        presences extends indefinitely into the past and future.</p>
        <p>An observer will typically only see a finite portion of this
        history and has to make inferences on the basis of those
        observations, but in general, we need to make inferences with
        partial information about the past and contingent assumptions
        about the future.</p>
        <p>A presence with <span class="math inline">\(t_0 =
        -\infty\)</span> represents a presence whose beginning is
        unknown, and <span class="math inline">\(t_1 = +\infty\)</span>
        represents a presence whose end is unknown.</p>
        <p>Presences with both start and end unknown are valid
        constructs and represent eternal presences.</p>
        <p>Many of the most interesting questions in the presence
        calculus involve reasoning about the signal dynamics of a domain
        under the epistemic uncertainty introduced by such
        presences.</p>
        <h4 data-number="3.2.2" id="a-note-on-epistemology"><span
        class="header-section-number">3.2.2</span> A note on
        epistemology</h4>
        <p>Presence assertions give us the ability to assign
        <em>provenance</em> to a presence— not just <em>what</em> we
        know, but <em>how</em> we know it. This is essential in<br />
        representing complex systems where the observer and the act of
        observation are first-class concerns.</p>
        <p>Further, since reasoning about time and history is a primary
        focus of the calculus, presences with unknown beginnings or
        endings provide a way to explicitly model what is known—and
        unknown—about that history. This will prove more valuable than
        it might initially seem.</p>
        <p>We won’t go too deeply into the epistemological aspects of
        the presence<br />
        calculus in this document—this remains an active and open area
        of research and development and complements much of what we
        discuss here.</p>
        <p>But it’s important to acknowledge that this layer exists,
        that modeling and interpreting the output of the presence
        calculus requires an explicit treatment of how observations are
        made and by whom, and the fact that this has a huge impact on
        the validity of the inferences one makes using the machinery of
        the calculus.</p>
        <p>With this caveat in place, once we’ve represented a problem
        domain as a<br />
        <em>system of presences</em>, much of the machinery of the
        presence calculus (which we’ll introduce next) can be applied
        uniformly.</p>
        <h4 data-number="3.2.3" id="a-note-on-path-dependence"><span
        class="header-section-number">3.2.3</span> A note on path
        dependence</h4>
        <p>By representing a presence at the granularity of an element
        in a boundary, we explicitly recognize the path dependent nature
        of the domain signals.</p>
        <p>In the presence calculus, signals represent some behavior of
        domain elements in boundaries. The calculus itself is agnostic
        to what counts as an “element” or a “boundary”— this is a domain
        modeling decision.</p>
        <p>Even if they represent the same underlying quantity, we
        recognize that system behavior emerges from the interactions
        between <em>individual</em> signals at the element-boundary
        granularity—each potentially with distinct presence density
        functions. We are interested in studying <em>how</em>
        system-level behavior arises from the <em>interactions</em>
        between these signals over time.</p>
        <p>Modeling boundaries are crucial because, for a given domain
        element, the boundary typically determines both which signals
        are relevant and how we wish to analyze system behavior using
        them.</p>
        <p>For example, a software feature (an element) may be modeled
        using one set of signals during development (one boundary),
        another during production (a second boundary), and yet another
        when customers begin using it (a third boundary).</p>
        <p>All three signals are part of the feature’s history. Each
        feature follows a unique path through these boundaries,
        producing its own independent set of signals with a distinct
        history and evolution. These signals interact in complex
        ways—over time, within boundaries, and with each other. The
        boundary is what brings coherence to the analysis—it defines
        which signals and interactions we choose to focus on, and
        why.</p>
        <blockquote>
        <p>The boundary allows us to bring a coherent set of elements
        and related signals together for analysis. That analysis focuses
        on how these signals interact in <em>time</em>.</p>
        </blockquote>
        <p>Constructing an appropriate set of element-boundary signals
        is <em>the</em> key modeling decision. But once these are
        defined, much of the machinery of the presence calculus can be
        applied without regard to the semantics of the specific
        element-boundaries involved.</p>
        <p>Semantics are, of course, crucial in <em>interpreting</em>
        the inferences one draws<br />
        using the machinery of the calculus.</p>
        <blockquote>
        <p>When we refer to a “system” in the presence calculus we are
        explicitly defining it as an <em>evolving</em> set of presence
        assertions—i.e., the “system” is what we can assert about a
        domain using presence assertions at a given time.</p>
        </blockquote>
        <p>In summary, this is what we refer to as a “system of
        presences” - a time-indexed collection of presence assertions
        derived from an underlying set of path dependent
        element-boundary signals.</p>
        <h2 data-number="4"
        id="co-presence-and-the-presence-invariant"><span
        class="header-section-number">4</span> Co-Presence and the
        Presence Invariant</h2>
        <p>Figure 5 illustrates an example of a system of presences,
        where we focus on the subset of presences observed over a
        <em>common interval</em>.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/presence_invariant_continuous.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 5: Co-Presence and The Presence Invariant</code></pre>
        </div>
        </div>
        <p>These presences are called <em>co-present</em>—they represent
        an observer making simultaneous measurements of presence mass
        across multiple signals over a common interval of time.</p>
        <p>Co-presence is a necessary (but not sufficient) condition for
        interaction between one or more signals. This section introduces
        a key construct: the presence invariant. It expresses a general
        and powerful relationship that holds for any co-present subset
        of presences within a finite observation window, and it is a
        fundamental relationship that governs how the masses of
        co-present signals interact.</p>
        <p>Let’s establish this relationship.</p>
        <p>Given, any finite observation interval, we’ve already shown
        that each presence density function has a <em>presence
        mass</em>, defined as the integral of the density over the
        observation interval.</p>
        <p>These can be thought of as mass <em>contributions</em> from
        those presences to that interval. The sum of these individual
        mass contributions gives the total presence mass observed across
        the system in that interval.</p>
        <p>In our example from Figure 5,</p>
        <p><span class="math display">\[ A = M_0 + M_1 + M_3
        \]</span></p>
        <p>is the cumulative mass contribution from the signals that
        have non-zero mass over the interval <span
        class="math inline">\([t_0, t_1)\)</span>. The length of this
        interval is <span class="math inline">\(T = t_1 -
        t_0\)</span>.</p>
        <p>Let <span class="math inline">\(N\)</span> be the number of
        <em>active signals</em>: distinct signals with a presence in the
        observation window<a href="#fn5" class="footnote-ref"
        id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
        <p>Now let’s consider the quantity <span class="math display">\[
        \delta = \frac{A}{T}
        \]</span></p>
        <p>Since the mass comes from integrating a density function over
        time, the quantity <span
        class="math inline">\(\frac{A}{T}\)</span> represents the
        <em>average presence density</em> over the observation interval
        <span class="math inline">\(T\)</span> <a href="#fn6"
        class="footnote-ref" id="fnref6"
        role="doc-noteref"><sup>6</sup></a>.</p>
        <p>We can now decompose this as:</p>
        <p><span class="math display">\[ \delta = \frac{A}{T} =
        \frac{A}{N} \times \frac{N}{T} \]</span></p>
        <p>This separates the average presence density into two
        interpretable components:</p>
        <ul>
        <li><span class="math inline">\(\bar{m} = \frac{A}{N}\)</span>:
        the <em>average mass contribution</em> per active signal,</li>
        <li><span class="math inline">\(\iota = \frac{N}{T}\)</span>:
        the signal <em>incidence rate</em>—i.e., the number of active
        signals per unit time.</li>
        </ul>
        <p>This leads to the <em>presence invariant</em>:</p>
        <p><span class="math display">\[ \text{Average Presence Density}
        = \text{Signal Incidence Rate} \times \text{Average Mass
        Contribution per Signal} \]</span> or in our notation</p>
        <p><span class="math display">\[ \delta = \iota \cdot \bar{m}
        \]</span></p>
        <p>This identity holds for <em>any</em> co-present subset of
        signals over <em>any</em> finite time interval.</p>
        <blockquote>
        <p>The key insight here is that the presence invariant
        establishes a fundamental relationship between the individual
        mass contributions of signals and their cumulative, observable
        effect over a shared time interval—that is, the presence density
        they induce together over time.</p>
        </blockquote>
        <p>That this identity holds for <em>any</em> co-present subset
        over <em>any</em> finite observation window makes it a powerful
        constraint. It connects local behaviors of individual signals to
        a global, observable quantity.</p>
        <p>As we’ll see, this constraint—linking contributions from
        parts of the system to measurable properties of the whole— are
        central to how the calculus enables reasoning about the dynamics
        of a system of presences.</p>
        <h3 data-number="4.1" id="an-example-1"><span
        class="header-section-number">4.1</span> An example</h3>
        <p>To build intuition for these abstract terms, let’s look at a
        practical example.</p>
        <p>For example, suppose our signals represent revenues from
        customer purchase over some time period. If we look at a system
        of presences, across an interval of time, say a week, the total
        presence mass <span class="math inline">\(A\)</span> represents
        the total revenues across all customers who contributed to that
        revenue. <span class="math inline">\(T\)</span> is the time
        period measured in some unit of time (say days) and <span
        class="math inline">\(N\)</span> is the number of paying
        customers in that period.</p>
        <p>The average presence density is the daily revenue rate, the
        signal mass contribution for each signal is revenue for each
        customer, the average signal mass contribution is the average
        revenue per customer for that week, and the incidence rate
        represents the average daily rate of active customers over the
        week.</p>
        <p>So the presence invariant is stating that the revenue rate
        for the week is the product of the average revenue per customer
        and the average number of active customers over the week.</p>
        <h3 data-number="4.2" id="why-it-matters"><span
        class="header-section-number">4.2</span> Why it matters</h3>
        <p>While algebraically, the presence invariant is a tautology,
        it imposes a powerful constraint on system behavior—one that is
        independent of the specific system, semantics, or timescale.
        Think of it as the generalization of our intuitive revenue
        example to any arbitrary system of presences.</p>
        <p>The presence invariant is a foundational conservation law of
        the presence calculus: the <em>conservation of mass
        (contribution)</em>.</p>
        <p>Just as the conservation of energy or mass constrains the
        evolution of physical systems—regardless of the specific
        materials or forces involved—the conservation of presence mass
        constrains how observable mass is distributed over time in a
        system of presences.</p>
        <p>Thus, the conservation of mass plays a role in the presence
        calculus similar to that of other conservation laws in physics:
        it constrains the behavior of three key observable, measurable
        parameters of any system of presences.</p>
        <p>While independent of the semantics of what is being observed,
        like energy, presence mass can shift, accumulate, or
        redistribute, but its total balance across presences within a
        finite time interval remains invariant.</p>
        <p>It is important to note that this makes the “averages” in the
        presence invariant much more than descriptive statistics. While
        they may be interpreted as such, these are not simply measures
        of centrality on a set of observed presences, but quantities
        with a concrete physical interpretation, expressed via the
        invariant, that directly govern how a system of presences
        behaves in time.</p>
        <p>Exploiting this constraint allows us to study and
        characterize the long-run behavior of a system.</p>
        <h3 data-number="4.3"
        id="binary-presences-and-littles-law"><span
        class="header-section-number">4.3</span> Binary presences and
        Little’s Law</h3>
        <p>Let’s make things a bit more concrete by interpreting this
        identity in the special case of <em>binary</em> presences.</p>
        <p>Recall that a <em>binary</em> signal is a function whose
        density is either <span class="math inline">\(0\)</span> or
        <span class="math inline">\(1\)</span>. That is, we are modeling
        the presence or absence of an underlying signal in the
        domain.</p>
        <p>In this case, the <em>mass contribution</em> of a signal
        becomes an <em>element-time duration</em>. For example, if the
        signal represents the time during which a task is present in
        development, the mass contribution of that task over an
        observation interval is the portion of its duration that
        intersects the interval. This is also called the <em>residence
        time</em> <a href="#fn7" class="footnote-ref" id="fnref7"
        role="doc-noteref"><sup>7</sup></a> for the task in the
        observation window.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/presence_invariant_binary.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 6: The Presence Invariant for binary signals</code></pre>
        </div>
        </div>
        <p>Figure 6 shows possible configurations of binary signals
        intersecting a finite observation interval. Suppose the unit of
        time is days.</p>
        <p>The total presence mass accumulation <span
        class="math inline">\(A\)</span> is <span
        class="math inline">\(11\)</span> task-days. The number <span
        class="math inline">\(N\)</span> of tasks that intersect the
        observation interval is <span class="math inline">\(4\)</span>.
        The length of the observation window is <span
        class="math inline">\(T = 4\)</span> days. It is straightforward
        to verify that the presence invariant holds.</p>
        <p>Now, let’s interpret its meaning.</p>
        <p>Since each task contributes <span
        class="math inline">\(1\)</span> unit of mass for each unit of
        time it is present, the average presence density <span
        class="math inline">\(\delta=\frac{A}{T}\)</span> represents the
        <em>average number of tasks</em> present per unit time in the
        interval—denoted <span class="math inline">\(L\)</span>.</p>
        <p>Conversely, since each unit of mass corresponds to a unit of
        time associated with a task, the average mass per active signal,
        <span class="math inline">\(\bar{m} = \frac{A}{N}\)</span>, is
        the average time a task spends in the observation window. This
        value is typically called the <em>residence time</em> <span
        class="math inline">\(w\)</span> of a task in the observation
        window.</p>
        <p>The incidence rate <span class="math inline">\(\iota =
        \frac{N}{T}\)</span> may be interpreted as the <em>activation
        rate</em> of tasks in the interval—a proxy for the rate at which
        tasks start (onset) or finish (reset) within the window.</p>
        <p>For example, <span class="math inline">\(N\)</span> may be
        counted as the number of tasks that start inside the interval,
        plus the number that started before but are still active. Thus,
        <span class="math inline">\(\frac{N}{T}\)</span> is a
        <em>cumulative onset rate</em> <span
        class="math inline">\(\Lambda\)</span>.</p>
        <p>The presence invariant can now be rewritten as:</p>
        <p><span class="math display">\[ L = \Lambda \times w
        \]</span></p>
        <p>which you may recognize as <em>Little’s Law</em> in its
        finite-window form.</p>
        <p>Thus, the presence invariant serves as a <em>generalization
        of Little’s Law</em>—extending it to arbitrary systems of
        presence density functions (signals) measured over finite
        observation windows.</p>
        <p>It is important to note that we are referring to <em>Little’s
        Law over a finite observation window</em>, rather than the much
        more familiar, steady-state equilibrium form of Little’s
        Law.</p>
        <p>Unlike the equilibrium form of the law, this version holds
        <em>unconditionally</em>. The key is that the quantities
        involved are <em>observer-relative</em>: the time tasks spend
        <em>within a finite observation window</em>, and the
        <em>activation rate</em> of tasks <em>over the window</em>,
        rather than the task-relative durations or long-run
        arrival/departure rates assumed in the equilibrium form.</p>
        <p>Indeed, the difference between these two forms of the
        identity will serve as the basis for how we <em>define</em>
        whether a system of presences is in equilibrium or not. The idea
        is that the system of presences is at equilibrium when observed
        over sufficiently long observation windows such that the
        observer-relative and task-relative values of average presence
        density, incidence rate and average presence mass converge.</p>
        <p>Since real-world systems often operate far from
        equilibrium—and since the presence invariant holds
        <em>regardless</em> of equilibrium—the finite-window form
        becomes far more valuable for analyzing the long-run behavior of
        such systems as they <em>move into and between</em> equilibrium
        states.</p>
        <p>All this is the focus of section 6, where we will formally
        make the connections between the presence invariant and
        equilibrium states in systems of presence with the help of a
        very general form of Little’s Law originally proven by
        Stidham.</p>
        <h3 data-number="4.4" id="signal-dynamics"><span
        class="header-section-number">4.4</span> Signal Dynamics</h3>
        <p>Now that we’ve defined how we observe a system of presences
        over a finite time interval, we turn to what happens when we
        observe the system across a continuous sequence of
        non-overlapping intervals.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/system_presences_discrete.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 7: Sampling a system of presences across uniform intervals. </code></pre>
        </div>
        </div>
        <p>This is a fundamental construct in the presence calculus—it
        allows us to study the evolution of presence density over time:
        the <em>signal dynamics</em> of the system.</p>
        <p>The presence invariant defines a constraint among three key
        parameters measured in each interval: average presence density,
        signal incidence rate, and average mass contribution per
        signal.</p>
        <blockquote>
        <p>Given any two of these, the third is completely
        determined.<br />
        Among them, presence density is the output; incidence rate and
        average mass contribution are the inputs.</p>
        </blockquote>
        <p>At each interval, presence density can be directly
        observed—but the invariant requires that it always equal the
        product of incidence rate and average mass contribution:</p>
        <blockquote>
        <p>Any change in presence density must result from a change in
        incidence rate, average mass contribution, or both.</p>
        <p>In other words, the system has only <em>two degrees of
        freedom</em> among three interdependent variables.</p>
        </blockquote>
        <p>Because the invariant holds across <em>any</em> finite
        interval, tracking how these parameters shift reveals how a
        particular system of presences evolves.</p>
        <p>In Section 7, we’ll explore a geometric view of this idea. If
        we treat these three parameters as coordinates of the system’s
        state in each interval, we can “trace” its evolution as a
        trajectory through time.</p>
        <blockquote>
        <p>This makes the presence invariant a powerful tool for causal
        reasoning—one that helps explain <em>why</em> presence density
        changes the way it does.</p>
        </blockquote>
        <p>In the next section, we’ll introduce the <em>presence
        matrix</em>—a compact representation of the sampled signals
        shown in Figure 7. It is a key building block in the machinery
        for computing these trajectories.</p>
        <h2 data-number="5" id="the-presence-matrix"><span
        class="header-section-number">5</span> The Presence Matrix</h2>
        <p>A <em>presence matrix</em> records the presence mass
        distribution obtained by sampling a set of presence density
        functions over a consecutive set of time intervals of fixed
        size. Specifically, if we fix a time granularity—such as hours,
        days or weeks—we can construct a matrix in which:</p>
        <ul>
        <li><p><em>Rows</em> correspond to individual signals (e.g., for
        each <span class="math inline">\((
        e, b)\)</span> pair),</p></li>
        <li><p><em>Columns</em> correspond to non-overlapping time
        intervals at that fixed time granularity <em>that cover the time
        axis</em>,</p></li>
        <li><p><em>Entries</em> contain the <em>presence mass</em>,
        i.e., the integral of the density function over the
        corresponding interval:</p>
        <p><span class="math display">\[ M_{(e,b),j} =
        \int_{t_j}^{t_{j+1}} f_{(e,b)}(t) \, dt \]</span></p></li>
        </ul>
        <p>The resulting matrix provides a discrete, temporally-aligned
        representation of this system of presences.</p>
        <p>Since we are accumulating presence masses over an interval,
        the value of presence mass in a matrix entry is always a a real
        number. Figure 8 shows the presence matrix for the system in
        Figure 7.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/presence_matrix_neat.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 8: Presence Matrix for a system of presences. </code></pre>
        </div>
        </div>
        <p>In Figure 8,</p>
        <ul>
        <li>Each row in the matrix maps to a single element-boundary
        signal.</li>
        <li>Each column represents an observation window, and we are
        assuming that the signal is sampled across non-overlapping time
        intervals that cover the timeline.</li>
        <li>Each matrix entry consists of the observed presence mass of
        the signal a time interval. This is computed by taking the
        integral the underlying signal over the observation window.</li>
        </ul>
        <p>The alert reader will note the difference between the first
        two rows in the matrix. Even though the underlying signals both
        have two distinct support intervals, the first signal is
        represented by a single presence in the matrix, while the second
        is broken up into two disjoint presences.</p>
        <p>This is entirely an artifact of the granularity at which the
        signal is sampled. At a suitably fine sampling granularity, the
        first signal could also be represented by two presences.
        However, this has no real impact on the behavior of the
        invariant (see <a href="#fn8" class="footnote-ref" id="fnref8"
        role="doc-noteref"><sup>8</sup></a>).</p>
        <p>The presence matrix encodes deep structural properties of a
        system of presences. Many of key concepts we want to highlight
        are easier to define and understand in terms of this
        representation.</p>
        <h3 data-number="5.1"
        id="the-presence-invariant-and-the-presence-matrix."><span
        class="header-section-number">5.1</span> The presence invariant
        and the presence matrix.</h3>
        <p>Let’s revisit Figure 3, reproduced below, which introduced
        the idea of thinking of presence mass as a sample of an
        underlying signal.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/multiple_support.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 3: A presence as a sample of a signal over an interval.</code></pre>
        </div>
        </div>
        <p>When we observe a system of presences across a finite
        observation window, as we do in deriving the presence invariant,
        we are looking at presence mass across a “vertical” slice of
        time across all signals.</p>
        <div style="text-align: center; margin:2em">
        <table>
        <thead>
        <tr>
        <th>
        E
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td>
        e1_b1
        </td>
        <td style="background-color: #c6f6c6">
        0.3
        </td>
        <td style="background-color: #c6f6c6">
        2.3
        </td>
        <td style="background-color: #c6f6c6">
        3.4
        </td>
        <td style="background-color: #c6f6c6">
        1.1
        </td>
        <td style="background-color: #c6f6c6">
        2.9
        </td>
        <td style="background-color: #c6f6c6">
        3.2
        </td>
        <td style="background-color: #c6f6c6">
        1.1
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        </tr>
        <tr>
        <td>
        e1_b2
        </td>
        <td style="background-color: #c6f6c6">
        0.3
        </td>
        <td style="background-color: #c6f6c6">
        2.3
        </td>
        <td style="background-color: #c6f6c6">
        3.4
        </td>
        <td style="background-color: #c6f6c6">
        1.1
        </td>
        <td>
        0.0
        </td>
        <td style="background-color: #c6f6c6">
        1.1
        </td>
        <td style="background-color: #c6f6c6">
        2.2
        </td>
        <td style="background-color: #c6f6c6">
        2.4
        </td>
        <td style="background-color: #c6f6c6">
        2.3
        </td>
        <td style="background-color: #c6f6c6">
        0.8
        </td>
        </tr>
        <tr>
        <td>
        e2_b2
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td style="background-color: #c6f6c6">
        0.9
        </td>
        <td style="background-color: #c6f6c6">
        1.8
        </td>
        <td style="background-color: #c6f6c6">
        3.2
        </td>
        <td style="background-color: #c6f6c6">
        0.9
        </td>
        </tr>
        <tr>
        <td>
        e2_b1
        </td>
        <td style="background-color: #c6f6c6">
        0.8
        </td>
        <td style="background-color: #c6f6c6">
        1.3
        </td>
        <td style="background-color: #c6f6c6">
        2.4
        </td>
        <td style="background-color: #c6f6c6">
        2.8
        </td>
        <td style="background-color: #c6f6c6">
        3.0
        </td>
        <td style="background-color: #c6f6c6">
        3.2
        </td>
        <td style="background-color: #c6f6c6">
        3.4
        </td>
        <td style="background-color: #c6f6c6">
        2.4
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 9: The presence matrix from Figure 8, reproduced</code></pre>
        </div>
        </div>
        <p>The presence matrix, shown in figure 9, makes this notion
        explict - assuming some fixed granularity of observation window,
        a vertical slice in time corresponds to a column in the presence
        matrix.</p>
        <p>Further, you can see that we can construct a presence matrix
        from any subset of the rows of another presence matrix, and it
        would still be a presence matrix. What is more, the presence
        invariant applies for any <em>consecutive</em> sequence of
        columns of any such presence matrix.</p>
        <p>It’s straightforward now to interpret the presence invariant
        in terms of a presence matrix.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/matrix_invariant.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 9: Computing the invariant from the matrix.</code></pre>
        </div>
        </div>
        <p>For any observation window:</p>
        <ul>
        <li>The sums of the presence masses along the rows of the
        sub-matrix induced by the window gives us the mass contributions
        per signal for that window.</li>
        <li>The total of these row sums give us cumulative presence mass
        <span class="math inline">\(A\)</span> across all signals for
        the window.</li>
        <li>The number of active signals <span
        class="math inline">\(N\)</span> is simply the number of
        distinct rows that have non-zero values those rows in the
        induced matrix.</li>
        <li><span class="math inline">\(T\)</span> is simply the number
        of columns in the window expressed in units of the sampling
        granularity.</li>
        </ul>
        <p>Given that we can interpret <span
        class="math inline">\(A\)</span>, <span
        class="math inline">\(N\)</span> and <span
        class="math inline">\(T\)</span> as matrix properties, we can
        also derive the three parameters: presence density <span
        class="math inline">\(\delta\)</span>, signal incidence rate
        <span class="math inline">\(\iota\)</span> and average mass
        contribution per signal <span
        class="math inline">\(\bar{m}\)</span> of the presence invariant
        from this matrix for any observation window.</p>
        <p>Next, we will look at a structure that is well suited for
        analyzing these parameters <em>across</em> multiple observation
        windows.</p>
        <h3 data-number="5.2"
        id="the-presence-accumulation-matrix"><span
        class="header-section-number">5.2</span> The Presence
        Accumulation Matrix</h3>
        <p>The presence invariant encodes very strong <em>local</em>
        constraints in how presence mass distributes in time across
        signals, and we can use these to derive meaningful constraints
        on the global accumulation of presence mass across a system of
        presences.</p>
        <p>To visualize and reason about both the micro and macro
        behaviors of a system of presences in a natural way, we will
        need the help of a data structure called the Presence
        Accumulation matrix. It compresses a lot of information about
        the interaction between local and global behavior of a system of
        presences as encoded in the presence matrix, across time windows
        and time scales.</p>
        <p>It highlights clearly the fact that the presence invariant is
        scale-independent.</p>
        <p>We’ll continue with the presence matrix of Figure 9, to
        illustrate how it is constructed.</p>
        <p>Recall that the columns of the presence matrix represent time
        intervals at the finest level of granularity at which an
        underlying system of signals and presences is sampled. If we
        have an <span class="math inline">\(MxN\)</span> of <span
        class="math inline">\(M\)</span> signal sampled at <span
        class="math inline">\(N\)</span> consecutive intervals, the
        value in the presence matrix at row <span
        class="math inline">\(i\)</span> and column <span
        class="math inline">\(j\)</span> represents the sampled presence
        mass of signal <span class="math inline">\(i\)</span> at time
        interval <span class="math inline">\(j\)</span>.</p>
        <p>Consider the matrix <span class="math inline">\(A\)</span>
        that we accumulates presences masses across consecutive time
        intervals of various length between 1 and <span
        class="math inline">\(N\)</span>. This is a square matrix of
        size <span class="math inline">\(NxN\)</span> and is constructed
        as follows. For our example, this is a 10x10 matrix.</p>
        <p>The diagonal of the matrix contains the accumulated presence
        mass across each interval of length 1. This corresponds to the
        sum of each column of the presence matrix.</p>
        <div style="text-align: center; margin:2em">
        <table>
        <thead>
        <tr>
        <th>
        i\j
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td>
        1
        </td>
        <td style="background-color:#e6ffe6">
        1.4
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        2
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        3
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        9.2
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        4
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.0
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        5
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        6
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.5
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        7
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.6
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        8
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        6.6
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.5
        </td>
        </tr>
        <tr>
        <td>
        10
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        1.7
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 10: Cumulative presence mass along intervals of length 1. </code></pre>
        </div>
        </div>
        <p>The next diagonal row contains the cumulative presence mass
        for intervals of length 2 - ie <span
        class="math inline">\(A(1,2)\)</span> contains the sum of all
        entries in columns 1 and 2. <span
        class="math inline">\(A(2,3)\)</span> contains the sum of all
        entries column 2 and 3 and so on..</p>
        <div style="text-align: center; margin:2em">
        <table>
        <thead>
        <tr>
        <th>
        i\j
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td>
        1
        </td>
        <td style="background-color:#e6ffe6">
        1.4
        </td>
        <td style="background-color:#e6f0ff">
        7.3
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        2
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td style="background-color:#e6f0ff">
        15.1
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        3
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        9.2
        </td>
        <td style="background-color:#e6f0ff">
        14.2
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        4
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.0
        </td>
        <td style="background-color:#e6f0ff">
        10.9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        5
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td style="background-color:#e6f0ff">
        13.4
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        6
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.5
        </td>
        <td style="background-color:#e6f0ff">
        15.1
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        7
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.6
        </td>
        <td style="background-color:#e6f0ff">
        14.2
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        8
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        6.6
        </td>
        <td style="background-color:#e6f0ff">
        12.1
        </td>
        </tr>
        <tr>
        <td>
        9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.5
        </td>
        <td style="background-color:#e6f0ff">
        7.2
        </td>
        </tr>
        <tr>
        <td>
        10
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        1.7
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 11: Cumulative presence mass along intervals of length 2. </code></pre>
        </div>
        </div>
        We can continue filling the matrix out in diagonal order this
        way until we get the presence accumulation matrix shown below.
        <div style="text-align: center; margin:2em">
        <table>
        <thead>
        <tr>
        <th>
        i\j
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <!-- Row 1 -->
        <tr>
        <td>
        1
        </td>
        <td style="background-color:#e6ffe6">
        1.4
        </td>
        <td style="background-color:#e6f0ff">
        7.3
        </td>
        <td style="background-color:#e6ffe6">
        16.5
        </td>
        <td style="background-color:#e6f0ff">
        21.5
        </td>
        <td style="background-color:#e6ffe6">
        27.4
        </td>
        <td style="background-color:#e6f0ff">
        34.9
        </td>
        <td style="background-color:#e6ffe6">
        42.5
        </td>
        <td style="background-color:#e6f0ff">
        49.1
        </td>
        <td style="background-color:#e6ffe6">
        54.6
        </td>
        <td style="background-color:#e6f0ff">
        56.3
        </td>
        </tr>
        <!-- Row 2 -->
        <tr>
        <td>
        2
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td style="background-color:#e6f0ff">
        15.1
        </td>
        <td style="background-color:#e6ffe6">
        20.1
        </td>
        <td style="background-color:#e6f0ff">
        26.0
        </td>
        <td style="background-color:#e6ffe6">
        33.5
        </td>
        <td style="background-color:#e6f0ff">
        41.1
        </td>
        <td style="background-color:#e6ffe6">
        47.7
        </td>
        <td style="background-color:#e6f0ff">
        53.2
        </td>
        <td style="background-color:#e6ffe6">
        54.9
        </td>
        </tr>
        <!-- Row 3 -->
        <tr>
        <td>
        3
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        9.2
        </td>
        <td style="background-color:#e6f0ff">
        14.2
        </td>
        <td style="background-color:#e6ffe6">
        20.1
        </td>
        <td style="background-color:#e6f0ff">
        27.6
        </td>
        <td style="background-color:#e6ffe6">
        35.2
        </td>
        <td style="background-color:#e6f0ff">
        41.8
        </td>
        <td style="background-color:#e6ffe6">
        47.3
        </td>
        <td style="background-color:#e6f0ff">
        49.0
        </td>
        </tr>
        <!-- Row 4 -->
        <tr>
        <td>
        4
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.0
        </td>
        <td style="background-color:#e6f0ff">
        10.9
        </td>
        <td style="background-color:#e6ffe6">
        18.4
        </td>
        <td style="background-color:#e6f0ff">
        26.0
        </td>
        <td style="background-color:#e6ffe6">
        32.6
        </td>
        <td style="background-color:#e6f0ff">
        38.1
        </td>
        <td style="background-color:#e6ffe6">
        39.8
        </td>
        </tr>
        <!-- Row 5 -->
        <tr>
        <td>
        5
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td style="background-color:#e6f0ff">
        13.4
        </td>
        <td style="background-color:#e6ffe6">
        21.0
        </td>
        <td style="background-color:#e6f0ff">
        27.6
        </td>
        <td style="background-color:#e6ffe6">
        33.1
        </td>
        <td style="background-color:#e6f0ff">
        34.8
        </td>
        </tr>
        <!-- Row 6 -->
        <tr>
        <td>
        6
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.5
        </td>
        <td style="background-color:#e6f0ff">
        15.1
        </td>
        <td style="background-color:#e6ffe6">
        21.7
        </td>
        <td style="background-color:#e6f0ff">
        27.2
        </td>
        <td style="background-color:#e6ffe6">
        28.9
        </td>
        </tr>
        <!-- Row 7 -->
        <tr>
        <td>
        7
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.6
        </td>
        <td style="background-color:#e6f0ff">
        14.2
        </td>
        <td style="background-color:#e6ffe6">
        19.7
        </td>
        <td style="background-color:#e6f0ff">
        21.4
        </td>
        </tr>
        <!-- Row 8 -->
        <tr>
        <td>
        8
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        6.6
        </td>
        <td style="background-color:#e6f0ff">
        12.1
        </td>
        <td style="background-color:#e6ffe6">
        13.8
        </td>
        </tr>
        <!-- Row 9 -->
        <tr>
        <td>
        9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.5
        </td>
        <td style="background-color:#e6f0ff">
        7.2
        </td>
        </tr>
        <!-- Row 10 -->
        <tr>
        <td>
        10
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        1.7
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 12: Final presence accumulation matrix for the presence matrix of Figure 9. </code></pre>
        </div>
        </div>
        <p>We can see that in practice, this matrix compresses a large
        amount of information in a very compact form. For example if the
        columns represent weekly samples of signals a 52x52 matrix
        allows us to analyze a whole years worth of presence
        accumulation across every time-scale ranging from a single week
        to a whole year in one compact structure. We’ll shortly see why
        this is useful.</p>
        <p>First lets define the general structure of the presence
        accumulation matrix.</p>
        <p>Let <span class="math inline">\(P \in \mathbb{R}^{M \times
        N}\)</span> be the presence matrix of <span
        class="math inline">\(M\)</span> signals sampled at <span
        class="math inline">\(N\)</span> consecutive time intervals. The
        <strong>Presence Accumulation Matrix</strong> <span
        class="math inline">\(A \in \mathbb{R}^{N \times N}\)</span> is
        defined by:</p>
        <p><span class="math display">\[
        A(i, j) = \sum_{k=1}^{M} \sum_{\ell=i}^{j} P(k, \ell)
        \quad \text{for all } 1 \leq i \leq j \leq N
        \]</span></p>
        <p>That is, <span class="math inline">\(A(i,j)\)</span> gives
        the total presence mass of all signals across the interval <span
        class="math inline">\([i,j]\)</span>.</p>
        <h4 data-number="5.2.1" id="properties"><span
        class="header-section-number">5.2.1</span> Properties</h4>
        <ul>
        <li><span class="math inline">\(A\)</span> is upper triangular:
        <span class="math inline">\(A(i,j)\)</span> is defined only when
        <span class="math inline">\(i \leq j\)</span>.</li>
        <li>The diagonal entries <span
        class="math inline">\(A(i,i)\)</span> equal the column sums of
        <span class="math inline">\(P\)</span>.</li>
        <li>Each entry <span class="math inline">\(A(i,j)\)</span>
        reflects the cumulative presence mass over the interval <span
        class="math inline">\([i,j]\)</span> in the original presence
        matrix.</li>
        </ul>
        <p>As we will see below, this matrix compactly encodes
        multi-scale information about system behavior and supports the
        analysis of both micro and macro scale behavior of a system of
        presences.</p>
        <h2 data-number="6" id="applying-the-presence-calculus"><span
        class="header-section-number">6</span> Applying the Presence
        Calculus</h2>
        <p>The presence calculus might seem like a highly abstract,
        theoretical framework, but much of its utility emerges when we
        <em>interpret</em> its concepts in a specific, applied
        context.</p>
        <p>Concepts such as presence mass, incidence rates, and density
        are not unlike abstract physical notions like force, mass, and
        acceleration. In principle, these are measurable quantities that
        nature constrains to behave in prescribed ways at a micro
        scale.</p>
        <p>Once we understand the rules governing their micro-scale
        behavior, we gain tools to systematically measure, reason about,
        and explain a vast range of observable macro-scale phenomena.
        Much of physics is built on this principle.</p>
        <p>In a similar vein, the presence calculus—and especially the
        <em>presence invariant</em> —provides a foundational law that
        governs the local, time-based behavior of any system composed of
        time-varying signals and the presences they induce.</p>
        <p>Once we recognize that such a governing constraint exists,
        the presence calculus equips us with tools to describe,
        interpret, explain, and, in certain cases,<br />
        make verifiable predictions about the macro-scale behavior of
        these systems.</p>
        <p>Newtonian mechanics, for example, allows us to describe and
        predict the motion of physical systems with remarkable
        precision—such as planetary orbits or the paths of falling
        objects. Yet even within this well-established framework,
        certain limits remain: the general three-body problem has no
        closed-form solution, and systems like the double pendulum
        exhibit chaotic behavior that defies long-term prediction.</p>
        <p>Still, we can represent the behavior and evolutions of such
        systems as deterministic trajectories through a parameter space,
        uncovering structure even where precise global behavior remain
        unpredictable. In much the same way, the presence calculus does
        not seek to forecast the exact evolution of systems of
        presences. Instead, by explicitly modeling signal histories and
        representing element trajectories over time, it equips us with
        powerful descriptive and tools to explain how these systems
        evolved retrospectively.</p>
        <p>In this way, structural constraints and local invariants help
        us interpret locally observed dynamics and connect it to system
        behavior at the macro scale.</p>
        <p>Let’s see how.</p>
        <h3 data-number="6.1"
        id="equilibrium-convergence-and-divergence"><span
        class="header-section-number">6.1</span> Equilibrium:
        Convergence and Divergence</h3>
        Consider the highlighted portions of the accumulation matrix
        <span class="math inline">\(A\)</span> in figure 13.
        <div style="text-align: center; margin:2em">
        <table>
        <thead>
        <tr>
        <th>
        i\j
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td>
        1
        </td>
        <td style="background-color:#e6ffe6">
        1.4
        </td>
        <td style="background-color:#ffffcc">
        7.3
        </td>
        <td style="background-color:#ffffcc">
        16.5
        </td>
        <td style="background-color:#ffffcc">
        21.5
        </td>
        <td style="background-color:#ffffcc">
        27.4
        </td>
        <td style="background-color:#ffffcc">
        34.9
        </td>
        <td style="background-color:#ffffcc">
        42.5
        </td>
        <td style="background-color:#ffffcc">
        49.1
        </td>
        <td style="background-color:#ffffcc">
        54.6
        </td>
        <td style="background-color:#ffffcc">
        56.3
        </td>
        </tr>
        <tr>
        <td>
        2
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        3
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        9.2
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        4
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.0
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        5
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        6
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.5
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        7
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.6
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        8
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        6.6
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.5
        </td>
        </tr>
        <tr>
        <td>
        10
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        1.7
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 13: Diagonal and top row of the accumulation matrix</code></pre>
        </div>
        </div>
        <p>Each diagonal entry represents the total presence mass
        observed across all signals in a single time interval. Thus, the
        diagonal traces the discrete-time evolution of the system’s
        observable activity—one step at a time. We will call the
        sequence of values on the diagonal a <em>sample path</em> for
        the system of presences.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/diagonal_values.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 14: The values on the diagonal of the accumulation matrix.  </code></pre>
        </div>
        </div>
        <p>Recall that each entry on the diagonal represents the sum of
        the presence masses of the signals active in a time window. This
        implies that each entry on the top row is an approximation of an
        integral<a href="#fn9" class="footnote-ref" id="fnref9"
        role="doc-noteref"><sup>9</sup></a> and equals the <em>area
        under the sample path</em> represented by the diagonal.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/top_row_values.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 15: The values on the top row accumulation matrix.  </code></pre>
        </div>
        </div>
        <p>In other words, the diagonal and top row represent
        fundamentally different views of the system: the diagonal
        captures the <em>micro-level behavior</em>—instantaneous<br />
        presence mass across signals in each interval—while the top row
        encodes the<br />
        <em>macro-level behavior</em>—the cumulative effect of those
        presences over time.<br />
        Both views are compactly encoded in the structure of the<br />
        accumulation matrix.</p>
        <p>Figure 16 shows this geometric interpretation of the diagonal
        and top rows of the accumulation matrix.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/sample_path_area.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 16: Sample path and the area under it.   </code></pre>
        </div>
        </div>
        <p>If we divide each of the entries in the accumulation matrix
        by the length of the time interval it covers, we get the
        presence density for each interval. For the diagonal interval
        has length 1 (time unit) and for the top row the lengths range
        from 1 to <span class="math inline">\(N-1.\)</span></p>
        <div style="text-align: center; margin:2em">
        <table style="border-collapse: collapse;">
        <thead>
        <tr>
        <th>
        i\j
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td>
        1
        </td>
        <td style="background-color:#e6ffe6">
        1.4
        </td>
        <td style="background-color:#ffffcc">
        3.6
        </td>
        <td style="background-color:#ffffcc">
        5.5
        </td>
        <td style="background-color:#ffffcc">
        5.4
        </td>
        <td style="background-color:#ffffcc">
        5.5
        </td>
        <td style="background-color:#ffffcc">
        5.8
        </td>
        <td style="background-color:#ffffcc">
        6.1
        </td>
        <td style="background-color:#ffffcc">
        6.1
        </td>
        <td style="background-color:#ffffcc">
        6.1
        </td>
        <td style="background-color:#ffffcc">
        5.6
        </td>
        </tr>
        <tr>
        <td>
        2
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        3
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        9.2
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        4
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.0
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        5
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        6
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.5
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        7
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.6
        </td>
        <td>
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        8
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        6.6
        </td>
        <td>
        </td>
        </tr>
        <tr>
        <td>
        9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.5
        </td>
        </tr>
        <tr>
        <td>
        10
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        1.7
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 17: Presence density: diagonal and top row.   </code></pre>
        </div>
        </div>
        <p>So now we have the left-hand side of the presence invariant
        encoded in matrix<br />
        form for every pair of continuous intervals in the system.</p>
        <p>Let’s chart the values in both of these rows in the
        matrix.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/first_row_vs_main_diagonal.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 18: Convergence of long-run average presence density.</code></pre>
        </div>
        </div>
        <p>We can see that while the values on the sample path are
        volatile, the values on the top row converge towards a finite
        value. We can define this notion of convergence precisely using
        the mathematical concept of a limit. Let:</p>
        <p><span class="math display">\[
        \Delta = \lim_{T \to \infty} \frac{1}{T} \int_0^T \delta(t) \,
        dt
        \]</span></p>
        <p>Here, <span class="math inline">\(\delta(t)\)</span> is the
        instantaneous presence density introduced in the presence
        invariant.<br />
        The quantity <span class="math inline">\(\Delta\)</span>, by
        contrast, represents the long-run average presence density
        —<br />
        the time-averaged total presence across all signals in the
        system.</p>
        <p>In the discrete setting, this corresponds to the limiting
        value of the top row of the<br />
        accumulation matrix — a value toward which the green line in
        Figure 18 appears to converge:</p>
        <p><span class="math display">\[
        \Delta = \lim_{j \to \infty} \frac{A(1,j)}{j}
        \]</span></p>
        <p>where <span class="math inline">\(A(1,j)\)</span> is the
        total presence mass accumulated from time 0 through interval
        <span class="math inline">\(j\)</span>.</p>
        <p>Not every system of presences has such a limit. We call a
        system <em>convergent</em> if<br />
        <span class="math inline">\(\Delta\)</span> exists and is
        finite, and <em>divergent</em> otherwise.</p>
        <p>To summarize: in a convergent system, the average presence
        density over time<br />
        settles toward a finite value. Intuitively, this means that
        after observing<br />
        enough of the system’s history, additional observation does not
        significantly<br />
        alter our understanding of its long-term behavior.</p>
        <p>Some systems converge rapidly to a single stable limit.
        Others may not settle<br />
        at all, but instead move among a small number of such limits.
        These<br />
        represent dominant behavioral modes — quasi-equilibrium states
        that the system<br />
        can enter and sustain for extended periods. Such behavior is
        called<br />
        <em>metastable</em> or <em>multi-modal</em>.</p>
        <p>Divergence, by contrast, implies the absence of such limiting
        behavior. The<br />
        presence density in these systems continues to grow without
        bound,<br />
        indicating that no stable long-term pattern emerges.</p>
        <p>Figure 19 shows examples of each of these behaviors.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/convergence_divergence.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 19: Convergent, Divergent, and Metastable behavior in systems of presences.</code></pre>
        </div>
        </div>
        <p>If a limit <span class="math inline">\(\Delta\)</span> exists
        and is sustained over time, it signifies a stable long-run
        average presence density for the system. This value represents a
        specific pattern of average behavior toward which the system’s
        observable presence density gravitates over extended periods,
        regardless of short-term fluctuations.</p>
        <p>This concept aligns with the broader notion of attractors in
        dynamical systems. While a system’s full, high-dimensional state
        might exhibit complex dynamics, the long-run average presence
        density can itself stabilize around a particular value or set of
        values. When the presence density consistently settles around
        such a limit, it indicates that the system’s observable behavior
        has entered a sustainable regime.</p>
        <p>This provides a powerful way to characterize the system’s
        overall operational modes in the long term, even if its
        micro-level details remain complex and unpredictable. We will
        have more to say on this topic shortly.</p>
        <p>It is important to emphasize that convergence and divergence
        are properties<br />
        of the <em>observed long-run behavior</em> of a system of
        presences — not intrinsic<br />
        properties of the underlying system itself.</p>
        <p>We cannot infer the system’s nature solely from whether it
        appears convergent or divergent at any given time. We can only
        observe the dynamics of presence<br />
        over time and assess whether they exhibit convergence.</p>
        <p>The key difference between convergence and the other two
        modes is that a<br />
        convergent system can effectively <em>forget</em> its history
        beyond the point of stabilization.Its future behavior becomes
        representative of its past, allowing the system to be
        characterized by a stable long-run average.</p>
        <p>Among other things, the presence calculus equips us with the
        computational tools needed to identify convergent, divergent and
        metastable states of a system of presences.</p>
        <h4 data-number="6.1.1" id="the-semantics-of-convergence"><span
        class="header-section-number">6.1.1</span> The Semantics of
        Convergence</h4>
        <p>An important point to emphasize is that, depending on how
        presence density<br />
        is interpreted in a given domain, <em>any</em> of the three
        behavioral modes —<br />
        convergent, divergent, or metastable — may be desirable.<br />
        Convergence is not inherently “good,” nor is divergence
        necessarily “bad.”</p>
        <p>For example, in repetitive manufacturing or many customer
        service domains,<br />
        convergence is often desired. In these contexts, presence
        typically<br />
        represents <em>demand</em> on a constrained resource, and
        keeping that demand<br />
        stable is essential for ensuring consistent service times,
        throughput,<br />
        and resource utilization. Traditional operations management and
        queueing theory therefore seek out — and emphasize — stability
        and convergence in key<br />
        operational signals.</p>
        <p>By contrast, if presence represents a company’s <em>customer
        base</em> or <em>market<br />
        share</em>, we <em>want</em> the long-run presence density to
        look like the chart on<br />
        the right: up and to the right — that is,
        <em>divergent</em>.</p>
        <p>Metastable modes are common and often highly desirable in
        software<br />
        development, where teams must shift between modes of operation
        in<br />
        response to external demands or changing market conditions.
        Indeed, the<br />
        ability to transition between such modes effectively is often a
        hallmark of a high-functioning, adaptive organization —
        <em>provided</em> it is done intentionally and with
        awareness.</p>
        <p>All too often, however, organizations drift from metastable
        to chaotic<br />
        behavior, losing the capacity to stabilize in any mode. This
        often results<br />
        from a lack of visibility into — or the inability to reason
        about — their<br />
        operational state.</p>
        <p>One of the major practical applications of the presence
        calculus is to<br />
        bring new analytical tools to <em>observe</em>,
        <em>categorize</em>, and <em>steer</em> the<br />
        behavior of such systems — aligning them with the desired modes
        of<br />
        operation in a given domain, <em>before</em> critical tipping
        points are reached.</p>
        <h3 data-number="6.2" id="detecting-convergence"><span
        class="header-section-number">6.2</span> Detecting
        Convergence</h3>
        <p>In the last section, we <em>defined</em> convergent behavior
        in terms of the<br />
        existence of the limit <span
        class="math inline">\(\Delta\)</span>, the long-run average
        presence density<br />
        of the system.</p>
        <p>Now we ask: under what observable conditions does such a
        limit exist?</p>
        <p>If we can identify these conditions, we gain levers to begin
        <em>steering</em><br />
        systems toward desired modes of operation.</p>
        <p>It turns out the answer is hiding in plain sight — in the
        presence<br />
        invariant, which, as we’ve seen, holds for <em>any</em> finite
        observation<br />
        window. The limit <span class="math inline">\(\Delta\)</span>
        represents the asymptotic average of <span
        class="math inline">\(\delta(t)\)</span>,<br />
        the left-hand side of the invariant, measured over a sequence of
        consecutive<br />
        overlapping intervals, each one a prefix of the sample path.</p>
        <p>For each such prefix interval <span
        class="math inline">\(t\)</span> , the <em>presence
        invariant</em> gives us:</p>
        <p><span class="math display">\[
        \delta(t) = \iota(t) \times \bar{m}(t)
        \]</span></p>
        <p>This tells us that each value of <span
        class="math inline">\(\delta(t)\)</span> is determined by
        the<br />
        product of two measurable quantities: <span
        class="math inline">\(\iota(t)\)</span>, the incidence
        rate,<br />
        and <span class="math inline">\(\bar{m}(t)\)</span>, the average
        mass contribution per signal.</p>
        <p>To understand when the long-run average of <span
        class="math inline">\(\delta(t)\)</span> converges, we can
        ask<br />
        a simpler question: do the corresponding long-run averages of
        <span class="math inline">\(\iota(t)\)</span><br />
        and <span class="math inline">\(\bar{m}(t)\)</span> converge? If
        both do, we should expect that their product —<br />
        and hence <span class="math inline">\(\Delta\)</span> —
        converges as well,and it does, with some technical conditions in
        place<a href="#fn10" class="footnote-ref" id="fnref10"
        role="doc-noteref"><sup>10</sup></a>.</p>
        <p>So, let’s write down precise definitions for the limits of
        <span class="math inline">\(\iota(t)\)</span> and<br />
        <span class="math inline">\(\bar{m}(t)\)</span> and examine how
        these limits behave.</p>
        <h4 data-number="6.2.1"
        id="convergence-of-average-signal-mass-contribution"><span
        class="header-section-number">6.2.1</span> Convergence of
        Average Signal Mass Contribution</h4>
        <p>We will derive the limit for <span
        class="math inline">\(\bar{m}\)</span>. We will denote this by
        <span class="math inline">\(\bar{M}\)</span>.</p>
        <p><span class="math display">\[
        \bar{M} = \lim_{T \to \infty} \frac{1}{N(0,T)} \sum_{(e,b)}
        \int_0^T P_{(e,b)}(t) \, dt
        \]</span></p>
        <p>This expression means: for each signal <span
        class="math inline">\((e,b)\)</span>, accumulate its total
        presence mass over time, then sum across all signals, and divide
        by the total number of signals active during that window. Each
        integral in the sum is a row sum in the original presence matrix
        - the mass contribution of an individual signal over the
        interval.</p>
        <p>Thus we can also write this as</p>
        <p><span class="math display">\[
        \bar{M} = \lim_{j \to \infty} \frac{1}{N(1,j)} \sum_{(e,b)}
        \sum_{k=1}^j P_{(e,b)}(k)
        \]</span></p>
        <p><span class="math inline">\(\bar{M}\)</span> is the limit of
        average mass contribution per signal over a sufficiently long
        observation interval. Here, <span
        class="math inline">\(P_{(e,b)}(t)\)</span> is the presence
        density function for signal <span
        class="math inline">\((e,b)\)</span>, and <span
        class="math inline">\(N(0,T)\)</span> is the total number of
        signals observed during the interval <span
        class="math inline">\([0,T]\)</span>.</p>
        <p>Let’s work this out for our running example and see what it
        means. We’ll reproduce Figure 9, our starting presence matrix,
        here for easy reference.</p>
        <div style="text-align: center; margin:2em">
        <table>
        <thead>
        <tr>
        <th>
        E
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td>
        e1_b1
        </td>
        <td style="background-color: #c6f6c6">
        0.3
        </td>
        <td style="background-color: #c6f6c6">
        2.3
        </td>
        <td style="background-color: #c6f6c6">
        3.4
        </td>
        <td style="background-color: #c6f6c6">
        1.1
        </td>
        <td style="background-color: #c6f6c6">
        2.9
        </td>
        <td style="background-color: #c6f6c6">
        3.2
        </td>
        <td style="background-color: #c6f6c6">
        1.1
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        </tr>
        <tr>
        <td>
        e1_b2
        </td>
        <td style="background-color: #c6f6c6">
        0.3
        </td>
        <td style="background-color: #c6f6c6">
        2.3
        </td>
        <td style="background-color: #c6f6c6">
        3.4
        </td>
        <td style="background-color: #c6f6c6">
        1.1
        </td>
        <td>
        0.0
        </td>
        <td style="background-color: #c6f6c6">
        1.1
        </td>
        <td style="background-color: #c6f6c6">
        2.2
        </td>
        <td style="background-color: #c6f6c6">
        2.4
        </td>
        <td style="background-color: #c6f6c6">
        2.3
        </td>
        <td style="background-color: #c6f6c6">
        0.8
        </td>
        </tr>
        <tr>
        <td>
        e2_b2
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td style="background-color: #c6f6c6">
        0.9
        </td>
        <td style="background-color: #c6f6c6">
        1.8
        </td>
        <td style="background-color: #c6f6c6">
        3.2
        </td>
        <td style="background-color: #c6f6c6">
        0.9
        </td>
        </tr>
        <tr>
        <td>
        e2_b1
        </td>
        <td style="background-color: #c6f6c6">
        0.8
        </td>
        <td style="background-color: #c6f6c6">
        1.3
        </td>
        <td style="background-color: #c6f6c6">
        2.4
        </td>
        <td style="background-color: #c6f6c6">
        2.8
        </td>
        <td style="background-color: #c6f6c6">
        3.0
        </td>
        <td style="background-color: #c6f6c6">
        3.2
        </td>
        <td style="background-color: #c6f6c6">
        3.4
        </td>
        <td style="background-color: #c6f6c6">
        2.4
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 9: The presence matrix from Figure 8, reproduced</code></pre>
        </div>
        </div>
        <p>The cumulative mass per signal over each interval <span
        class="math inline">\([1, j], j \le 10\)</span> is shown below.
        Each value in this matrix is the sum of all the values in that
        row to the left (inclusive) of the value.</p>
        <div style="text-align: center; margin:2em">
        <table>
        <thead>
        <tr>
        <th>
        E
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td>
        e1_b1
        </td>
        <td>
        0.3
        </td>
        <td>
        2.6
        </td>
        <td>
        6.0
        </td>
        <td>
        7.1
        </td>
        <td>
        10.0
        </td>
        <td>
        13.2
        </td>
        <td>
        14.3
        </td>
        <td>
        14.3
        </td>
        <td>
        14.3
        </td>
        <td>
        14.3
        </td>
        </tr>
        <tr>
        <td>
        e1_b2
        </td>
        <td>
        0.3
        </td>
        <td>
        2.6
        </td>
        <td>
        6.0
        </td>
        <td>
        7.1
        </td>
        <td>
        7.1
        </td>
        <td>
        8.2
        </td>
        <td>
        10.4
        </td>
        <td>
        12.8
        </td>
        <td>
        15.1
        </td>
        <td>
        15.9
        </td>
        </tr>
        <tr>
        <td>
        e2_b2
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.0
        </td>
        <td>
        0.9
        </td>
        <td>
        2.7
        </td>
        <td>
        5.9
        </td>
        <td>
        6.8
        </td>
        </tr>
        <tr>
        <td>
        e2_b1
        </td>
        <td>
        0.8
        </td>
        <td>
        2.1
        </td>
        <td>
        4.5
        </td>
        <td>
        7.3
        </td>
        <td>
        10.3
        </td>
        <td>
        13.5
        </td>
        <td>
        16.9
        </td>
        <td>
        19.3
        </td>
        <td>
        19.3
        </td>
        <td>
        19.3
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 20: The cumulative mass contribution matrix for the presence matrix.</code></pre>
        </div>
        </div>
        <p>Now lets chart each row of this matrix to see how this
        cumulative mass grows over time. Here we are showing each row in
        the matrix as a line in the chart.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/mass_contribution_per_signal.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 21: Mass contribution per signal</code></pre>
        </div>
        </div>
        <p>Finally figure 22 shows the cumulative average of the mass
        contribution. Each point in this chart represents the cumulative
        average of the mass contributions for the window <span
        class="math inline">\([1,j]\)</span> which is the sum of the
        value in column j divided by the number of non-zero rows in the
        sub-matrix spanned by the columns in $[1,j]</p>
        <p>As we can see, this curve converges to a limit.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/avg_mass_contribution_per_signal.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 22: Average Mass contribution per signal</code></pre>
        </div>
        </div>
        <p>So lets ask, what would make the average mass contribution
        <em>not</em> converge to a finite value?</p>
        <p>Figure 21 suggests that the mass contribution of every
        individual signal is monotonically non-decreasing and it
        increases continuously over every non-zero support interval of
        the signal and flattens out over every interval where the
        underlying signal is zero.</p>
        <p>Suppose when measured over a sufficient long interval, each
        signal remains bounded, that is every onset is matched with a
        corresponding reset, then each individual signal contributes a
        finite mass to the cumulative average.</p>
        <p>Thus, the only way the cumulative average mass can grow
        without limit is if <em>some</em> signal grows without
        limit.</p>
        <p>For example, in figure 3 we show some onset-reset patterns
        for signals and the last signal in Figure 3, which has an onset
        but no apparent reset within the observation window, would grow
        without limit in Figure 21 if there was no reset.</p>
        <p>This gives us the first condition for convergence of <span
        class="math inline">\(\Delta\)</span> :</p>
        <div
        style="border: 1px solid #ccc; border-radius: 6px; padding: 1em; background-color: #f9f9f9; margin: 2em 0;">
        <p><strong>Boundedness of Signal Mass</strong></p>
        <p><em>In a convergent system of presences, every signal onset
        is eventually followed by a corresponding reset, when observed
        over a sufficiently long time interval.</em></p>
        </div>
        <p>We’ll note, once again, that depending upon the semantics of
        the domain, we may or may not want to have this condition hold
        depending upon whether we are looking to steer the system
        towards convergence or towards divergence.</p>
        <p>For example, if a signal represents a new revenue source, a
        mass contribution represents incremental revenues and ideally we
        want many onsets without matching resets: every reset
        corresponds to a lost revenue stream.</p>
        <p>If, on the other hand, a signal onset represents a new
        unfinished task on your to-do list, then a reset marks its
        completion — and convergence becomes desirable, as it indicates
        tasks are being completed in a timely manner and that your todo
        list is not growing without limit.</p>
        <h4 data-number="6.2.2" id="convergence-of-incidence-rate"><span
        class="header-section-number">6.2.2</span> Convergence of
        Incidence Rate</h4>
        <p>We now define the long-run incidence rate, denoted <span
        class="math inline">\(I\)</span>, in exact analogy to how we
        defined the average mass contribution per signal <span
        class="math inline">\(\bar{M}\)</span>. Recall that <span
        class="math inline">\(\iota(t)\)</span> is the incidence rate
        observed over the interval <span class="math inline">\([0,
        t]\)</span>, defined as the number of signals observed in that
        interval divided by its duration. Then we define the long-run
        incidence rate as the following limit:</p>
        <p><span class="math display">\[
        I = \lim_{T \to \infty} \iota(T) = \lim_{T \to \infty}
        \frac{N(0, T)}{T}
        \]</span></p>
        <p>where <span class="math inline">\(N(0, T)\)</span> is the
        number of signals observed over the interval <span
        class="math inline">\([0, T]\)</span>— that is, the number of
        distinct element-boundary signals that are active at some point
        during the interval. The incidence rate measures how many such
        signals are activated, on average, per unit time.</p>
        <p>This limit <span class="math inline">\(I\)</span>, when it
        exists, represents the asymptotic rate at which signals appear
        in the system. It plays a symmetric role to <span
        class="math inline">\(\bar{M}\)</span> in the convergence of the
        presence density, and its existence is the second key condition
        we will examine next.</p>
        <p>To better understand the behavior of the incidence rate <span
        class="math inline">\(\iota(T)\)</span>, let’s now examine the
        cumulative signal count <span
        class="math inline">\(N(0,T)\)</span> over the interval <span
        class="math inline">\([0,T]\)</span> for increasing values of
        <span class="math inline">\(T\)</span>. This is directly
        analogous to how we analyzed the growth of cumulative mass
        contributions per signal when analyzing <span
        class="math inline">\(\bar{M}\)</span>.</p>
        <p>Recall that for a given observation window <span
        class="math inline">\([0,T]\)</span>, <span
        class="math inline">\(N(0,T)\)</span> counts the number of
        element-boundary signals that are active at some point within
        the interval. We can compute this by scanning across the
        presence matrix and, for each column (from <span
        class="math inline">\(t=1\)</span> to <span
        class="math inline">\(t=T\)</span>), counting how many
        <em>new</em> signals appear—that is, how many unique
        element-boundary rows have non-zero values in that column.</p>
        <p>The result is a sequence of signal counts, which we can
        arrange as a <span class="math inline">\(1 \times T\)</span> row
        vector:</p>
        <div style="text-align: center; margin:2em">
        <table>
        <thead>
        <tr>
        <th>
        Time
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td>
        Signal Count
        </td>
        <td>
        3
        </td>
        <td>
        3
        </td>
        <td>
        3
        </td>
        <td>
        3
        </td>
        <td>
        3
        </td>
        <td>
        3
        </td>
        <td>
        4
        </td>
        <td>
        4
        </td>
        <td>
        4
        </td>
        <td>
        4
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 23: Cumulative count of distinct signals observed over the interval $[0, T]$.</code></pre>
        </div>
        </div>
        <p>where <span class="math inline">\(n_j\)</span> is the total
        number of distinct signals that have appeared at or before time
        <span class="math inline">\(j\)</span>. Each <span
        class="math inline">\(n_j\)</span> counts the number of signals
        with support intersecting the interval <span
        class="math inline">\([0, j]\)</span>.</p>
        <p>We can chart this row to visualize how the cumulative number
        of observed signals grows over time. If <span
        class="math inline">\(N(0,T)\)</span> grows linearly in <span
        class="math inline">\(T\)</span>, then the incidence rate <span
        class="math inline">\(\iota(T) = N(0,T)/T\)</span> should
        converge to a finite value <span
        class="math inline">\(I\)</span>. On the other hand, if <span
        class="math inline">\(N(0,T)\)</span> grows faster than
        linearly, the incidence rate will diverge—and if it grows
        sublinearly, the rate will decay toward zero.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/avg_incidence_rate.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 24: Average signal incidence rate </code></pre>
        </div>
        </div>
        <p>Figure 24 shows the incidence rate <span
        class="math inline">\(\iota(T) = N(0,T)/T\)</span> over time. In
        this example, the rate initially decreases and then stabilizes,
        since the number of distinct signals <span
        class="math inline">\(N(0,T)\)</span> stops increasing after a
        point. In general, the incidence rate will converge to a finite
        limit if <span class="math inline">\(N(0,T)\)</span> grows no
        faster than linearly with <span
        class="math inline">\(T\)</span>. If <span
        class="math inline">\(N(0,T)\)</span> grows <em>faster</em> than
        <span class="math inline">\(T\)</span>, the ratio <span
        class="math inline">\(\iota(T)\)</span> will diverge—indicating
        that signals are being activated at an unbounded rate.
        Conversely, if <span class="math inline">\(N(0,T)\)</span> grows
        <em>slower</em> than <span class="math inline">\(T\)</span>, the
        incidence rate will decay toward zero. Thus, convergence of
        <span class="math inline">\(\iota(T)\)</span> requires that
        <span class="math inline">\(N(0,T)\)</span> grows approximately
        linearly in <span class="math inline">\(T\)</span>.</p>
        <p>This kind of divergence typically arises in systems where the
        <em>onset rate</em>— the rate at which new signals are
        activated—exceeds the <em>reset rate</em>, which closes those
        signals. While transient imbalances between onsets and resets
        are common during transitions between equilibrium states,
        divergence only occurs if this imbalance is sustained
        indefinitely. In that case, <span
        class="math inline">\(N(0,T)\)</span> grows without bound
        relative to <span class="math inline">\(T\)</span>, and the
        system exhibits an asymptotically increasing incidence rate. So,
        divergence of <span class="math inline">\(\iota(T)\)</span>
        directly reflects a persistent structural imbalance between
        signal onsets and resets over time.</p>
        <div
        style="border: 1px solid #ccc; border-radius: 6px; padding: 1em; background-color: #f9f9f9; margin: 2em 0;">
        <p><b>Boundedness of incidence rate</b></p>
        <p><i> In a convergent system of presences, the long-run rate of
        signal onsets does not exceed the long-run rate of resets, when
        observed over a sufficiently long time interval. </i></p>
        </div>
        <h4 data-number="6.2.3" id="recap"><span
        class="header-section-number">6.2.3</span> Recap</h4>
        <p>We began by defining convergence in terms of the existence of
        a long-run limit for average presence density.</p>
        <p>We then showed how the existence of this global limit depends
        on the existence of two other measurable limits: the average
        incidence rate of signals and their average mass
        contribution.</p>
        <p>Next, we traced each of these limits back to the local
        behavior of individual signals—specifically, the presence or
        absence of well-behaved signal onsets and resets.</p>
        <p>With this connection in place, we now have a principled way
        to reason about the global convergence or divergence of a system
        by analyzing the patterns of local signal behavior over time. In
        the next section, we’ll see how to apply this principle in
        practice.</p>
        <h4 data-number="6.2.4"
        id="formal-proof-of-convergence-and-littles-law"><span
        class="header-section-number">6.2.4</span> Formal Proof of
        Convergence and Little’s Law</h4>
        <p>In this document, we have presented an accurate—though
        somewhat simplified— account of the criteria required to ensure
        that a system of presences is convergent. Specifically, based on
        the definitions above, we assert that for a given system of
        presences, if the limits <span class="math inline">\(I\)</span>
        and <span class="math inline">\(\bar{M}\)</span> exist and are
        finite, then the limit <span
        class="math inline">\(\Delta\)</span> also exists and is finite.
        Furthermore, we claim that</p>
        <p><span class="math display">\[
        \Delta = I \times \bar{M}
        \]</span></p>
        <p>Technically, this relationship does not follow automatically
        from the arguments we have presented so far. In fact, the
        statement above is a restatement of a generalized form of
        Little’s Law originally proven by Brumelle, and later by Heyman
        and Stidham. The full proof—along with the additional technical
        conditions required to ensure that the limit of the product
        equals the product of the limits—is beyond the scope of this
        document.</p>
        <p>For our purposes, it is safe to state that this relationship,
        and the conditions under which it holds, <em>constitute</em> the
        general form of Little’s Law for a system of presences.</p>
        <p>With the exception of certain carefully constructed
        pathological cases, the criteria we have outlined—bounded signal
        mass and balanced onset/reset rates—will typically suffice to
        determine whether a system is convergent or divergent.</p>
        <h4 data-number="6.2.5"
        id="convergence-coherence-and-littles-law"><span
        class="header-section-number">6.2.5</span> Convergence,
        Coherence, and Little’s Law</h4>
        <p>One important point to note is that “Little’s Law” is not a
        single law, but rather a family of related laws that apply at
        different time scales, in different forms, and with different
        interpretations. The presence invariant is the most general
        version of this law.</p>
        <p>In this document, we stated it as a relationship between
        average presence density, signal incidence rate, and average
        mass contribution per signal over any finite observation window.
        This relationship holds at all time scales, <em>including</em>
        those sufficiently long windows where the limits <span
        class="math inline">\(\Delta\)</span>, <span
        class="math inline">\(I\)</span>, and <span
        class="math inline">\(\bar{M}\)</span> exist.</p>
        <p>It is natural to ask: what is special, if anything, about
        those limiting values?</p>
        <p>Without going too deeply into technical arguments here, we
        note that the limits are indeed special. When a system is
        observed over a non-convergent interval, the quantities in the
        presence invariant are dominated by <em>partial</em> mass
        contributions from signals that have not yet completed. Figure 5
        shows an example of this behavior.</p>
        <p>The system may <em>appear</em> convergent when the window is
        long enough for <em>complete</em> signals to dominate the
        averages. For example, if we extended the window in Figure 5 to
        include the full duration of each signal, the system would
        appear convergent over that interval.</p>
        <p>The important point here is that in such situations, the
        presence invariant is not just a relationship about mass
        <em>contributions</em>, but also implicitly a relationship about
        the <em>masses</em> of the signals involved. This distinction
        has direct operational implications.</p>
        <p>For instance, if the signals in Figure 5 represent customer
        service times, then over a convergent interval, mass
        contributions reflect what the customer actually experiences.
        But over shorter, non-convergent intervals, those same
        contributions primarily reflect what a system operator observes
        on a day-to-day basis.</p>
        <p>In Figure 5, for example, if we extend the observation window
        far enough to include the full duration of all signals shown,
        the system will appear convergent over that interval. When we
        measure presence density, signal masses, and incidence rates
        over this longer window, we are implicitly aligning the
        customer’s perspective of the system with the operator’s
        perspective.</p>
        <p>In this way, convergence brings these two perspectives—the
        customer’s and the operator’s—into alignment. More generally, it
        aligns time-based averages with signal-based averages. When
        these quantities agree, we enter a state of epistemic coherence:
        multiple observers, using different vantage points, arrive at
        consistent measurements of system behavior.</p>
        <p>This alignment occurs only when the system is operating at or
        near equilibrium.</p>
        <p>We will return to this important idea in other posts,
        particularly in the context of flow measurement in systems that
        operate far from equilibrium. But for now, it is enough to
        recognize that identifying whether a system is operating in a
        convergent or divergent mode is fundamental to making meaningful
        decisions when reasoning about a system of presences.</p>
        <h4 data-number="6.2.6" id="a-note-on-determinism"><span
        class="header-section-number">6.2.6</span> A Note on
        Determinism</h4>
        <p>A final point we emphasize in this section is that the form
        of Little’s Law derived here is entirely deterministic. It does
        not depend on any probabilistic or stochastic assumptions about
        the behavior of the system. In fact, the notion of a sample path
        used here originates in a deterministic proof of Little’s Law by
        Stidham in 1972. The presence invariant, as we have introduced
        it, is a direct analogue of the finite-window constructs used in
        that proof.</p>
        <p>It is important to recognize that Little’s Law is not a
        statistical artifact. It is a structural property deeply woven
        into the behavior of of <em>any</em> system of presences.
        Convergence and divergence are deterministic features of how
        signals evolve and interact over time—regardless of whether the
        underlying signals are random or not.</p>
        <p>Even when the signals have randomness, the <em>observed</em>
        evolution of presence density is deterministic<a href="#fn11"
        class="footnote-ref" id="fnref11"
        role="doc-noteref"><sup>11</sup></a> and governed by the law of
        conservation of presence mass—that is, the presence invariant.
        This determinism extends to any functional quantity that depends
        on presence density. As we will see, a large and operationally
        useful class of system behaviors can be characterized in terms
        of the presence density of domain signals. That is why the
        machinery developed here is more than just a theoretical
        curiosity.</p>
        <p>This is our main point of departure in the presence calculus:
        we treat equilibrium not as a precondition for Little’s Law, but
        as a special case of a more general principle. We place the
        finite version of Little’s Law - in the general form of the
        presence invariant at the center of our analysis, because it
        continues to hold and yield meaningful insight even when the
        system is far from equilibrium—precisely the operating modes
        traditional queuing theory and other classical approaches
        de-emphasize, but where most real-world systems actually
        live.</p>
        <p>In fact, there is an even more general principle at work.
        Miyazawa was among the first to note the existence of a broader
        class of <em>rate conservation laws</em> that exhibit structural
        similarities to Little’s Law across diverse domains. Sigman
        later demonstrated that the generalized form of Little’s Law can
        be viewed as an equivalent formulation of such rate conservation
        principles.</p>
        <p>From this perspective, the presence calculus offers
        constructive tools to <em>discover</em> and formulate such laws
        within a domain. The interactions of element-boundary signals in
        any system of presences naturally give rise to rate conservation
        laws based on the principle of conserved signal mass. Mapping
        these laws back to the language of the domain appears to be a
        fruitful path toward uncovering the mechanisms by which systems
        in that domain behave.</p>
        <h2 data-number="7"
        id="navigation-defining-place-and-direction"><span
        class="header-section-number">7</span> Navigation: defining
        place and direction</h2>
        <p>Convergence, as discussed in the last section, is a
        fundamental concept in the<br />
        presence calculus. We now have the tools to detect convergence
        or divergence in the long-run behavior of a system of
        presences—specifically, the evolution of<br />
        presence density and its underlying drivers: signal mass
        contributions and<br />
        signal incidence rates.</p>
        <p>As noted earlier, <em>whenever we can model the meaningful
        behaviors of a system as interactions between element-boundary
        signals within a system of presences</em>,<br />
        this provides a constructive analytical framework for studying
        system<br />
        dynamics. This framework applies to a wide range of operational
        problems across many domains.</p>
        <p>We also observed that much of the structure in the presence
        calculus is<br />
        deterministic. That is, given the observed behavior of a system
        of presences,<br />
        we can deterministically explain the evolution of presence
        density in terms of<br />
        its drivers, signal mass and incidence rate—across time and
        across different timescales.</p>
        <p>In this section, we elaborate on this deterministic structure
        and introduce<br />
        new tools that help us analyze how local behaviors evolve into
        global patterns, and help us understand the dynamics of a system
        of presence.</p>
        <p>First we will develop fundamental tools that help us
        establish a uniform sense of place and direction of the system
        evolution, tha scales from the local to global timescales. While
        convergence and divergence establish this for longer timescales,
        these concepts are somewhat unwieldy and inapplicable when the
        system is operating far from equilibrium, and we need some
        better tools to navigate in this complementary region.</p>
        <p>We will show how to detect emerging patterns in this
        evolution that reveal the<br />
        <em>direction</em> in which the system is moving—and just as
        importantly, how to<br />
        detect when that direction is <em>changing</em>.</p>
        <p>These are essential capabilities for building mechanisms that
        can steer system behavior in a desired direction—toward
        convergence or divergence of presence density.</p>
        <h3 data-number="7.1"
        id="the-presence-accumulation-recurrence"><span
        class="header-section-number">7.1</span> The Presence
        Accumulation Recurrence</h3>
        <p>We noted at the end of the last section that the observed
        evolution of a the presence density of a system of presences is
        deterministic. Let’s see why this is the case.</p>
        <p>We have reproduced the presence accumulation matrix from
        Figure 12 below. Recall from section 5 that given an <span
        class="math inline">\(MxN\)</span> presence matrix, the
        accumulation matrix is an <span
        class="math inline">\(NxN\)</span> upper triangular matrix where
        each cell <span class="math inline">\(A[i,j], 1 \le i \lt j \le
        N\)</span> records the cumulative presence mass in columns <span
        class="math inline">\([i,j]\)</span> in the original presence
        matrix.</p>
        <p>We also noted that the diagonal in the accumulation matrix is
        a sample path through the system history and the top row records
        the cumulative presence mass over an observation period. It
        represents the area under the sample path. In essence, the
        accumulation matrix is a compact encoding of the evolution of
        presence density across timescales.</p>
        <div style="text-align: center; margin:2em">
        <table>
        <thead>
        <tr>
        <th>
        i\j
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <!-- Row 1 -->
        <tr>
        <td>
        1
        </td>
        <td style="background-color:#e6ffe6">
        1.4
        </td>
        <td style="background-color:#e6f0ff">
        7.3
        </td>
        <td style="background-color:#e6ffe6">
        16.5
        </td>
        <td style="background-color:#e6f0ff">
        21.5
        </td>
        <td style="background-color:#e6ffe6">
        27.4
        </td>
        <td style="background-color:#e6f0ff">
        34.9
        </td>
        <td style="background-color:#e6ffe6">
        42.5
        </td>
        <td style="background-color:#e6f0ff">
        49.1
        </td>
        <td style="background-color:#e6ffe6">
        54.6
        </td>
        <td style="background-color:#e6f0ff">
        56.3
        </td>
        </tr>
        <!-- Row 2 -->
        <tr>
        <td>
        2
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td style="background-color:#e6f0ff">
        15.1
        </td>
        <td style="background-color:#e6ffe6">
        20.1
        </td>
        <td style="background-color:#e6f0ff">
        26.0
        </td>
        <td style="background-color:#e6ffe6">
        33.5
        </td>
        <td style="background-color:#e6f0ff">
        41.1
        </td>
        <td style="background-color:#e6ffe6">
        47.7
        </td>
        <td style="background-color:#e6f0ff">
        53.2
        </td>
        <td style="background-color:#e6ffe6">
        54.9
        </td>
        </tr>
        <!-- Row 3 -->
        <tr>
        <td>
        3
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        9.2
        </td>
        <td style="background-color:#e6f0ff">
        14.2
        </td>
        <td style="background-color:#e6ffe6">
        20.1
        </td>
        <td style="background-color:#e6f0ff">
        27.6
        </td>
        <td style="background-color:#e6ffe6">
        35.2
        </td>
        <td style="background-color:#e6f0ff">
        41.8
        </td>
        <td style="background-color:#e6ffe6">
        47.3
        </td>
        <td style="background-color:#e6f0ff">
        49.0
        </td>
        </tr>
        <!-- Row 4 -->
        <tr>
        <td>
        4
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.0
        </td>
        <td style="background-color:#e6f0ff">
        10.9
        </td>
        <td style="background-color:#e6ffe6">
        18.4
        </td>
        <td style="background-color:#e6f0ff">
        26.0
        </td>
        <td style="background-color:#e6ffe6">
        32.6
        </td>
        <td style="background-color:#e6f0ff">
        38.1
        </td>
        <td style="background-color:#e6ffe6">
        39.8
        </td>
        </tr>
        <!-- Row 5 -->
        <tr>
        <td>
        5
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.9
        </td>
        <td style="background-color:#e6f0ff">
        13.4
        </td>
        <td style="background-color:#e6ffe6">
        21.0
        </td>
        <td style="background-color:#e6f0ff">
        27.6
        </td>
        <td style="background-color:#e6ffe6">
        33.1
        </td>
        <td style="background-color:#e6f0ff">
        34.8
        </td>
        </tr>
        <!-- Row 6 -->
        <tr>
        <td>
        6
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.5
        </td>
        <td style="background-color:#e6f0ff">
        15.1
        </td>
        <td style="background-color:#e6ffe6">
        21.7
        </td>
        <td style="background-color:#e6f0ff">
        27.2
        </td>
        <td style="background-color:#e6ffe6">
        28.9
        </td>
        </tr>
        <!-- Row 7 -->
        <tr>
        <td>
        7
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        7.6
        </td>
        <td style="background-color:#e6f0ff">
        14.2
        </td>
        <td style="background-color:#e6ffe6">
        19.7
        </td>
        <td style="background-color:#e6f0ff">
        21.4
        </td>
        </tr>
        <!-- Row 8 -->
        <tr>
        <td>
        8
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        6.6
        </td>
        <td style="background-color:#e6f0ff">
        12.1
        </td>
        <td style="background-color:#e6ffe6">
        13.8
        </td>
        </tr>
        <!-- Row 9 -->
        <tr>
        <td>
        9
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        5.5
        </td>
        <td style="background-color:#e6f0ff">
        7.2
        </td>
        </tr>
        <!-- Row 10 -->
        <tr>
        <td>
        10
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color:#e6ffe6">
        1.7
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 12: Final presence accumulation matrix for the presence matrix of Figure 9. </code></pre>
        </div>
        </div>
        <p>If we look closely at the entries in the matrix though, we
        will see that there is much tighter local relationship between
        the matrix entries. We can, in general, write the following
        recurrence describing the entries in the matrix.</p>
        <p>We can express this local relationship using the following
        recurrence:</p>
        <p><span class="math display">\[
        A[i,j] = A[i,j-1] + A[i+1,j] - A[i+1,j-1]
        \quad \text{for } 1 \le i &lt; j \le N
        \]</span></p>
        <p>It tells us that the cumulative mass from column <span
        class="math inline">\(i\)</span> to column <span
        class="math inline">\(j\)</span> can be<br />
        computed from three nearby entries:</p>
        <ul>
        <li><span class="math inline">\(A[i,j-1]\)</span>: the
        cumulative mass from <span class="math inline">\(i\)</span> to
        <span class="math inline">\(j{-}1\)</span></li>
        <li><span class="math inline">\(A[i+1, j]\)</span>: the mass
        from <span class="math inline">\(i{+}1\)</span> to <span
        class="math inline">\(j\)</span></li>
        <li>a correction term subtracting the overlap <span
        class="math inline">\(A[i+1, j-1]\)</span> from <span
        class="math inline">\(i{+}1\)</span> to <span
        class="math inline">\(j{-}1\)</span></li>
        </ul>
        <p>This equation reflects the additivity of cumulative presence
        mass over<br />
        rectangular regions in the presence matrix <a href="#fn12"
        class="footnote-ref" id="fnref12"
        role="doc-noteref"><sup>12</sup></a>.</p>
        <p>Thus, the presence accumulation matrix—and by extension, the
        evolution of<br />
        presence density—is governed by a simple local rule. Given the
        values on the<br />
        diagonal, the rest of the matrix is fully determined.</p>
        <p>The physical interpretation of this is that, given the first
        <span class="math inline">\(N{-}1\)</span> points<br />
        on the sample path, knowing the <span
        class="math inline">\(N^{\text{th}}\)</span> point allows us to
        trace how<br />
        the macro behavior of the system—across every timescale—evolved
        up to that<br />
        point.</p>
        <p>This is the foundation for<br />
        the next tools we will develop: tools for detecting direction
        and change in<br />
        system behavior based on how these recurrence relationships
        evolve over time.</p>
        <h3 data-number="7.2"
        id="a-phase-space-representation-of-the-presence-invariant"><span
        class="header-section-number">7.2</span> A phase space
        representation of the presence invariant</h3>
        <p>So now that we know presence accumulates over timescales
        according to a<br />
        deterministic rule, let’s assemble some machinery that gives us
        a sense of the<br />
        <em>direction</em> in which the system as a whole is moving. For
        this, it is useful to<br />
        construct an alternate view of the entries in this matrix using
        our favorite<br />
        tool: the presence invariant.</p>
        <p>We’ll start by noting that the presence invariant holds for
        every entry in the<br />
        accumulation matrix. Therefore, we can write:</p>
        <p><span class="math display">\[
        \frac{A(i,j)}{T} = \frac{N}{T} \times \frac{A(i,j)}{N} \quad
        \text{where } T = j - i
        \]</span></p>
        <p>Which is just the presence invariant <span
        class="math inline">\(\delta = \iota \cdot \bar{m}\)</span>
        written out for every entry in the matrix A.</p>
        <p>Now, let’s revisit the relationship <span
        class="math inline">\(\delta = \iota \cdot
        \bar{m}\)</span>.<br />
        As noted earlier, this expresses that each entry in the
        accumulation matrix can<br />
        be factored into two components: the incidence rate <span
        class="math inline">\(\iota\)</span> and the average mass<br />
        <span class="math inline">\(\bar{m}\)</span> per incident
        signal. There are useful insights to be gained by<br />
        examining this multiplicative structure in log space.</p>
        <p>Taking the logarithm of both sides of the presence
        invariant<br />
        <span class="math inline">\(\delta = \iota \cdot
        \bar{m}\)</span>, we obtain:</p>
        <p><span class="math display">\[
        \log \delta = \log \iota + \log \bar{m}
        \]</span></p>
        <p>This expresses the invariant as an additive relation in log
        space, which<br />
        provides a natural coordinate system for analyzing system
        behavior.</p>
        <p>Each component now contributes linearly to the overall
        presence density (in log space), and changes in incidence rate
        or average mass can be interpreted as<br />
        vector displacements along orthogonal axes in this space.</p>
        <p>This gives us a mapping of the presence invariant in the
        complex plane.</p>
        <p>We now introduce a geometric interpretation of this relation
        in the complex<br />
        plane. Let’s define a mapping:</p>
        <p><span class="math display">\[
        z = \log \iota + i \log \bar{m}
        \]</span></p>
        <p>Here, the real part of <span class="math inline">\(z\)</span>
        encodes the logarithm of the incidence rate, and the imaginary
        part encodes the logarithm of the average mass.</p>
        <p>This mapping allows us to compactly represent the two degrees
        of freedom in presence density as a vector in a two-dimensional
        plane, with the magnitude of the vector encoding the intensity
        of the presence: increasing or decreasing, and the direction of
        the vector encoding what is driving the increase or decrease:
        incidence rate or presence mass.</p>
        <p>This vector representation gives us a compact and precise
        definition of the magnitude and direction of the <em>flow</em>
        of presence density in the system at any point in time. This is
        what we will use as the <em>definition</em> of flow in a system
        of presences.</p>
        <p>Lets formalize these notions.</p>
        <p>Following standard techniques in complex analysis, we now
        introduce two derived quantities: the norm and the phase angle
        for the complex number that represents a presence density of
        system of presences over a finite interval of time in
        log-space.</p>
        <ul>
        <li><p>The <strong>norm</strong> of <span
        class="math inline">\(z\)</span> represents the overall
        intensity of presence accumulation<br />
        in log space:</p>
        <p><span class="math display">\[
        \|z\| = \sqrt{(\log \iota)^2 + (\log \bar{m})^2}
        \]</span></p>
        <p>This gives a scale-invariant measure of the strength of
        presence density<br />
        combining the effects of incidence and mass into a single
        magnitude.</p>
        <ul>
        <li><p>The <strong>phase angle</strong> of <span
        class="math inline">\(z\)</span> represents the balance between
        rate and mass<br />
        contributions:</p>
        <p><span class="math display">\[
        \theta = \arg(z) = \tan^{-1} \left( \frac{\log \bar{m}}{\log
        \iota} \right)
        \]</span></p>
        <p>This tells us whether the system’s current trajectory is more
        rate-dominant -ie due to lots of small signals with small mass
        (small <span class="math inline">\(\theta\)</span>) or
        mass-dominant (larger <span
        class="math inline">\(\theta\)</span>) or fewer signals with
        large mass. <span class="math inline">\(\theta\)</span> provides
        a directional sense of what’s driving the observed evolution of
        presence density.</p></li>
        </ul></li>
        </ul>
        <p>Put together <span class="math inline">\(\|z\|\)</span> and
        <span class="math inline">\(\theta\)</span> allow us to
        represent the present density over any interval in polar
        co-ordinates in log-space as <span class="math display">\[
        z = \|z\| \cdot e^{i.\theta}
        \]</span></p>
        <p>Together, <span class="math inline">\(\|z\|\)</span> and
        <span class="math inline">\(\theta\)</span> allow us to
        decompose each matrix entry’s behavior into <em>how fast</em> it
        is growing (norm) and <em>in what direction</em> the
        contributing factors lie (phase).</p>
        <p>Figure 25 shows this mapping of the presence accumulation
        matrix into polar co-ordinates in the complex plane.</p>
        <div style="text-align: center; margin: 2em">
        <table style="border-collapse: collapse; margin: auto; font-family: serif; font-size: 0.95em;">
        <thead>
        <tr>
        <th>
        i
        </th>
        <th>
        1
        </th>
        <th>
        2
        </th>
        <th>
        3
        </th>
        <th>
        4
        </th>
        <th>
        5
        </th>
        <th>
        6
        </th>
        <th>
        7
        </th>
        <th>
        8
        </th>
        <th>
        9
        </th>
        <th>
        10
        </th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td>
        <b>1</b>
        </td>
        <td style="background-color: #eef;">
        1.34 ∠ -0.61
        </td>
        <td style="background-color: #ddf;">
        0.98 ∠ 1.14
        </td>
        <td style="background-color: #eef;">
        1.70 ∠ 1.57
        </td>
        <td style="background-color: #ddf;">
        1.99 ∠ 1.72
        </td>
        <td style="background-color: #eef;">
        2.27 ∠ 1.80
        </td>
        <td style="background-color: #ddf;">
        2.55 ∠ 1.85
        </td>
        <td style="background-color: #eef;">
        2.43 ∠ 1.80
        </td>
        <td style="background-color: #ddf;">
        2.60 ∠ 1.84
        </td>
        <td style="background-color: #eef;">
        2.74 ∠ 1.87
        </td>
        <td style="background-color: #ddf;">
        2.80 ∠ 1.90
        </td>
        </tr>
        <tr>
        <td>
        <b>2</b>
        </td>
        <td>
        </td>
        <td style="background-color: #eef;">
        1.29 ∠ 0.55
        </td>
        <td style="background-color: #ddf;">
        1.67 ∠ 1.32
        </td>
        <td style="background-color: #eef;">
        1.90 ∠ 1.57
        </td>
        <td style="background-color: #ddf;">
        2.18 ∠ 1.70
        </td>
        <td style="background-color: #eef;">
        2.47 ∠ 1.78
        </td>
        <td style="background-color: #ddf;">
        2.36 ∠ 1.74
        </td>
        <td style="background-color: #eef;">
        2.54 ∠ 1.79
        </td>
        <td style="background-color: #ddf;">
        2.68 ∠ 1.83
        </td>
        <td style="background-color: #eef;">
        2.74 ∠ 1.87
        </td>
        </tr>
        <tr>
        <td>
        <b>3</b>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color: #eef;">
        1.57 ∠ 0.80
        </td>
        <td style="background-color: #ddf;">
        1.61 ∠ 1.32
        </td>
        <td style="background-color: #eef;">
        1.90 ∠ 1.57
        </td>
        <td style="background-color: #ddf;">
        2.24 ∠ 1.70
        </td>
        <td style="background-color: #eef;">
        2.19 ∠ 1.67
        </td>
        <td style="background-color: #ddf;">
        2.38 ∠ 1.74
        </td>
        <td style="background-color: #eef;">
        2.53 ∠ 1.79
        </td>
        <td style="background-color: #ddf;">
        2.60 ∠ 1.84
        </td>
        </tr>
        <tr>
        <td>
        <b>4</b>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color: #eef;">
        1.21 ∠ 0.44
        </td>
        <td style="background-color: #ddf;">
        1.35 ∠ 1.27
        </td>
        <td style="background-color: #eef;">
        1.81 ∠ 1.57
        </td>
        <td style="background-color: #ddf;">
        1.87 ∠ 1.57
        </td>
        <td style="background-color: #eef;">
        2.11 ∠ 1.68
        </td>
        <td style="background-color: #ddf;">
        2.29 ∠ 1.75
        </td>
        <td style="background-color: #eef;">
        2.36 ∠ 1.81
        </td>
        </tr>
        <tr>
        <td>
        <b>5</b>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color: #eef;">
        1.28 ∠ 1.00
        </td>
        <td style="background-color: #ddf;">
        1.55 ∠ 1.31
        </td>
        <td style="background-color: #eef;">
        1.68 ∠ 1.40
        </td>
        <td style="background-color: #ddf;">
        1.93 ∠ 1.57
        </td>
        <td style="background-color: #eef;">
        2.12 ∠ 1.68
        </td>
        <td style="background-color: #ddf;">
        2.20 ∠ 1.76
        </td>
        </tr>
        <tr>
        <td>
        <b>6</b>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color: #eef;">
        1.43 ∠ 0.70
        </td>
        <td style="background-color: #ddf;">
        1.50 ∠ 1.09
        </td>
        <td style="background-color: #eef;">
        1.72 ∠ 1.40
        </td>
        <td style="background-color: #ddf;">
        1.92 ∠ 1.57
        </td>
        <td style="background-color: #eef;">
        1.99 ∠ 1.68
        </td>
        </tr>
        <tr>
        <td>
        <b>7</b>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color: #eef;">
        1.53 ∠ 0.43
        </td>
        <td style="background-color: #ddf;">
        1.44 ∠ 1.07
        </td>
        <td style="background-color: #eef;">
        1.62 ∠ 1.39
        </td>
        <td style="background-color: #ddf;">
        1.68 ∠ 1.57
        </td>
        </tr>
        <tr>
        <td>
        <b>8</b>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color: #eef;">
        1.35 ∠ 0.62
        </td>
        <td style="background-color: #ddf;">
        1.45 ∠ 1.29
        </td>
        <td style="background-color: #eef;">
        1.53 ∠ 1.57
        </td>
        </tr>
        <tr>
        <td>
        <b>9</b>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color: #eef;">
        1.23 ∠ 0.97
        </td>
        <td style="background-color: #ddf;">
        1.28 ∠ 1.57
        </td>
        </tr>
        <tr>
        <td>
        <b>10</b>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td>
        </td>
        <td style="background-color: #eef;">
        0.71 ∠ -0.23
        </td>
        </tr>
        </tbody>
        </table>
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 25: Representation of the presence accumulation matrix in polar co-ordinates on the complex plane. </code></pre>
        </div>
        </div>
        <p>Figure 25 is not particularly insightful, so let’s visualize
        this as a field of vectors as in Figure 26. We will call this
        the flow field for the system.</p>
        <div style="text-align: center; margin:2em">
        <img src="../assets/pandoc/flow_field.png" width="600px" />
        <div
        style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
        <pre><code>Figure 26: Figure 25 visualized as a flow field </code></pre>
        </div>
        </div>
        <p>Lets dig into how this flow field is constructed and what it
        means.</p>
        <p>Starting with Figure 25, the grid represents rows and columns
        of the accumulation matrix.</p>
        <ul>
        <li>Each cell is represented by the log-space vector of the
        presence density over that interval in polar coordinates.</li>
        <li>The length of the vector encodes magnitude <span
        class="math inline">\(\|z\|\)</span> and the orientation of the
        vector encodes <span class="math inline">\(\theta\)</span> in
        radians.</li>
        </ul>
        <p>Since the matrix in Figure 25 is upper triangular, there is
        no significant information encoded in the bottom half of the
        matrix.</p>
        <p>So as a convention, when we visualize it we will drop the
        bottom half of the matrix and only show the entries in the upper
        diagonal.</p>
        <p>Now imagine this matrix rotated by 90 degrees so that entries
        on the diagonal are on the bottom row, the entries on the next
        diagonal are laid out above it, and so on until the we get to
        the top left entry in the matrix. The result in the pyramid
        shape of the flow field diagram in figure 26.</p>
        <p>Each vector in the flow field is drawn with a proportional
        magnitude and theta.</p>
        <p>Figure 26 compresses a tremendous amount information about
        the dynamics of a system into a very compact representation that
        is easy to scan and interpret visually as well as
        analytically.</p>
        <p>In this view, we are not interested in absolute magnitudes or
        angles for the most part. Rather we focus on the relative change
        in magnitude and direction of these vectors as we sweep left to
        right in time along a row and from bottom to top in time scales
        across the rows. That is, the flow field encodes the
        <em>dynamics</em> of the system across time and time scales.</p>
        <p>Lets see how this encoding works.</p>
        <div style="text-align: center; margin:2em">
        <table style="border-collapse: collapse; margin: auto; font-family: sans-serif;">
        <thead>
        <tr>
        <th style="padding: 0.5em 1em;">
        θ (radians)
        </th>
        <th style="padding: 0.5em 1em;">
        Direction
        </th>
        <th style="padding: 0.5em 1em;">
        Interpretation
        </th>
        </tr>
        </thead>
        <tbody>
        <tr>
        <td>
        π−ϵ
        </td>
        <td>
        <svg width="24" height="24" viewBox="0 0 24 24" overflow="visible">
        <line x1="20" y1="14" x2="4" y2="10" stroke="black" stroke-width="2"
                        marker-end="url(#arrow)"/>
        </svg>
        </td>
        <td>
        Signal mass dominates completely; incidence rate is negligible
        </td>
        </tr>
        <tr>
        <td>
        3π/4
        </td>
        <td>
        <svg width="24" height="24" viewBox="0 0 24 24" overflow="visible">
        <line x1="20" y1="20" x2="4" y2="4" stroke="black" stroke-width="2"
                        marker-end="url(#arrow)"/>
        </svg>
        </td>
        <td>
        Mass dominates strongly; rate contributes weakly
        </td>
        </tr>
        <tr>
        <td>
        π/2
        </td>
        <td>
        <svg width="24" height="24" viewBox="0 0 24 24" overflow="visible">
        <line x1="12" y1="20" x2="12" y2="4" stroke="black" stroke-width="2"
                        marker-end="url(#arrow)"/>
        </svg>
        </td>
        <td>
        Mass increase only; rate is steady
        </td>
        </tr>
        <tr>
        <td>
        π/4
        </td>
        <td>
        <svg width="24" height="24" viewBox="0 0 24 24" overflow="visible">
        <line x1="4" y1="20" x2="20" y2="4" stroke="black" stroke-width="2"
                        marker-end="url(#arrow)"/>
        </svg>
        </td>
        <td>
        Both mass and rate contribute; mass dominates slightly
        </td>
        </tr>
        <tr>
        <td>
        0
        </td>
        <td>
        <svg width="24" height="24" viewBox="0 0 24 24" overflow="visible">
        <line x1="4" y1="12" x2="20" y2="12" stroke="black" stroke-width="2"
                        marker-end="url(#arrow)"/>
        </svg>
        </td>
        <td>
        Balanced contribution: mass and rate scale proportionally
        </td>
        </tr>
        <tr>
        <td>
        −π/4
        </td>
        <td>
        <svg width="24" height="24" viewBox="0 0 24 24" overflow="visible">
        <line x1="4" y1="4" x2="20" y2="20" stroke="black" stroke-width="2"
                        marker-end="url(#arrow)"/>
        </svg>
        </td>
        <td>
        Both mass and rate contribute; rate dominates slightly
        </td>
        </tr>
        <tr>
        <td>
        −π/2
        </td>
        <td>
        <svg width="24" height="24" viewBox="0 0 24 24" overflow="visible">
        <line x1="12" y1="4" x2="12" y2="20" stroke="black" stroke-width="2"
                        marker-end="url(#arrow)"/>
        </svg>
        </td>
        <td>
        Rate increase only; mass is steady
        </td>
        </tr>
        <tr>
        <td>
        −3π/4
        </td>
        <td>
        <svg width="24" height="24" viewBox="0 0 24 24" overflow="visible">
        <line x1="20" y1="4" x2="4" y2="20" stroke="black" stroke-width="2"
                        marker-end="url(#arrow)"/>
        </svg>
        </td>
        <td>
        Rate dominates strongly; mass contributes weakly or not at all
        </td>
        </tr>
        <tr>
        <td>
        −π+ϵ
        </td>
        <td>
        <svg width="24" height="24" viewBox="0 0 24 24" overflow="visible">
        <line x1="20" y1="10" x2="4" y2="14" stroke="black" stroke-width="2"
                        marker-end="url(#arrow)"/>
        </svg>
        </td>
        <td>
        Incidence rate dominates completely; signal mass is negligible
        </td>
        </tr>
        </tbody>
        </table>
        <p><svg height="0" width="0"> <defs>
        <marker id="arrow" markerWidth="6" markerHeight="6" refX="0" refY="3" orient="auto" markerUnits="strokeWidth">
        <path d="M0,0 L0,6 L6,3 z" fill="#000"/> </marker> </defs>
        </svg></p>
        <div style="font-size: 0.9em; color: #555; margin-top: 1em;">
        <pre><code>Table: Interpretation of θ as directional flow in log-space between incidence rate and average mass, with fully rendered vector arrows.</code></pre>
        </div>
        </div>
        <h2 data-number="8" id="attractors"><span
        class="header-section-number">8</span> Attractors</h2>
        <p>Since there are only two degrees of freedom theses system
        trajectories will always lie on a two dimensional manifold <a
        href="#fn13" class="footnote-ref" id="fnref13"
        role="doc-noteref"><sup>13</sup></a> in a 3 dimensional
        space.</p>
        <blockquote>
        <p>Remarkably, the state of <em>every</em> possible system of
        presences, no matter how general, always has a trajectory in
        time that lies on this <em>same</em> manifold. This is a
        powerful constraint and insight that we can use to study the
        behavior of a system of presences over time.</p>
        </blockquote>
        <section id="footnotes"
        class="footnotes footnotes-end-of-document" role="doc-endnotes">
        <hr />
        <ol>
        <li id="fn1"><p>This document is the first step in that
        direction. We welcome feedback on how it can be improved,and the
        concepts clarified. Please feel free to open a pull request with
        thoughts, suggestions or feedback.<a href="#fnref1"
        class="footnote-back" role="doc-backlink">↩︎</a></p></li>
        <li id="fn2"><p>If integration signs in a “gentle” introduction
        feels like a bait-and-switch, rest assured, for the purposes of
        this document you just need to think of them as a way to add up
        presence masses, in a way that the ideas we use for binary
        presences will generalize when we apply them to arbitrary
        functions.<a href="#fnref2" class="footnote-back"
        role="doc-backlink">↩︎</a></p></li>
        <li id="fn3"><p>The way we’ve defined signals and mass is
        directly<br />
        analogous to how mass is defined for matter occupying space in
        physics.</p>
        <p>A binary signal can be thought of as defining a
        one-dimensional interval over<br />
        time. For a fixed element and boundary, this gives us an area
        under the<br />
        curve in two dimensions: time vs. density.</p>
        <p>If we treat elements and boundaries as additional independent
        dimensions,<br />
        then the signal defines a <em>volume</em> in three dimensions,
        with time as one axis.</p>
        <p>This interpretation—presence as a physical manifestation of
        density over<br />
        time—is a powerful way to reason intuitively and computationally
        about duration, overlap, and accumulation in time.</p>
        <p>And when we allow multiple signals to interact over the same
        time periods, we<br />
        begin to model complex, higher-dimensional effects of
        presence—exactly the<br />
        kind of generality we’ll need when we move beyond simple binary
        presences.<a href="#fnref3" class="footnote-back"
        role="doc-backlink">↩︎</a></p></li>
        <li id="fn4"><p>The use of a rough curve here is an example of
        how presences can encode<br />
        continuous inputs more effectively than discrete techniques,
        thanks to their<br />
        explicit model of time. Forcing a developer to rank their
        productivity on a<br />
        Likert scale often loses valuable nuance—whereas a fine-grained
        presence<br />
        captures temporal variation with ease, making it available for
        downstream<br />
        analysis.<a href="#fnref4" class="footnote-back"
        role="doc-backlink">↩︎</a></p></li>
        <li id="fn5"><p>It is equally valid to define <span
        class="math inline">\(N\)</span> as the number of distinct
        <em>presences</em> in the observation window. For example for
        the signal <span class="math inline">\(P2\)</span> in Figure 5,
        this corresponds to asking if <span
        class="math inline">\(N=5\)</span> (if we count the disjoint
        presences individually) or <span
        class="math inline">\(N=3\)</span> (if we count the signals).
        These give different values for <span
        class="math inline">\(\bar{m}\)</span> and <span
        class="math inline">\(\iota\)</span> but their product <em>still
        equals</em> <span class="math inline">\(\delta\)</span>, as long
        as a single consistent definition of N is used. This is
        ultimately a modeling decision that depends on what you are
        trying to measure. By default we will assume that <span
        class="math inline">\(N\)</span> is measured at the signal
        granularity.<a href="#fnref5" class="footnote-back"
        role="doc-backlink">↩︎</a></p></li>
        <li id="fn6"><p><span class="math inline">\(\delta\)</span> may
        also be considered the rate at which presence density
        <em>accumulates</em> over the interval. This latter view will be
        useful when interpreting the dynamics of the system.<a
        href="#fnref6" class="footnote-back"
        role="doc-backlink">↩︎</a></p></li>
        <li id="fn7"><p>We note that the residence time represents only
        the portion of the duration of the task in some arbitrary
        observation window. This is a different quantity from the
        overall duration of the task from start to finish (or from
        signal onset to reset in our terminology). This is the more
        familiar metric typically called the cycle time.<a
        href="#fnref7" class="footnote-back"
        role="doc-backlink">↩︎</a></p></li>
        <li id="fn8"><p>It is equally valid to define <span
        class="math inline">\(N\)</span> as the number of distinct
        <em>presences</em> in the observation window. For example for
        the signal <span class="math inline">\(P2\)</span> in Figure 5,
        this corresponds to asking if <span
        class="math inline">\(N=5\)</span> (if we count the disjoint
        presences individually) or <span
        class="math inline">\(N=3\)</span> (if we count the signals).
        These give different values for <span
        class="math inline">\(\bar{m}\)</span> and <span
        class="math inline">\(\iota\)</span> but their product <em>still
        equals</em> <span class="math inline">\(\delta\)</span>, as long
        as a single consistent definition of N is used. This is
        ultimately a modeling decision that depends on what you are
        trying to measure. By default we will assume that <span
        class="math inline">\(N\)</span> is measured at the signal
        granularity.<a href="#fnref8" class="footnote-back"
        role="doc-backlink">↩︎</a></p></li>
        <li id="fn9"><p>Specifically, the Riemann sum approximation of
        the integral <span class="math display">\[
        A(1, j) = \sum_{k=1}^j A(k,k)
        \approx \int_0^j \left( \sum_{(e,b)} P_{(e,b)}(t) \right) dt
        \]</span> where each <span
        class="math inline">\(P_{(e,b)}(t)\)</span> represents the
        presence density function of an underlying element-boundary
        signal.<a href="#fnref9" class="footnote-back"
        role="doc-backlink">↩︎</a></p></li>
        <li id="fn10"><p>While it’s tempting to assume that the limit of
        a product is simply the product of the limits, this doesn’t
        automatically hold here. The long-run average of presence
        density, <span class="math inline">\(\Delta\)</span>, is defined
        as a time-based average, while the average signal mass
        contribution, <span class="math inline">\(\bar{M}\)</span>, is
        defined over the number of signals. Since these limits are taken
        over different denominators, additional technical conditions are
        required to ensure that their product equals the limit of the
        product.<a href="#fnref10" class="footnote-back"
        role="doc-backlink">↩︎</a></p></li>
        <li id="fn11"><p>It is worth emphasizing the word
        <em>observed</em> in this statement. Even though the evolution
        is deterministic, this does not mean the future behavior of the
        system is predictable based on past behavior. That depends
        entirely on the nature of the signals involved, which may or may
        not be predictable. What we can say is that, given a
        sufficiently complete history of the system, we can
        deterministically reconstruct the current state of presence
        density from any starting point. This explanatory power is
        useful in its own right, as we will soon see.<a href="#fnref11"
        class="footnote-back" role="doc-backlink">↩︎</a></p></li>
        <li id="fn12"><p>Recall that we required signals to be
        measurable. This recurrence<br />
        reflects the finite additivity of measures over overlapping
        regions. If we<br />
        interpret <span class="math inline">\(A[i,j]\)</span> as the
        measure of the union of two intervals, then the<br />
        recurrence follows the identity <span
        class="math display">\[\mu(A \cup B) = \mu(A) + \mu(B) - \mu(A
        \cap B)\]</span>,<br />
        where <span class="math display">\[A = [i, j{-}1] \text{ and } B
        = [i{+}1, j]\]</span>. The subtraction removes the overlap <span
        class="math inline">\([i{+}1, j{-}1]\)</span>, ensuring the
        correct cumulative mass is assigned to <span
        class="math inline">\([i, j]\)</span>.<a href="#fnref12"
        class="footnote-back" role="doc-backlink">↩︎</a></p></li>
        <li id="fn13"><p>All solutions to an equation of the form <span
        class="math inline">\(x=y \times z,\)</span> which is the form
        of the presence invariant, lie on a 2D surface, called a
        manifold in 3 dimensions: think of the manifold a rubber sheet
        suspended in a 3 dimensional space. All the values of <span
        class="math inline">\(x, y, \text{ and } z\)</span> that satisfy
        this equation will lie on the surface of this rubber sheet. This
        is a powerful geometric constraint that we can exploit to reason
        about the possible behavior of a system of presences over
        time.<a href="#fnref13" class="footnote-back"
        role="doc-backlink">↩︎</a></p></li>
        </ol>
        </section>
      </div>
</body>
</html>
