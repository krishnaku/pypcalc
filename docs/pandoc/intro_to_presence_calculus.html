<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>intro_to_presence_calculus</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h1 id="the-presence-calculus-a-gentle-introduction">The Presence
Calculus, <br> A Gentle Introduction</h1>
<p><strong>Dr. Krishna Kumar</strong><br />
<em>The Polaris Advisor Program</em></p>
<h2 id="what-is-the-presence-calculus">What is The Presence
Calculus?</h2>
<p>The Presence Calculus is a new approach for reasoning quantitatively
about the<br />
relationships between signals in a domain over time.</p>
<p>The primary goal is to support rigorous modeling and principled<br />
decision-making with operational data in complex, business-critical
domains.</p>
<p>A key objective was ensuring that the use of data in such decisions
rests on a<br />
mathematically precise, logically coherent, and epistemically
grounded<br />
foundation.</p>
<p>The presence calculus emerged from a search for better quantitative
tools to<br />
reason about software product development and engineering, where
current<br />
approaches leave much to be desired in all three aspects.</p>
<p>Minimally, the foundational constructs of the calculus bring
mathematical<br />
precision to widely used—but poorly defined—concepts such as flow,
stability,<br />
equilibrium, and coherence in a domain. More importantly, it allows us
to<br />
relate them to business-oriented concerns like delay, cost, revenue,
and<br />
user experience.</p>
<p>As you’ll see, however, the ideas behind the calculus are far more
general,<br />
with potential applications well beyond the software product
development<br />
context it emerged from.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pcalc/presence_calculus.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 1: Key constructs—presences, element trajectories, presence matrix,  
and basis topology</code></pre>
</div>
</div>
<h2 id="the-pitch">The pitch</h2>
<p>We introduce the simple but powerful concept of a
<em>presence</em>.</p>
<p>This lets us reason about time, history and evolution using
techniques from measure theory, topology and complex analysis.</p>
<p>Classical statistics and probability theory often struggle here.
<em>History</em>—the sequence and structure of changes in the domain
over time— is usually fenced off under assumptions like ergodicity,
stationarity, and independence.</p>
<p>However, probability theory and statistics remain very powerful tools
for describing local behavior, identifying patterns and correlations in
this behavior, and modeling uncertainty—all crucial aspects of
meaningful analysis of complex systems.</p>
<p>Our thesis is that to move beyond simple descriptive statistics,
statistical inference and probabilistic models, and start reasoning
about global and long run behavior of complex systems, we need models
that treat time and history as first-class concepts we can reason and
compute with.</p>
<p>This, in turn, lets us apply techniques from disciplines such as
stochastic process dynamics, queueing theory, and complex systems
science, to reason holistically about global, long run behavior of
complex systems.</p>
<p>We claim the presence calculus is a novel, constructive approach to
do this - a new and powerful reasoning tool for anyone working with
complex systems.</p>
<p>But this is a bold claim, and it deserves further scrutiny and
validation, and so we invite anyone interested in doing so to
collaborate with us on this open source project.</p>
<h2 id="learning-more-about-the-presence-calculus">Learning more about
The Presence Calculus</h2>
<p>While the calculus was developed with mathematical rigor, an equally
important<br />
goal was not to let mathematics get in the way of understanding the
simple but very powerful and general ideas the calculus embodies<a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>.</p>
<p>In this document, we’ll motivate and introduce the key ideas in the
calculus<br />
informally, with lots of highly evocative examples and simplifications
to<br />
illustrate concepts.</p>
<p>It is aimed squarely at the non-technical reader. We’ll also continue
with ongoing informal exposition on our blog<br />
<a href="https://www.polaris-flow-dispatch.com">The Polaris Flow
Dispatch</a>.</p>
<p>We recommend reading and understanding the ideas here before jumping
deeper<br />
into the rest of the documentation at this site, which does get a fair
bit<br />
more dense. The next level of detail is in the API docs for <a
href="https://py.pcalc.org">The Presence Calculus<br />
Toolkit</a>.</p>
<p>The toolkit is an open source python library that is designed to
provide efficient implementations for all the core concepts in the
presence calculus. In the API documentation, we go into the concepts at
a level of rigor that you’ll need to work with the pcalc API and apply
the concepts. Some mathematical background will be useful here if you
want to develop extend the core.</p>
<p>The toolkit provides a reference implementation of the concepts in
this document, and is designed as an analytical middleware layer
suitable for interfacing real world operational systems and complex
system simulation, to the analytical machinery of the presence
calculus.</p>
<p>Finally, for those who want to dive deeper into the formal
mathematical<br />
underpinnings of the calculus, we have the theory track, which perhaps
goes<br />
into more detail than most people will need to read or understand, but
is<br />
useful for the mathematically trained to connect the ideas to their
roots in<br />
mainstream mathematics.</p>
<p>Let’s jump in…</p>
<h2 id="why-presence">Why Presence?</h2>
<p>Presence is how reality reveals itself.</p>
<p>We do not perceive the world as disjointed events in time, but rather
as an unfolding—things come into being, endure for a time, and slip
away.</p>
<p>Permanence is just a form of lasting presence. What we call
<em>change</em> is the<br />
movement of presences in and out of our awareness, often set against
that<br />
permanence.</p>
<p>The sense of something being present—or no longer present—is our
most<br />
immediate way of detecting change.</p>
<p>This applies to both the tangible—people, places, and things—and
the<br />
intangible—emotions, feelings, and experiences.</p>
<p>Either way, we reason about the world by reasoning about the
presences and<br />
absences in our environment over time.</p>
<p>The presence calculus begins here. Before we count, measure, compare,
or<br />
optimize, we observe what <em>is</em>.</p>
<p>And what we model is presence.</p>
<h2 id="an-example">An example</h2>
<p>Imagine you see a dollar bill on the sidewalk on your way to get
coffee.<br />
Later, on your way back home, you see it again—still lying in the same
spot.<br />
It would be reasonable for you to assume that the dollar bill was
present<br />
there the whole time.</p>
<p>Of course, that may not be true. Someone might have picked it up in
the<br />
meantime, felt guilty, and quietly returned it. But in the absence of
other<br />
information, your assumption holds: it was there before, and it’s there
now,<br />
so it must have been there in between.</p>
<p>This simple act of inference is something we do all the time. We fill
in gaps,<br />
assume continuity, and reason about what must have been present based on
what<br />
we know from partial glimpses of the world.</p>
<p>The presence calculus gives formal shape to this kind of
inference—and shows<br />
how we can build upon it to <em>reason</em> about presence and
<em>measure</em> its<br />
effects in an environment.</p>
<h2 id="a-software-example">A software example</h2>
<p>Since the ideas here emerged from the software world, let’s begin
with a<br />
mundane, but familiar example: task work in a software team.</p>
<p>By looking closely at how we reason about tasks, we can see how a
subtle shift<br />
to a presence-centered perspective changes not just what we observe, but
what<br />
we measure, and thus can reason about.</p>
<p>We usually reason about task work using <em>events</em> and
<em>snapshots</em> of the state<br />
of a process in time. A task “starts” when it enters development,
and<br />
“finishes” when it’s marked complete. We track “cycle time” by measuring
the<br />
elapsed time between events, “throughput” by counting finish events,
and<br />
“work-in-process” by counting tasks that have started but not yet
finished.</p>
<p>When we look at a Kanban board, we see a point-in-time snapshot of
where tasks<br />
are at that moment—but not how they got there. And by the time we read
a<br />
summary report of how many tasks were finished and how long they took to
go through the process on average, much of the history of the system
that produced those measurements has been lost. That makes it hard to
reason about <em>why</em> those measurements are the way they are.</p>
<p>In complex knowledge work, each task often has a distinct
history—different<br />
from other tasks present at the same time. Losing history makes it hard
to<br />
reason about their interactions and how they impact the global behavior
of the<br />
process.</p>
<p>This problem is not unique to task work. Similar problems exist in
almost all areas of business analysis that rely primarily on descriptive
statistics as the primary measurement tool for system behavior.</p>
<p>We are reduced to trying to make inferences from local
descriptive<br />
statistics —things like cycle times, throughput, and work-in-process
levels- over a rapidly changing process.</p>
<p>We try to reason about a process which is shaped by its history, with
measurement techniques that struggle to represent or reason about that
history. This is difficult to do, and we have no good tools right now
that are fit for<br />
this purpose.</p>
<p>This is where the presence calculus begins.</p>
<p>While it often starts from the same snapshots, the calculus focuses
on the<br />
time <em>in between</em>: when the task was present, where it was
present, for how<br />
long, and whether its presence shifted across people, tools, or
systems.</p>
<p>The connective tissue is no longer the task itself, or the process
steps it<br />
followed, or who was working on it, but a continuous, observable
<em>thread of<br />
presence</em>—through all of them, moving through time, interacting,
crossing boundaries—a mathematical representation of history.</p>
<p>With the presence calculus, these threads and their interactions
across time<br />
and space can now be measured, dissected, and analyzed as
first-class<br />
constructs—built on a remarkably simple primitive—the presence.</p>
<h2 id="the-heart-of-the-matter">The heart of the matter</h2>
<p>At its core, the calculus exploits the difference between the two
independent<br />
statements—“The task started development on Monday” and “The task
completed<br />
development on Friday”—and a single, unified assertion: “The task was
present<br />
in development from Monday through Friday.”</p>
<p>The latter is called a <em>presence</em>, and it is the foundational
building block<br />
of the calculus.</p>
<p>At first glance, this might not seem like a meaningful
difference.</p>
<p>But treating the presence as the primary object of reasoning—as a
<em>first-class</em> construct—opens up an entirely new space of
possibilities.</p>
<p>Specifically, it allows us to apply powerful mathematical tools that
exploit the continuity of time and the algebra of time intervals to
reason about the interactions and emergent configurations of presences
in a rigorous and structured, and more importantly, computable way.</p>
<h2 id="what-is-a-presence">What is a presence?</h2>
<p>Let’s start by building intuition for this concept we call a
<em>presence</em>.<br />
Consider the statement: “The task <span class="math inline">\(X\)</span>
was in Development from Monday to<br />
Friday.”</p>
<p>In the presence calculus, this would be expressed as a presence of
the form:<br />
“The element <span class="math inline">\(X\)</span> was in boundary
<span class="math inline">\(Y\)</span> from <span
class="math inline">\(t_0\)</span> to <span
class="math inline">\(t_1\)</span> with mass 1.”<br />
Presences are statements about elements (from some domain) being present
in a<br />
boundary (from a defined set of boundaries) over a <em>continuous</em>
period of time,<br />
measured using some timescale.</p>
<p>So why do we say “with mass 1”?</p>
<p>The presence calculus treats time as a physical dimension, much like
space.<br />
Just as matter occupies space, presences occupy time. Just as mass
quantifies<br />
<em>how</em> matter occupies space, the mass of a presence quantifies
<em>how</em> a<br />
presence occupies time.</p>
<p>The statement “The task <span class="math inline">\(X\)</span> was in
Development from Monday through Friday” is<br />
a <strong>binary presence</strong> with a uniform mass of 1 over the
entire duration. The<br />
units of this mass are element-time—in this case, task-days.</p>
<p>Binary presences are sufficient to describe the <em>fact</em> of
presence or absence<br />
of things in places in a domain. These presences always have mass 1 in
whatever units we use for elements and time.</p>
<h2 id="what-is-the-mass-of-a-presence">What is the mass of a
presence?</h2>
<p>Let’s consider a more detailed set of statements:</p>
<blockquote>
<p>“Task <span class="math inline">\(X\)</span> had 2 developers working
on it from Monday to Wednesday,<br />
3 developers on Thursday, and 1 developer on Friday.”</p>
</blockquote>
<p>These are no longer about just presence, but about the
<em>effects</em> of presence.<br />
They describe the <strong>load</strong> that task <span
class="math inline">\(X\)</span> placed on the Development
boundary<br />
over time.</p>
<p>The units of this presence are developer-days - potentially in a
completely different space from the task, but grounded over the same
time interval as the task.</p>
<p>Here we are saying: “the task being in this boundary over this time
period, had this effect.”</p>
<p>We will describe such presences using a <em>presence density
function,</em> called <span class="math inline">\(\mathsf{load}\)</span>
in this case:</p>
<ul>
<li><span class="math inline">\((\mathsf{load}, X, \text{Development},
\text{Monday}, \text{Wednesday}, 2)\)</span></li>
<li><span class="math inline">\((\mathsf{load}, X, \text{Development},
\text{Thursday}, \text{Thursday}, 3)\)</span></li>
<li><span class="math inline">\((\mathsf{load}, X, \text{Development},
\text{Friday}, \text{Friday}, 1)\)</span></li>
</ul>
<p>Here, <span class="math inline">\(\mathsf{load}(e, b, t)\)</span> is
a time-varying function that takes an<br />
element <span class="math inline">\(e\)</span>, a boundary <span
class="math inline">\(b\)</span>, and a time <span
class="math inline">\(t\)</span>, and returns a real-valued number<br />
describing how much presence is concentrated at that point in time.</p>
<p>The <em>presence mass</em> of such a presence is the total presence
over the<br />
interval <span class="math inline">\([t_0, t_1]\)</span>, defined
as:</p>
<p><span class="math display">\[ \text{mass} = \int_{t_0}^{t_1}
\mathsf{load}(e, b, t)\, dt \]</span></p>
<p>where <span class="math inline">\(\mathsf{load}\)</span> is the
presence density function <a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<h2
id="domain-signals-presence-density-functions-and-measure-theory">Domain
Signals, presence density functions and measure theory</h2>
<p>Binary presence functions are much easier to understand intuitively,
and we’ll<br />
continue using them in our examples. But the real power of the
presence<br />
calculus comes from generalizing to <em>presence density
functions</em>.</p>
<p>We may think of presence density functions as <em>domain signals</em>
that we are interesting in observing and measuring, and the presence
density functions can model a very large and general class of domain
signals, so in what follows we will often refer to presence density
functions as <em>signals</em> and use the two terms interchangeably.</p>
<p>In our earlier example, we interpreted the signal as<br />
expressing the <em>load</em> placed on an element at a boundary. But
more generally,<br />
a signal can be <em>any</em> real-valued function over time.</p>
<p>The mass of a presence, over any given time <em>interval</em> <span
class="math inline">\([t_0, t1)\)</span> is the integral above, which is
also the area under the signal over that interval<a href="#fn3"
class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/presence_definition.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 2: Signals, Presence,  and Presence Mass</code></pre>
</div>
</div>
<p>The only requirement for a function to be a presence density function
(signal) is that it is <em>measurable</em>, and that you can interpret
<em>presence mass</em>—defined as the integral of the function over a
finite interval—as a meaningful <em>measure</em> of the effect of
presence in your domain.</p>
<p>This is where measure theory enters the picture. It’s not essential
to<br />
understand the full technical details, but at its core, measure theory
tells<br />
us which kinds of functions are <em>measurable</em>—in other words,
which functions<br />
can support meaningful accumulation, comparison, and composition of
values.</p>
<p>Measurability gives us the confidence to do things like compute
statistics,<br />
aggregate over elements or boundaries, and compose presence
effects—while<br />
preserving the semantics of the domain. Informally, when a signal is
measurable, we can treat its values like any other real number and do
math over them, as long as we carefully respect the units involved.</p>
<p>From our perspective, a presence density function represents a signal
whose value that can be <em>accumulated</em> across time and across
presences. This<br />
lets us reason mathematically about presences with confidence—and since
most<br />
of this reasoning will be performed by algorithms, we need
technical<br />
constraints that ensure those calculations are both mathematically valid
and<br />
semantically sound.</p>
<p>To summarize:</p>
<p>A general presence is defined by:</p>
<ul>
<li>a density function <span class="math inline">\(f(e, b,
t)\)</span>,</li>
<li>an element <span class="math inline">\(e\)</span>,</li>
<li>a boundary <span class="math inline">\(b\)</span>,</li>
<li>and a time interval <span class="math inline">\([t_0,
t_1]\)</span>.</li>
</ul>
<p>Its <strong>mass</strong> is the integral of <span
class="math inline">\(f\)</span> over the interval:</p>
<p><span class="math display">\[ \text{mass}(e, b, [t_0, t_1]) =
\int_{t_0}^{t_1} f(e, b, t)\, dt \]</span></p>
<p>This mass captures both <em>that</em> the element was present, and
<em>how</em> it was<br />
present—uniformly, variably, or intermittently—over the time interval of
the presence.</p>
<p>A domain signal, in general, may consist of one or more disjoint
presences over time - the entire sequence of presences constitutes the
signal. Figure 3 shows the relationship between signals and presences
and the types of presences we will typically encounter in practice.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/pdf_examples.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 3: Signals and Presences</code></pre>
</div>
</div>
<h2 id="more-examples">More examples</h2>
<p>Let’s firm up our intuition about what presences can describe with a
few more<br />
examples.</p>
<h3 id="work-in-software">“Work” in software</h3>
<p>If you’ve ever written a line of code in your life, you’ve heard the
question:<br />
“When will it be done?” Work in software can be a slippery, fungible
concept—<br />
and the presence calculus offers a useful way to describe it.</p>
<p>We can express the work on a task using a presence density function
whose<br />
value at time <span class="math inline">\(t\)</span> is the
<em>remaining</em> work on the task at that moment.</p>
<p>This lets us model tasks whose duration is uncertain in general, but
whose<br />
remaining duration can be described at any given time—a common scenario
in<br />
software contexts.</p>
<p>A series of presences, where the (non-zero) mass of each presence
corresponds<br />
to the total remaining work over its interval (interpreting the integral
as a<br />
sum), gives us a way to represent <em>work as presence</em>.</p>
<p>Such presences can represent estimates, forecasts, or
confidence-weighted<br />
projections—and as we’ll see shortly, they can be reasoned about and
computed<br />
with just like any other kind of presence.</p>
<h3 id="the-effects-of-interruptions">The effects of interruptions</h3>
<p>Another useful example from the software world illustrates a
different<br />
application of a presence. Let’s assume the boundary in this case is
a<br />
developer, the element is an interruption (defined appropriately in
the<br />
domain), and the presence density function captures the <em>context
switching<br />
cost</em>—measured in lost time—associated with that interruption.</p>
<p>The key insight here is that the <em>effects</em> of the presence
extend beyond the<br />
interval of the interruption itself. This is a classic case of a delayed
or<br />
<em>decaying effect</em>—a pattern that appears frequently in real-world
systems.</p>
<p>The presence density function can be modeled in different ways:</p>
<ul>
<li>As a constant cost: for example, each interruption causes a
fixed<br />
15-minute recovery period, regardless of its duration.</li>
<li>As a decaying function: the cost is highest at the moment of
interruption<br />
and gradually decreases to zero over a defined recovery window
(e.g.<br />
15 minutes), representing a return to full focus.</li>
</ul>
<p>This approach gives us a precise way to model and reason about
the<br />
<em>aftereffects</em> of events—effects that outlast the events
themselves and<br />
accumulate in subtle but measurable ways over longer timeframes.</p>
<p>In this case, we measured an effect that decayed from a peak, but a
similar<br />
approach can be taken, for example, with a presence density function
that<br />
grows from zero and plateaus over the duration of the presence—such as
the net<br />
increase in revenue due to a released feature.</p>
<p>Used this way, presence density functions give us a powerful tool for
modeling<br />
the impact of binary presences—capturing their downstream or
distributed<br />
effects over time, and reasoning about their relationship over a
shared<br />
timeline.</p>
<p>Another important use case in the same vein is modeling the cost of
delay for a portfolio-level element—and analyzing its cascading impact
across the<br />
portfolio.</p>
<p>These use cases show that it is possible to analyze not just binary
presences,<br />
but entire chains of influence they exert across a timeline—a key
prerequisite<br />
for reasoning about causality.</p>
<h3 id="self-reported-developer-productivity">Self-reported developer
productivity</h3>
<p>Imagine a developer filling out a simple daily check-in:<br />
“How productive did you feel today?”—scored from 1 to 5, or sketched out
as a<br />
rough curve over the day<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>.</p>
<p>Over a week, this forms a presence density function—not of the
developer in a<br />
place, but of their <em>sense</em> of productivity over time.</p>
<p>These types of presences, representing perceptions, are
powerful—helping<br />
teams track experience, spot early signs of burnout, or correlate
perceived<br />
flow with meetings, environment changes, build failures, or
interruptions.</p>
<p>Now, let’s look at some examples outside software development.</p>
<h3 id="browsing-behavior-on-an-e-commerce-site">Browsing behavior on an
e-commerce site</h3>
<p>Imagine a shopper visiting an online store. They spend 90 seconds
browsing<br />
kitchen gadgets, then linger for five full minutes comparing
high-end<br />
headphones, before briefly glancing at a discounted blender.</p>
<p>Each of these interactions can be modeled as a presence: the
shopper’s<br />
(element) attention occupying different parts of the site (boundaries)
over<br />
time. The varying durations reflect interest, and the shifting presence
reveals<br />
patterns of engagement.</p>
<p>By analyzing these presences—where and for how long attention
dwells—we can<br />
begin to understand preferences, intent, and even the likelihood of
conversion<br />
(modeled as a different presence density function).</p>
<h3 id="patient-movement-in-a-hospital">Patient movement in a
hospital</h3>
<p>Consider a patient navigating a hospital stay. They spend the morning
in<br />
radiology, move to a recovery ward for several hours, then are
briefly<br />
transferred to the ICU overnight.</p>
<p>Each location records a presence—when and where the patient was, and
for how<br />
long. These presences can reveal bottlenecks, resource utilization,
and<br />
potential risks.</p>
<p>Over time, analyzing patient presences helps surface patterns in
care<br />
delivery, delays in treatment, and opportunities for improving patient
flow.</p>
<p>These are examples of classic operations management problems
expressed in the<br />
language of the presence calculus. The calculus is well-suited to
modeling<br />
scenarios like these as a base case.</p>
<h2 id="systems-of-presences">Systems of Presences</h2>
<p>Let’s summarize what we’ve described so far.</p>
<p>With signals and presences, we now have a general structure<br />
for describing and measuring the behaviour of an arbitrary time varying
function. The key feature of a presence is that it abstracts these
behaviors into a uniform representation—one that we can reason about and
compute with.</p>
<h3 id="presence-assertions">Presence Assertions</h3>
<p>In Figure 2, we showed the <em>onset</em> and <em>reset</em> times of
a presence density<br />
function. The interval between an onset and a reset is called the
<em>support</em> of<br />
the signal. Within this interval, the function is non-zero.</p>
<p>As we see in Figure 3, a given signal may have <em>multiple</em> such
disjoint support<br />
intervals. These represent non-contigous presences of the same element
within the same boundary over time. These may correspond, to episodic
behavior in the underlying domain, for example, user sessions in an
e-commerce context, or rework loops in software development when a task
“returns” to development many times over its lifecycle.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/multiple_support.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 3: A presence mass as a sample of a signal over an interval.</code></pre>
</div>
</div>
<p>A presence mass may be computed over <em>any</em> sub-interval of a
signal, as shown in Figure 3.</p>
<p>There are many possible ways of deriving presence mass from a signal,
including across disjoint support intervals. All we require is that the
interval in question intersects a region of non-zero area that can be
reduced to a presence mass.</p>
<p>So, a presence mass is best thought of as a <em>sampled
measurement</em> of the underlying<br />
signal, taken by an <em>observer</em> over two specific points in time
and reduced to a point-mass measurement over that interval.</p>
<p>A given observer may not even “see” the full underlying signal—only
the <em>mass</em><br />
of the presence they experience over the interval they observed.</p>
<p>Different observers may observe different intervals of the same
signal and derive different presence values, depending on what part of
the function they<br />
encounter.</p>
<p>This brings us to the concept of <em>presence assertions</em>, which
formalize this<br />
idea of an observer recording a presence based on their local view of
the<br />
underlying density function.</p>
<p>A <em>presence assertion</em> is simply a presence augmented with
metadata:</p>
<ul>
<li><em>who</em> the observer was</li>
<li>and an <em>assertion timestamp</em>—the time at which the
observation was made.</li>
</ul>
<p>The assertion time doesn’t need to align with the time interval of
the presence. This allows assertions to refer to the past, reflect the
present, or even anticipate the future behavior of a signal.</p>
<p>Presence assertions give us the ability to assign <em>provenance</em>
to a presence—<br />
not just <em>what</em> we know, but <em>how</em> we know it. This is
essential in<br />
representing complex systems where the observer and the act of
observation<br />
are first-class concerns.</p>
<p>We won’t go too deeply into the epistemological aspects of the
presence<br />
calculus here—this remains an active and open area of research.</p>
<p>But it’s important to acknowledge that this layer exists, that
modeling and interpreting the output of the presence calculus requires
an explicit treatment of how observations are made and by whom, and the
fact that this has a huge impact on the validity of the inferences one
makes using the machinery of the calculus.</p>
<p>With this caveat in place, once we’ve represented a problem domain as
a<br />
<em>system of presences</em>, much of the machinery of the presence
calculus (which<br />
we’ll introduce next) can be applied uniformly.</p>
<p>For the next couple of sections, where we will introduce this
machinery, we will operate under the assumption that there is a
<em>signal</em> that can be observed and what we observe about the
signal reflects what an observer knows about the domain. We don’t
presume anything about the “truth” of the observations, we treat them
uniformly as signals.</p>
<p>One thing we will see is that from the perspective of this machinery,
there are no fundamental differences in behavior between binary signals
and arbitrary signals once they’ve been reduced to a canonical,
presence-mass oriented representation<a href="#fn5" class="footnote-ref"
id="fnref5" role="doc-noteref"><sup>5</sup></a>.</p>
<p>This greatly increases the scope of the problem domains where this
machinery can be applied, and our examples in the previous section began
to hint at the possibilities.</p>
<p>This, in the end, is the source of the generality and power of the
presence calculus.</p>
<h2 id="co-presence-and-the-presence-invariant">Co-Presence and The
Presence Invariant</h2>
<p>In the last section, we introduced <em>systems of presences</em> as
collections of presence assertions defined over a set of presence
density functions (signals).</p>
<p>Figure 5 illustrates an example of such a system, where we focus on
the subset of presences defined over a <em>shared observation
interval</em>.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/presence_invariant_continuous.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 5: The Presence Invariant</code></pre>
</div>
</div>
<p>These presences are called <em>co-present</em>—they represent an
observer making simultaneous measurements of presence mass across
multiple signals over a common interval of time.</p>
<p>Co-presence is a necessary (but not sufficient) condition for
interaction between one or more signals. In this section, we introduce a
key construct in the presence calculus: the <em>presence invariant</em>.
It expresses a general and powerful relationship that holds for any
co-present subset of presences within a finite observation window. and
is a fundamental relationship tha governs how the masses of co-present
signal’s interact.</p>
<p>Let’s establish this relationship.</p>
<p>Given, any finite observation interval, we’ve already shown that each
presence density function has a <em>presence mass</em>, defined as the
integral of the density over the observation interval.</p>
<p>These can be thought of These can be thought of as mass contributions
from those presences to that interval. The sum of these individual
<em>mass contributions</em> gives the total presence mass observed
across the system in that interval.</p>
<p>Let</p>
<p><span class="math display">\[ A = M_0 + M_1 + M_3 \]</span></p>
<p>be the total mass contribution from the signals that have non-zero
mass over the interval <span class="math inline">\([t_0, t_1)\)</span>.
The length of this interval is <span class="math inline">\(T = t_1 -
t_0\)</span>.</p>
<p>Since the mass comes from integrating a density function over time,
the quantity <span class="math inline">\(\frac{A}{T}\)</span> represents
the <em>average presence density</em> over the observation interval. We
can now decompose this as:</p>
<p><span class="math display">\[ \delta = \frac{A}{T} = \frac{A}{N}
\times \frac{N}{T} \]</span></p>
<p>Here <span class="math inline">\(N\)</span> is the number of distinct
signals with a presence in the observation window<a href="#fn6"
class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.
This separates the average presence density into two interpretable
components:</p>
<ul>
<li><span class="math inline">\(\bar{m} = \frac{A}{N}\)</span>: the
<em>average mass contribution</em> per active signal,</li>
<li><span class="math inline">\(\iota = \frac{N}{T}\)</span>: the
<em>incidence rate</em>—i.e., the number of active signals per unit
time.</li>
</ul>
<p>This leads to the <em>presence invariant</em>:</p>
<p><span class="math display">\[ \text{Average Presence Density} =
\text{Signal Incidence Rate} \times \text{Average Mass Contribution per
Signal} \]</span> or in our notation</p>
<p><span class="math display">\[ \delta = \iota \cdot \bar{m}
\]</span></p>
<p>This identity holds for <em>any</em> co-present subset of signals
over <em>any</em> finite time interval.</p>
<p>While algebraically, this relationship is a tautology, it imposes a
powerful constraint on system behavior—one that is independent of the
specific system, semantics, or timescale.</p>
<p>Indeed, it forms a foundational conservation law of the presence
calculus: the <em>conservation of mass (contribution)</em>.</p>
<p>Just as the conservation of energy or mass constrains the evolution
of physical systems—regardless of the specific materials or forces
involved—the conservation of presence mass constrains how observable
activity is distributed over time in a system of presences.</p>
<p>It is independent of the semantics of what is being observed: like
energy, presence mass can shift, accumulate, or redistribute, but it
remains balanced when distributed across presences over a finite time
interval.</p>
<p>Thus, the conservation of mass plays a role in the presence calculus
similar to that of other conservation laws in physics: it constrains the
behavior of three key observable, measurable parameters of any system of
presences.</p>
<p>Exploiting this constraint allows us to study and characterize the
long-run behavior of a system.</p>
<h3 id="the-presence-invariant-for-binary-presences">The Presence
Invariant for Binary Presences</h3>
<p>At this stage, the presence invariant may still feel rather abstract.
Let’s make it more concrete by interpreting this identity in the special
case of <em>binary</em> presences.</p>
<p>Recall that a <em>binary</em> signal is a function whose density is
either <span class="math inline">\(0\)</span> or <span
class="math inline">\(1\)</span>. That is, we are modeling the presence
or absence of an underlying signal in the domain.</p>
<p>In this case, the <em>mass contribution</em> of a signal becomes an
<em>element-time duration</em>. For example, if the signal represents
the time during which a task is present in development, the mass
contribution of that task over an observation interval is the portion of
its duration that intersects the interval. This is also called the
<em>residence time</em> for the task in the observation window.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/presence_invariant_binary.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 6: The Presence Invariant for binary signals</code></pre>
</div>
</div>
<p>Figure 6 shows possible configurations of binary signals intersecting
a finite observation interval. Suppose the unit of time is days.</p>
<p>The total presence mass accumulation <span
class="math inline">\(A\)</span> is <span
class="math inline">\(11\)</span> task-days. The number <span
class="math inline">\(N\)</span> of tasks that intersect the observation
interval is <span class="math inline">\(4\)</span>. The length of the
observation window is <span class="math inline">\(T = 4\)</span> days.
It is straightforward to verify that the presence invariant holds.</p>
<p>Now, let’s unpack its meaning.</p>
<p>Since each task contributes <span class="math inline">\(1\)</span>
unit of mass for each unit of time it is present, the average presence
density <span class="math inline">\(\delta=\frac{A}{T}\)</span>
represents the <em>average number of tasks</em> present per unit time in
the interval—denoted <span class="math inline">\(L\)</span>.</p>
<p>Conversely, since each unit of mass corresponds to a unit of time
associated with a task, the average mass per active signal, <span
class="math inline">\(\bar{m} = \frac{A}{N}\)</span>, is the average
time a task spends in the observation window. This value is typically
called the <em>residence time</em> <span
class="math inline">\(w\)</span> of a task in the observation
window.</p>
<p>The incidence rate <span class="math inline">\(\iota =
\frac{N}{T}\)</span> may be interpreted as the <em>activation rate</em>
of tasks in the interval—a proxy for the rate at which tasks start
(onset) or finish (reset) within the window.</p>
<p>For example, <span class="math inline">\(N\)</span> may be counted as
the number of tasks that start inside the interval, plus the number that
started before but are still active. Thus, <span
class="math inline">\(\frac{N}{T}\)</span> is a <em>cumulative onset
rate</em> <span class="math inline">\(\Lambda\)</span>.</p>
<p>The presence invariant can now be rewritten as:</p>
<p><span class="math display">\[ L = \Lambda \times w \]</span></p>
<p>which you may recognize as <em>Little’s Law</em> applied over a
finite observation window. Thus, the presence invariant serves as a
<strong>generalization of Little’s Law</strong>—extending it to
arbitrary systems of presence density functions (signals).</p>
<p>It is important to note that we are referring to <em>Little’s Law
over a finite observation window</em>, rather than the much more
familiar, steady-state equilibrium form of Little’s Law.</p>
<p>Just like the presence invariant in the general case, this version of
the law holds <em>unconditionally</em>. The key is that the quantities
involved are <em>observer-relative</em>: the time tasks spend <em>within
the observation window</em>, and the <em>activation rate</em> of tasks
<em>over the window</em>, rather than the task-relative durations or
steady-state arrival/departure rates used in classical queueing
theory.</p>
<p>Indeed, in the general case, the difference between these two forms
of the identity will serve as the basis for how we <em>define</em>
whether a system of presences is in equilibrium or not. The idea is that
the system of presences is at equilibrium when observed over
sufficiently long observation windows such that the observer-relative
and task-relative values of average presence density, incidence rate and
average presence mass converge.</p>
<p>Since complex systems often operate far from equilibrium—and since
the presence invariant holds <em>regardless</em> of equilibrium—the
finite-window form becomes far more valuable for analyzing the long-run
behavior of such systems as they <em>move into and between</em>
equilibrium states.</p>
<p>We’ll also note that for any arbitrary signal, we can always define a
binary presence corresponding to the intervals over which the value of
the density function is non-zero (the support interval). In general
then, we can say the finite window version of Little’s Law, with the
above definitions, always applies to <em>any</em> signal under this
interpretation. <em>In addition,</em> the general presence invariant
also applies to the full signal.</p>
<p>We will return to this important topic shortly. But first, let’s
understand the implications of the presence invariant.</p>
<h3 id="the-presence-invariant-and-causal-reasoning">The Presence
Invariant and Causal Reasoning</h3>
<p>The presence invariant gave us an important constraint that applies
to the behavior of three key parameters of a system of presences when
measured over any finite time interval: the average presence density,
the incidence rate, and the average mass contribution per signal.</p>
<p>This means that if we observe the behavior of a system of presences
over a continuous sequence of non-overlapping time intervals, the
presence invariant holds in <em>each</em> such interval, and given the
value of any two of the parameters, the third is completely
determined.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/system_presences_discrete.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 7: Sampling a system of presence across uniform intervals. </code></pre>
</div>
</div>
<p>The requirement that presence mass is conserved across each interval
means that there are only two degrees of freedom between three
variables.</p>
<p>If, for example, we know that the average presence density has
increased between one interval and the next, then we know for sure that
either the incidence rate, or average mass per presence (or both) have
increased.</p>
<p>Moreover, these are the <em>only</em> possible explanations for the
increase in the average presence mass. Any other explanation must also
lead to the changes in one of the two primary drivers of the average
presence density.</p>
<p>This is a powerful tool in being able to analyze <em>why</em> a
system of presences behaves the way it does and this is a fundamental
benefit of having a tool like the presence invariant at our
disposal.</p>
<p>This implies that if we study how these parameters change in concert
as we move <em>across</em> time intervals, we will get unique insights
into how a <em>particular</em> system of presences evolves over
time.</p>
<p>Specifically, if we think of these three parameters of a system as
defining the unique co-ordinates of the state of the system over a small
finite interval, we can “trace” the movement of the system by following
these co-ordinates. But since there are only two degrees of freedom
these coordinates will always lie on a two dimensional manifold<a
href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a>, in a 3 dimensional space.</p>
<p>If fact, remarkably, the state of every possible system of presences,
no matter how general, always has a trajectory in time that lies on this
<em>same</em> manifold. This is a powerful constraint and insight that
we can use to study the behavior of a system of presences over time.</p>
<p>We will now introduce a tool called the presence matrix that makes it
easier to visualize and manage the computations involved.</p>
<h2 id="the-presence-matrix">The Presence Matrix</h2>
<p>A <em>presence matrix</em> records the presence mass distribution
obtained by sampling a set of presence density functions over a fixed
set of time intervals. Specifically, if we fix a time granularity—such
as hours, days or weeks—we can construct a matrix in which:</p>
<ul>
<li><p><em>Rows</em> correspond to individual signals (e.g., for each
<span class="math inline">\((
e, b)\)</span> pair),</p></li>
<li><p><em>Columns</em> correspond to non-overlapping time intervals at
that fixed time granularity <em>that cover the time axis</em>,</p></li>
<li><p><em>Entries</em> contain the <em>presence mass</em>, i.e., the
integral of the density function over the corresponding interval:</p>
<p><span class="math display">\[ M_{(e,b),j} = \int_{t_j}^{t_{j+1}}
f_{(e,b)}(t) \, dt \]</span></p></li>
</ul>
<p>The resulting matrix provides a discrete, temporally-aligned
representation of this system of presences.</p>
<p>Since we are accumulating presence masses over an interval, the value
of presence mass in a matrix entry is always a a real number. Figure 8
shows the presence matrix for the system in Figure 7<a href="#fn8"
class="footnote-ref" id="fnref8"
role="doc-noteref"><sup>8</sup></a>.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/presence_matrix.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 8: Presence Matrix for a system of presences. </code></pre>
</div>
</div>
<p>The presence matrix encodes some deep structural properties of a
system of presences. Many of key concepts we want to highlight are
easier to define and understand in terms of this representation.</p>
<p>The Presence Calculus Toolkit documentation has more details on the
mechanics of its construction and the computations it enables. For now,
these details are not critical, and we will focus on the insights it
surfaces.</p>
<h3 id="the-presence-invariant-and-the-presence-matrix.">The presence
invariant and the presence matrix.</h3>
<p>Let’s revisit Figure 3, reproduced below, which introduced the idea
of thinking of presence mass as a sample of an underlying signal.</p>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/multiple_support.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 3: A presence as a sample of a signal over an interval.</code></pre>
</div>
</div>
<p>When we observe a system of presences across a finite observation
window, as we do in deriving the presence invariant, we are looking at
presence mass across a “vertical” slice of time across all signals.</p>
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
E
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
e1_b1
</td>
<td style="background-color: #c6f6c6">
0.3
</td>
<td style="background-color: #c6f6c6">
2.3
</td>
<td style="background-color: #c6f6c6">
3.4
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td style="background-color: #c6f6c6">
2.9
</td>
<td style="background-color: #c6f6c6">
3.2
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
<tr>
<td>
e1_b2
</td>
<td style="background-color: #c6f6c6">
0.3
</td>
<td style="background-color: #c6f6c6">
2.3
</td>
<td style="background-color: #c6f6c6">
3.4
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td>
0.0
</td>
<td style="background-color: #c6f6c6">
1.1
</td>
<td style="background-color: #c6f6c6">
2.2
</td>
<td style="background-color: #c6f6c6">
2.4
</td>
<td style="background-color: #c6f6c6">
2.3
</td>
<td style="background-color: #c6f6c6">
0.8
</td>
</tr>
<tr>
<td>
e2_b2
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td style="background-color: #c6f6c6">
0.9
</td>
<td style="background-color: #c6f6c6">
1.8
</td>
<td style="background-color: #c6f6c6">
3.2
</td>
<td style="background-color: #c6f6c6">
0.9
</td>
</tr>
<tr>
<td>
e2_b1
</td>
<td style="background-color: #c6f6c6">
0.8
</td>
<td style="background-color: #c6f6c6">
1.3
</td>
<td style="background-color: #c6f6c6">
2.4
</td>
<td style="background-color: #c6f6c6">
2.8
</td>
<td style="background-color: #c6f6c6">
3.0
</td>
<td style="background-color: #c6f6c6">
3.2
</td>
<td style="background-color: #c6f6c6">
3.4
</td>
<td style="background-color: #c6f6c6">
2.4
</td>
<td>
0.0
</td>
<td>
0.0
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 9: The presence matrix from Figure 8, reproduced</code></pre>
</div>
</div>
<p>The presence matrix, shown in figure 9, makes this notion explict -
assuming some fixed granularity of observation window, a vertical slice
in time corresponds to a column in the presence matrix.</p>
<p>Looking at the sums of the presence masses along the rows of the
matrix gives us the mass contributions per signal (or presence) and the
row sums give us cumulative presence mass per signal across all
presences.</p>
<p>Given a sequence of columns of the presence matrix, the signal
incidences are simply the number of rows that have non-zero values those
rows. It’s straightforward now to interpret the presence invariant in
terms of the presence matrix.</p>
<p>Further, you can see that we can construct a presence matrix from any
subset of the rows of another presence matrix, and it would still be a
presence matrix, for which the presence invariant applies. What is more,
the presence invariant applies for any <em>consecutive</em> sequence of
columns of the presence matrix.</p>
<p>Thus the presence invariant encodes very strong <em>local</em>
constraints in how presence mass distributes in time across signals, and
we can use these to derive meaningful constraints on the global
accumulation of presence mass across a system of presences.</p>
<h2 id="application-of-the-presence-invariant">Application of the
Presence Invariant</h2>
<p>The presence calculus might seem like a highly abstract, theoretical
framework, but much of its utility emerges when we <em>interpret</em>
its concepts in a specific, applied context.</p>
<p>Concepts such as presence mass, incidence rates, and density are not
unlike abstract physical notions like force, mass, and acceleration. In
principle, these are measurable quantities that nature constrains to
behave in prescribed ways at a micro scale.</p>
<p>Once we understand the rules governing their micro-scale behavior, we
gain tools to systematically measure, reason about, and explain a vast
range of observable macro-scale phenomena. Much of physics is built on
this principle.</p>
<p>In a similar vein, the presence calculus—and especially the
<em>presence invariant</em> —provides a foundational law that governs
the local, time-based behavior of any system composed of time-varying
signals and the presences they induce.</p>
<p>Once we recognize that such a governing constraint exists, the
presence calculus equips us with tools to describe, interpret, explain,
and, in certain cases,<br />
make verifiable predictions about the macro-scale behavior of these
systems.</p>
<p>Newtonian mechanics, for example, allows us to describe and predict
the motion of physical systems with remarkable precision—such as
planetary orbits or the paths of falling objects. Yet even within this
well-established framework, certain limits remain: the general
three-body problem has no closed-form solution, and systems like the
double pendulum exhibit chaotic behavior that defies long-term
prediction.</p>
<p>Still, we can represent the behavior and evolutions of such systems
as deterministic trajectories through a parameter space, uncovering
structure even where precise global behavior remain unpredictable. In
much the same way, the presence calculus does not seek to forecast the
exact evolution of complex systems. Instead, by explicitly modeling
signal histories and representing element trajectories over time, it
equips us with powerful descriptive and explanatory tools.</p>
<p>In this way, structural constraints and local invariants help us
interpret locally observed dynamics and connect it to system behavior at
the macro scale.</p>
<p>Let’s see how.</p>
<h2 id="the-presence-accumulation-matrix">The Presence Accumulation
Matrix</h2>
<p>To visualize and reason about both the micro and macro behaviors of a
system of presences in a natural way, we will need the help of a data
structure called the Presence Accumulation matrix. It compresses a lot
of information about the interaction between local and global behavior
of a system of presences across time scales and relies heavily on the
fact that the presence invariant is scale-independent.</p>
<p>We’ll continue with the presence matrix of Figure 9, to illustrate
how it is constructed.</p>
<p>Recall that the columns of the presence matrix represent time
intervals at the finest level of granularity at which an underlying
system of signals and presences is sampled. If we have an <span
class="math inline">\(MxN\)</span> of <span
class="math inline">\(M\)</span> signal sampled at <span
class="math inline">\(N\)</span> consecutive intervals, the value in the
presence matrix at row <span class="math inline">\(i\)</span> and column
<span class="math inline">\(j\)</span> represents the sampled presence
mass of signal <span class="math inline">\(i\)</span> at time interval
<span class="math inline">\(j\)</span>.</p>
<p>Consider the matrix <span class="math inline">\(A\)</span> that we
accumulates presences masses across consecutive time intervals of
various length between 1 and <span class="math inline">\(N\)</span>.
This is a square matrix of size <span class="math inline">\(NxN\)</span>
and is constructed as follows. For our example, this is a 10x10
matrix.</p>
<p>The diagonal of the matrix contains the accumulated presence mass
across each interval of length 1. This corresponds to the sum of each
column of the presence matrix.</p>
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td style="background-color:#e6ffe6">
1.4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
9.2
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.0
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.6
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
6.6
</td>
<td>
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.5
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
1.7
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 10: Cumulative presence mass along intervals of length 1. </code></pre>
</div>
</div>
<p>The next diagonal row contains the cumulative presence mass for
intervals of length 2 - ie <span class="math inline">\(A(1,2)\)</span>
contains the sum of all entries in columns 1 and 2. <span
class="math inline">\(A(2,3)\)</span> contains the sum of all entries
column 2 and 3 and so on..</p>
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td style="background-color:#e6ffe6">
1.4
</td>
<td style="background-color:#e6f0ff">
7.3
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td style="background-color:#e6f0ff">
15.1
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
9.2
</td>
<td style="background-color:#e6f0ff">
14.2
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.0
</td>
<td style="background-color:#e6f0ff">
10.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td style="background-color:#e6f0ff">
13.4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.5
</td>
<td style="background-color:#e6f0ff">
15.1
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.6
</td>
<td style="background-color:#e6f0ff">
14.2
</td>
<td>
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
6.6
</td>
<td style="background-color:#e6f0ff">
12.1
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.5
</td>
<td style="background-color:#e6f0ff">
7.2
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
1.7
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 11: Cumulative presence mass along intervals of length 2. </code></pre>
</div>
</div>
We can continue filling the matrix out in diagonal order this way until
we get the presence accumulation matrix shown below.
<div style="text-align: center; margin:2em">
<table>
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<!-- Row 1 -->
<tr>
<td>
1
</td>
<td style="background-color:#e6ffe6">
1.4
</td>
<td style="background-color:#e6f0ff">
7.3
</td>
<td style="background-color:#e6ffe6">
16.5
</td>
<td style="background-color:#e6f0ff">
21.5
</td>
<td style="background-color:#e6ffe6">
27.4
</td>
<td style="background-color:#e6f0ff">
34.9
</td>
<td style="background-color:#e6ffe6">
42.5
</td>
<td style="background-color:#e6f0ff">
49.1
</td>
<td style="background-color:#e6ffe6">
54.6
</td>
<td style="background-color:#e6f0ff">
56.3
</td>
</tr>
<!-- Row 2 -->
<tr>
<td>
2
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td style="background-color:#e6f0ff">
15.1
</td>
<td style="background-color:#e6ffe6">
20.1
</td>
<td style="background-color:#e6f0ff">
26.0
</td>
<td style="background-color:#e6ffe6">
33.5
</td>
<td style="background-color:#e6f0ff">
41.1
</td>
<td style="background-color:#e6ffe6">
47.7
</td>
<td style="background-color:#e6f0ff">
53.2
</td>
<td style="background-color:#e6ffe6">
54.9
</td>
</tr>
<!-- Row 3 -->
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
9.2
</td>
<td style="background-color:#e6f0ff">
14.2
</td>
<td style="background-color:#e6ffe6">
20.1
</td>
<td style="background-color:#e6f0ff">
27.6
</td>
<td style="background-color:#e6ffe6">
35.2
</td>
<td style="background-color:#e6f0ff">
41.8
</td>
<td style="background-color:#e6ffe6">
47.3
</td>
<td style="background-color:#e6f0ff">
49.0
</td>
</tr>
<!-- Row 4 -->
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.0
</td>
<td style="background-color:#e6f0ff">
10.9
</td>
<td style="background-color:#e6ffe6">
18.4
</td>
<td style="background-color:#e6f0ff">
26.0
</td>
<td style="background-color:#e6ffe6">
32.6
</td>
<td style="background-color:#e6f0ff">
38.1
</td>
<td style="background-color:#e6ffe6">
39.8
</td>
</tr>
<!-- Row 5 -->
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td style="background-color:#e6f0ff">
13.4
</td>
<td style="background-color:#e6ffe6">
21.0
</td>
<td style="background-color:#e6f0ff">
27.6
</td>
<td style="background-color:#e6ffe6">
33.1
</td>
<td style="background-color:#e6f0ff">
34.8
</td>
</tr>
<!-- Row 6 -->
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.5
</td>
<td style="background-color:#e6f0ff">
15.1
</td>
<td style="background-color:#e6ffe6">
21.7
</td>
<td style="background-color:#e6f0ff">
27.2
</td>
<td style="background-color:#e6ffe6">
28.9
</td>
</tr>
<!-- Row 7 -->
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.6
</td>
<td style="background-color:#e6f0ff">
14.2
</td>
<td style="background-color:#e6ffe6">
19.7
</td>
<td style="background-color:#e6f0ff">
21.4
</td>
</tr>
<!-- Row 8 -->
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
6.6
</td>
<td style="background-color:#e6f0ff">
12.1
</td>
<td style="background-color:#e6ffe6">
13.8
</td>
</tr>
<!-- Row 9 -->
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.5
</td>
<td style="background-color:#e6f0ff">
7.2
</td>
</tr>
<!-- Row 10 -->
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
1.7
</td>
</tr>
</tbody>
</table>
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 12: Final presence accumulation matrix for the presence matrix of Figure 9. </code></pre>
</div>
</div>
<p>We can see that in practice, this matrix compresses a large amount of
information in a very compact form. For example if the columns represent
weekly samples of signals a 52x52 matrix allows us to analyze a whole
years worth of presence accumulation across every time-scale ranging
from a single week to a whole year in one compact structure. We’ll
shortly see why this is useful.</p>
<p>First lets define the general structure of the presence accumulation
matrix.</p>
<p>Let <span class="math inline">\(P \in \mathbb{R}^{M \times
N}\)</span> be the presence matrix of <span
class="math inline">\(M\)</span> signals sampled at <span
class="math inline">\(N\)</span> consecutive time intervals. The
<strong>Presence Accumulation Matrix</strong> <span
class="math inline">\(A \in \mathbb{R}^{N \times N}\)</span> is defined
by:</p>
<p><span class="math display">\[
A(i, j) = \sum_{k=1}^{M} \sum_{\ell=i}^{j} P(k, \ell)
\quad \text{for all } 1 \leq i \leq j \leq N
\]</span></p>
<p>That is, <span class="math inline">\(A(i,j)\)</span> gives the total
presence mass of all signals across the interval <span
class="math inline">\([i,j]\)</span>.</p>
<h3 id="properties">Properties</h3>
<ul>
<li><span class="math inline">\(A\)</span> is upper triangular: <span
class="math inline">\(A(i,j)\)</span> is defined only when <span
class="math inline">\(i \leq j\)</span>.</li>
<li>The diagonal entries <span class="math inline">\(A(i,i)\)</span>
equal the column sums of <span class="math inline">\(P\)</span>.</li>
<li>Each entry <span class="math inline">\(A(i,j)\)</span> reflects the
cumulative presence mass over the interval <span
class="math inline">\([i,j]\)</span>.</li>
</ul>
<p>As we will see below, this matrix compactly encodes multi-scale
information about system behavior and supports the analysis of both
micro and macro scale behavior of a system of presences.</p>
<table>
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td style="background-color:#e6ffe6">
1.4
</td>
<td style="background-color:#ffffcc">
7.3
</td>
<td style="background-color:#ffffcc">
16.5
</td>
<td style="background-color:#ffffcc">
21.5
</td>
<td style="background-color:#ffffcc">
27.4
</td>
<td style="background-color:#ffffcc">
34.9
</td>
<td style="background-color:#ffffcc">
42.5
</td>
<td style="background-color:#ffffcc">
49.1
</td>
<td style="background-color:#ffffcc">
54.6
</td>
<td style="background-color:#ffffcc">
56.3
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
9.2
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.0
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.6
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
6.6
</td>
<td>
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.5
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
1.7
</td>
</tr>
</tbody>
</table>
<div
style="width: 100%; display: flex; justify-content: center; margin-top: 1em; margin-bottom: 1em;">
<table style="border-collapse: collapse;">
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td style="background-color:#e6ffe6">
1.4
</td>
<td style="background-color:#ffffcc">
3.6
</td>
<td style="background-color:#ffffcc">
5.5
</td>
<td style="background-color:#ffffcc">
5.4
</td>
<td style="background-color:#ffffcc">
5.5
</td>
<td style="background-color:#ffffcc">
5.8
</td>
<td style="background-color:#ffffcc">
6.1
</td>
<td style="background-color:#ffffcc">
6.1
</td>
<td style="background-color:#ffffcc">
6.1
</td>
<td style="background-color:#ffffcc">
5.6
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
9.2
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.0
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
7.6
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
6.6
</td>
<td>
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
5.5
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td style="background-color:#e6ffe6">
1.7
</td>
</tr>
</tbody>
</table>
</div>
<div style="text-align: center; margin:2em">
<img src="../assets/pandoc/first_row_vs_main_diagonal.png" width="600px" />
<div
style="font-size: 0.9em; color: #555; margin-top: 1em; margin-bottom: 1em;">
<pre><code>Figure 9: Convergence of long run average presence density.</code></pre>
</div>
</div>
<table style="border-collapse: collapse; margin: auto;">
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
</tr>
</tbody>
</table>
<table style="border-collapse: collapse; margin: auto;">
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
</tr>
</tbody>
</table>
<table style="border-collapse: collapse; margin: auto;">
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
</tr>
</tbody>
</table>
<table style="border-collapse: collapse; margin: auto;">
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
</tr>
</tbody>
</table>
<table style="border-collapse: collapse; margin: auto;">
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
</tr>
</tbody>
</table>
<table style="border-collapse: collapse; margin: auto;">
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
3
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
3
</td>
<td>
4
</td>
<td>
4
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
2
</td>
</tr>
</tbody>
</table>
<table style="border-collapse: collapse; margin: auto;">
<thead>
<tr>
<th>
i\j
</th>
<th>
1
</th>
<th>
2
</th>
<th>
3
</th>
<th>
4
</th>
<th>
5
</th>
<th>
6
</th>
<th>
7
</th>
<th>
8
</th>
<th>
9
</th>
<th>
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
1
</td>
<td>
0.13 ∠ -0.29
</td>
<td>
0.68 ∠ -0.3
</td>
<td>
0.9 ∠ -0.29
</td>
<td>
1.08 ∠ -0.31
</td>
<td>
1.15 ∠ -0.33
</td>
<td>
1.2 ∠ -0.38
</td>
<td>
1.22 ∠ -0.43
</td>
<td>
1.24 ∠ -0.47
</td>
<td>
1.26 ∠ -0.51
</td>
<td>
1.26 ∠ -0.55
</td>
</tr>
<tr>
<td>
2
</td>
<td>
</td>
<td>
0.86 ∠ -0.29
</td>
<td>
1.02 ∠ -0.29
</td>
<td>
1.17 ∠ -0.31
</td>
<td>
1.26 ∠ -0.34
</td>
<td>
1.29 ∠ -0.39
</td>
<td>
1.31 ∠ -0.44
</td>
<td>
1.32 ∠ -0.48
</td>
<td>
1.34 ∠ -0.52
</td>
<td>
1.34 ∠ -0.56
</td>
</tr>
<tr>
<td>
3
</td>
<td>
</td>
<td>
</td>
<td>
0.71 ∠ -0.23
</td>
<td>
0.94 ∠ -0.26
</td>
<td>
1.14 ∠ -0.3
</td>
<td>
1.2 ∠ -0.35
</td>
<td>
1.26 ∠ -0.41
</td>
<td>
1.31 ∠ -0.46
</td>
<td>
1.34 ∠ -0.5
</td>
<td>
1.36 ∠ -0.54
</td>
</tr>
<tr>
<td>
4
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.06 ∠ 0.0
</td>
<td>
0.42 ∠ 0.19
</td>
<td>
0.61 ∠ 0.18
</td>
<td>
0.8 ∠ 0.16
</td>
<td>
0.91 ∠ 0.14
</td>
<td>
0.97 ∠ 0.13
</td>
<td>
1.01 ∠ 0.12
</td>
</tr>
<tr>
<td>
5
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.1 ∠ 0.0
</td>
<td>
0.46 ∠ 0.21
</td>
<td>
0.69 ∠ 0.2
</td>
<td>
0.83 ∠ 0.18
</td>
<td>
0.91 ∠ 0.17
</td>
<td>
0.97 ∠ 0.16
</td>
</tr>
<tr>
<td>
6
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.19 ∠ 0.0
</td>
<td>
0.56 ∠ 0.23
</td>
<td>
0.75 ∠ 0.22
</td>
<td>
0.88 ∠ 0.21
</td>
<td>
0.97 ∠ 0.2
</td>
</tr>
<tr>
<td>
7
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.15 ∠ 0.0
</td>
<td>
0.51 ∠ 0.24
</td>
<td>
0.73 ∠ 0.23
</td>
<td>
0.89 ∠ 0.22
</td>
</tr>
<tr>
<td>
8
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.06 ∠ 0.0
</td>
<td>
0.43 ∠ 0.23
</td>
<td>
0.67 ∠ 0.22
</td>
</tr>
<tr>
<td>
9
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.08 ∠ 0.0
</td>
<td>
0.44 ∠ 0.25
</td>
</tr>
<tr>
<td>
10
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
0.0 ∠ 0.0
</td>
</tr>
</tbody>
</table>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This document is the first step in that direction. We
welcome feedback on how it can be improved,and the concepts clarified.
Please feel free to open a pull request with thoughts, suggestions or
feedback.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>If integration signs in a “gentle” introduction feels
like a bait-and-switch, rest assured, for the puposes of this document
you just need to think of them as a way to add up presence masses, in a
way that the ideas we use for binary presences will generalize when we
apply them to arbitrary functions.<a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The way we’ve defined signals and mass is directly<br />
analogous to how mass is defined for matter occupying space in
physics.</p>
<p>A binary signal can be thought of as defining a one-dimensional
interval over<br />
time. For a fixed element and boundary, this gives us an area under
the<br />
curve in two dimensions: time vs. density.</p>
<p>If we treat elements and boundaries as additional independent
dimensions,<br />
then the signal defines a <em>volume</em> in three dimensions, with time
as one axis.</p>
<p>This interpretation—presence as a physical manifestation of density
over<br />
time—is a powerful way to reason intuitively and computationally about
duration, overlap, and accumulation in time.</p>
<p>And when we allow multiple signals to interact over the same time
periods, we<br />
begin to model complex, higher-dimensional effects of presence—exactly
the<br />
kind of generality we’ll need when we move beyond simple binary
presences.<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The use of a rough curve here is an example of how
presences can encode<br />
continuous inputs more effectively than discrete techniques, thanks to
their<br />
explicit model of time. Forcing a developer to rank their productivity
on a<br />
Likert scale often loses valuable nuance—whereas a fine-grained
presence<br />
captures temporal variation with ease, making it available for
downstream<br />
analysis.<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>There are several technical conditions that must be
satisfied when<br />
mapping signals to a canonical system of presences in order for this
claim to<br />
hold. To avoid getting bogged down in those details, we’ll simply claim
it for<br />
now. The API docs go into more detail about the mechanics of this
canonical<br />
representation, and what’s needed to ensure a “clean” mapping from a
signal to a system of presences—or more precisely, a system of presence
assertions.<a href="#fnref5" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>It is equally valid to define <span
class="math inline">\(N\)</span> as the number of distinct
<em>presences</em> in the observation window. For example for the signal
<span class="math inline">\(P2\)</span> in Figure 5, this corresponds to
asking if <span class="math inline">\(N=5\)</span> (if we count the
disjoing presences individually) or <span
class="math inline">\(N=3\)</span> (if we count the signals). These give
different values for <span class="math inline">\(\bar{m}\)</span> and
<span class="math inline">\(\iota\)</span> but their product <em>still
equals</em> <span class="math inline">\(\delta\)</span>, as long as a
single consistent definition of N is used. This is ultimately a modeling
decision that depends on what you are trying to measure. By default we
will assume that <span class="math inline">\(N\)</span> is measured at
the signal granularity.<a href="#fnref6" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>All solutions to an equation of the form <span
class="math inline">\(x=y \times z,\)</span> which is the form of the
presence invariant, lie on a 2D surface, called a manifold in 3
dimensions: think of the manifold a rubber sheet suspended in a 3
dimensional space. All the values of <span class="math inline">\(x, y,
\text{ and } z\)</span> that satisfy this equation will lie on the
surface of this rubber sheet. This is a powerful geometric constraint
that we can exploit to reason about the possible behavior of a system of
presences over time.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>The alert reader will note the difference between the
first two rows in the matrix. The first signal is represented by a
single presence, while the second is broken up into two disjoint
presences. This is entirely an artifact of the granularity at which the
timeline is divided. At a suitably fine sampling granularity, the first
signal could also be represented by two presences. As discussed earlier,
this has no real impact on the behavior of the invariant. In general we
will use signals as the unit at which incidence is measured.<a
href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
